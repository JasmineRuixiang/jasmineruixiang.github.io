---
layout: post
title: 'High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring'
date: 2026-02-06 11:16:09
description: Zscoring, block vs session level comparisons
tags: 
    - "Brain Computer Interface"
categories: 
    - "Brain-Computer Interface"
featured: false
related_posts: true
related_publications: true
citation: true
math: true
toc:
  beginning: true
  sidebar: left
---

This short blog provides a detailed, self-contained computational analysis of whether two z-scoring procedures applied to block-structured neural data are equivalent, and if indeed different how.

---

## 1] Problem setup

Let's say that we have neural data organized into blocks:

* Number of blocks:
  $$B$$
* Each block has data matrix in shape:
  $$(x_{i, j}^b) \in \mathbb{R}^{t \times n}, \quad b = 1, \dots, B$$
* $$t$$ = number of time points (samples)
* $$n$$ = number of neural features

Z-scoring is performed **feature-wise**, so all derivations below consider **one fixed feature** (column) at a time. The argument applies independently to every feature.

---

## 2] Notation for a single feature

For a fixed feature $$j$$:

* Let
  $$x_{i,j}^{b} \in \mathbb{R}^{1}$$
  denote the data at time $$i$$ for the feature $$j$$ in block $$b$$. For the following paragraphs, I will simplify $$x_{:,j}^{b}$$ into $$x_{j}^{b} \in \mathbb{R}^{t\times 1}$$, and $$x_{i,:}^{b}$$ into $$x_{i}^{b} \in \mathbb{R}^{1\times n}$$. Naturally, $$x^b \in \mathbb{R}^{t\times n}$$ with the same shape for all blocks. Basically, $$i$$ corresponds to the index of time $$t$$, and $$j$$ the index of the number of neurons $$n$$.

* Block-wise mean:
  $$\mu_{j}^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i, j} \in \mathbb{R}^{1}$$,

  $$\mu ^{b} = [\mu_{1}^{b}, \cdots, \mu_{n}^{b}] \in \mathbb{R}^{1 \times n}$$.

  Or, we could simply write $$\mu ^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i} \in \mathbb{R}^{1 \times n}$$

* Block-wise variance:
  $$(\sigma_{j}^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i, j} - \mu_{j}^b)^2 \in \mathbb{R}^{1}$$
  $$(\sigma ^{b})^2 = [(\sigma_{1}^{b})^2, \cdots, (\sigma_{n}^{b})^2] \in \mathbb{R}^{1 \times n}$$
  Or, we could simplify the above as  $$(\sigma ^{b})^2 =  \frac{1}{t}\sum_{i=1}^t (x^{b}_{i} - \mu^b) \odot (x^{b}_{i} - \mu^b) \in \mathbb{R}^{1}$$
  where $$\odot$$ is the Hadamard product between two vectors (or just elementwise multiplication), defined as $$x \odot y = diag(x)y = (x_i y_i)_i \in \mathbb{R}^{1\times n}, $$ where $$x,y \in \mathbb{R}^{1\times n}$$

---

## 3] Method A: Block-wise z-scoring, concatenate, and then global z-scoring

### Step 3a]: Z-score within each block

Each block is normalized independently:

$$
z^{b}_{i} = \frac{x^{b}_i - \mu^b}{\sigma^b} \in \mathbb{R}^{1\times n}
$$

Notice that this is element-wise division (to not complicate the symbols, I'll use this abuse of notation for the following).

By construction, for every block $$b$$:

$$
\frac{1}{t}\sum_{i=1}^t z^{b}_{i} = \vec{0} \in \mathbb{R}^{1\times n},
\qquad
\frac{1}{t}\sum_{i=1}^t <z^{b}_{i} - \vec{0}, z^{b}_{i} - \vec{0}> = \vec{1} \in \mathbb{R}^{1\times n},
$$

Consequently, each feature in each block has mean 0 and standard deviation 1. Notice that $$z^b$$ observes the same notation rule as I described above. 

---

### Step 3b]: Concatenate all normalized blocks

Concatenate all $$z^{b}$$ into a single vector of length $$Bt$$.

#### Global mean

$$
\mu'
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t z^{b}_i =  \frac{1}{B}\sum_{b=1}^B \vec{0}
= \vec{0} \in \mathbb{R}^{1\times n}
$$

#### Global variance

$$
(\sigma')^2
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t <z^{b}_i - \vec{0}, z^{b}_i - \vec{0}> \\
=  \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0})
= \frac{1}{B}\sum_{b=1}^B\vec{1} = \vec{1} \in \mathbb{R}^{1\times n}
$$

---

### Step 3c]: Second z-scoring?

Since the concatenated data already has zero mean and unit variance, adding any other layers of z-scoring has no effect.

**Final output of Method A:**

$$
\boxed{z^{b} \in \mathbb{R}^{t\times n}}
$$

for each block. So method A simply returns the block-wise standardized data.

---

## 4] Method B: Concatenate first, then global z-scoring

### Step 4a]: Concatenate raw data

Concatenate all blocks $$x^{b}$$ into a single matrix $$X \in \mathbb{R}^{Bt \times n}$$.

---

### Step 4b]: Compute global mean and standard deviation

$$
\mu
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t x^{b}_i
= \frac{1}{B}\sum_{b=1}^B \frac{1}{t}\sum_{i=1}^{t}x_i^b \\
= \frac{1}{B}\sum_{b = 1}^{B} \mu^b
$$

The global mean is the average of block-wise means (it's not hard to show that if each block has different samples, this average will become _weighted average_ by the ratio of the amount of each block's data to total data amount).

---

### Step 4c]: Compute global variance

$$
\sigma^2
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (x^{b}_i - \mu) \odot (x^{b}_i - \mu)
$$

Expand the product term:

$$
(x^{b}_i - \mu) \odot (x^{b}_i - \mu) \\
= (x^{b}_i - \mu^b + \mu^b - \mu) \odot (x^{b}_i - \mu^b + \mu^b - \mu) \\
= (x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + 2(x^{b}_i - \mu^b) \odot (\mu^b - \mu) \\ + (\mu^b - \mu) \odot (\mu^b - \mu) \\
$$

Let's see into each of the terms. Notice that when summing over $$i$$ in $$\sigma^2$$, the cross term vanishes:

$$
\frac{1}{t}\sum_{i=1}^t ((x^{b}_i - \mu^b) \odot (\mu^b - \mu))\\
= (\frac{1}{t}\sum_{i=1}^t (x^{b}_i - \mu^b)) \odot (\mu^b - \mu)\\
= (\mu^b - \mu^b) \odot (\mu^b - \mu) \\
= \vec{0}
$$

Thus,

$$\sigma^2
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t((x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + (\mu^b - \mu) \odot (\mu^b - \mu)) \\
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) +  \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (\mu^b - \mu) \odot (\mu^b - \mu) \\
= \frac{1}{B}\sum_{1}^{B}\frac{1}{t}\sum_{i=1}^{t}(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\\
= \frac{1}{B}\sum_{1}^{B}(\sigma^b)^2 + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)$$

This is a **variance decomposition** into:

* average within-block variance
* variance of block means (between-block variance)

which meets our intuitive expectation (what else could it be anyway...)

---

### Step 4d]: Global z-scoring

Each sample is normalized as:

$$
y^{b}_i
= \frac{x^{b}_i - \mu}{\sigma} \in \mathbb{R}^{1\times n}
$$

Now if we rewrite the above using block-wise z-scores, since 

$$z_i^b = \frac{x_i^b - \mu^b}{\sigma^b} \rightarrow x_i^b = \sigma^bz_i^b + \mu^b$$, then:

$$
\boxed{
y^{b}_i
= \frac{\sigma^b}{\sigma} z^{b}_i +  \frac{\mu^b - \mu}{\sigma}
}
$$

which reinforces the idea that these all are just linear transformations: Transformations being linear, linear into one another. 

---

## Comparison of Method A and Method B

Method A output:

$$
z^{b}_i \in \mathbb{R}^{1\times n}
$$

Method B output:

$$
y^{b}_i
= \frac{\sigma^b}{\sigma} z^{b}_i +  \frac{\mu^b - \mu}{\sigma}
\in \mathbb{R}^{1\times n}$$

For the two methods to be identical for all $b,i$, we must have:

$$
\sigma_b = \sigma \quad \text{and} \quad \mu_b = \mu \quad \forall b
$$

On the other hand, notice that if we concatenate all $$y^b$$ and $$z^b$$ together separately into $$Y, Z$$, they __BOTH__ have feature-wise 0 mean and std 1. 

---

## Final result

$$
\boxed{
\begin{array}{l}
\text{The two procedures are NOT equivalent in general, even though} \\
\text{both yield global mean } 0 \text{ and std } 1 \text{ after transformations}.
\end{array}
}
$$

They are equivalent **if and only if** every block already has identical feature-wise means and variances.

* **Method A** removes all block-level mean and variance differences before concatenation.
* **Method B** preserves block-level differences and normalizes relative to the pooled distribution.

Block-wise z-scoring and global z-scoring **do not commute**.
. These choices encode different assumptions about whether block identity (e.g., session, subject, condition) should be preserved or discarded. Our choice should be driven by whether block-to-block variability is meaningful signal or nuisance variability in your analysis.

Fun quesitons: 

* 1] What if block size $$t$$ is not the same across all blocks?
* 2] What are other (useful/effective) ways of normalization which also return a fixed mean/std (0/1, e.g.)?

Practical question: In practice, how much do the statistics from these two methods actually differ? How should we interpret such differences?
