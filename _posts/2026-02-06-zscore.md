---
layout: post
title: 'BCI nitty-gritty (1): Equivalence (or Lack Thereof) between Block-wise and Global Z-Scoring'
date: 2026-02-06 11:16:09
description: Zscoring, block vs session level comparisons
tags: 
    - "Brain Computer Interface"
categories: 
    - "Brain-Computer Interface"
featured: false
related_posts: true
related_publications: true
citation: true
math: true
toc:
  beginning: true
  sidebar: left
---

This short blog provides a detailed, self-contained mathematical analysis of whether two z-scoring procedures applied to block-structured neural data are equivalent, and if different how.

---

## 1] Problem setup

Let's say that we have neural data organized into blocks:

* Number of blocks:
  $$B$$
* Each block has data matrix in shape:
  $$(x_{i, j}^b) \in \mathbb{R}^{t \times n}, \quad b = 1, \dots, B$$
* $t$ = number of time points (samples)
* $n$ = number of neural features

Z-scoring is performed **feature-wise**, so all derivations below consider **one fixed feature** (column) at a time. The argument applies independently to every feature.

---

## 2] Notation for a single feature

For a fixed feature $j$:

* Let
  $$x_{i,j}^{b} \in \mathbb{R}^{1}$$
  denote the data at time $$i$$ for the feature $$j$$ in block $$b$$. For the following paragraphs, I will simplify $$x_{:,j}^{b}$$ into $$x_{j}^{b} \in \mathbb{R}^{t\times 1}$$, and $$x_{i,:}^{b}$$ into $$x_{i}^{b} \in \mathbb{R}^{1\times n}$$. Naturally, $$x^b \in \mathbb{R}^{t\times n}$$ The same for other data matrices. Basically, $$i$$ corresponds to the time $$t$$, and $$j$$ to the number of neurons $$n$$.

* Block-wise mean:
  $$\mu_{j}^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i, j} \in \mathbb{R}^{1}$$
  $$\mu ^{b} = [\mu_{1}^{b}, \cdots, \mu_{n}^{b}] \in \mathbb{R}^{1 \times n}$$
  Or, we could simply write $$\mu ^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i} \in \mathbb{R}^{1}$$

* Block-wise variance:
  $$(\sigma_{j}^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i, j} - \mu_{j}^b)^2 \in \mathbb{R}^{1}$$
  $$(\sigma ^{b})^2 = [(\sigma_{1}^{b})^2, \cdots, (\sigma_{n}^{b})^2] \in \mathbb{R}^{1 \times n}$$
  Or, we could simplify the above as  $$(\sigma ^{b})^2 =  \frac{1}{t}\sum_{i=1}^t (x^{b}_{i} - \mu^b) \odot (x^{b}_{i} - \mu^b) \in \mathbb{R}^{1}$$
  where $$\odot$$ is the Hadamard product between two vectors (or just elementwise multiplication), defined as $$x \odot y = diag(x)y = (x_i y_i)_i \in \mathbb{R}^{1\times n}, $$ where $$x,y \in \mathbb{R}^{1\times n}$$

---

## 3] Method A: Block-wise z-scoring, concatenate, and then global z-scoring

### Step 3a]: Z-score within each block

Each block is normalized independently:

$$
z^{b}_{i} = \frac{x^{b}_i - \mu^b}{\sigma^b} \in \mathbb{R}^{1\times n}
$$

Notice that this is element-wise division (to not complicate the symbols, I'll use this abuse of notation for the following).

By construction, for every block $b$:

$$
\frac{1}{t}\sum_{i=1}^t z^{b}_{i} = \vec{0} \in \mathbb{R}^{1\times n},
\qquad
\frac{1}{t}\sum_{i=1}^t <z^{b}_{i} - \vec{0}, z^{b}_{i} - \vec{0}> = \vec{1} \in \mathbb{R}^{1\times n},
$$

Consequently, each feature in each block has mean 0 and standard deviation 1. Notice that $$z^b$$ observes the same notation rule as I described above. 

---

### Step 3b]: Concatenate all normalized blocks

Concatenate all $z^{b}$ into a single vector of length $Bt$.

#### Global mean

$$
\mu'
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t z^{b}_i =  \frac{1}{B}\sum_{b=1}^B \vec{0}
= \vec{0} \in \mathbb{R}^{1\times n}
$$

#### Global variance

$$
(\sigma')^2
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t <z^{b}_i - \vec{0}, z^{b}_i - \vec{0}> \\
=  \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0})
= \frac{1}{B}\sum_{b=1}^B\vec{1} = \vec{1} \in \mathbb{R}^{1\times n}
$$

---

### Step 3c]: Second z-scoring?

Since the concatenated data already has zero mean and unit variance, adding any other layers of z-scoring has no effect.

**Final output of Method A:**

$$
\boxed{z^{b} \in \mathbb{R}^{t\times n}}
$$

for each block. So method A simply returns the block-wise standardized data.

---

## 4] Method B: Concatenate first, then global z-scoring

### Step 4a]: Concatenate raw data

Concatenate all blocks $x^{b}$ into a single matrix $$X \in \mathbb{R}^{Bt \times n}$$.

---

### Step 4b]: Compute global mean and standard deviation

$$
\mu
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t x^{b}_i
= \frac{1}{B}\sum_{b=1}^B \frac{1}{t}\sum_{i=1}^{t}x_i^b \\
= \frac{1}{B}\sum_{b = 1}^{B} \mu^b
$$

The global mean is the average of block-wise means.

---

### Step 4c]: Compute global variance

$$
\sigma^2
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (x^{b}_i - \mu) \odot (x^{b}_i - \mu)
$$

Expand the product term:

$$
(x^{b}_i - \mu) \odot (x^{b}_i - \mu) \\
= (x^{b}_i - \mu^b + \mu^b - \mu) \odot (x^{b}_i - \mu^b + \mu^b - \mu) \\
= (x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + 2(x^{b}_i - \mu^b) \odot (\mu^b - \mu) \\ + (\mu^b - \mu) \odot (\mu^b - \mu) \\
$$

Consequently, let's see into each of the terms. Notie that when summing over $$i$$ in $$\sigma^2$$, the cross term vanishes:

$$
\frac{1}{t}\sum_{i=1}^t ((x^{b}_i - \mu^b) \odot (\mu^b - \mu))\\
= (\frac{1}{t}\sum_{i=1}^t (x^{b}_i - \mu^b)) \odot (\mu^b - \mu)\\
= (\mu^b - \mu^b) \odot (\mu^b - \mu) \\
= \vec{0}
$$

Thus,

$$\sigma^2
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t((x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + (\mu^b - \mu) \odot (\mu^b - \mu)) \\
= \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) +  \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (\mu^b - \mu) \odot (\mu^b - \mu) \\
= \frac{1}{B}\sum_{1}^{B}\frac{1}{t}\sum_{i=1}^{t}(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\\
= \frac{1}{B}\sum_{1}^{B}(\sigma^b)^2 + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)$$

This is a **variance decomposition** into:

* average within-block variance
* variance of block means (between-block variance)

---

### Step 4d]: Global z-scoring

Each sample is normalized as:

$$
y^{b}_i
= \frac{x^{b}_i - \mu}{\sigma} \in \mathbb{R}^{1\times n}
$$

Now, the punchline: if we rewrite the above using block-wise z-scores, since 

$$z_i^b = \frac{x_i^b - \mu^b}{\sigma^b} \rightarrow x_i^b = \sigma^bz_i^b + \mu^b$$, then:

$$
\boxed{
y^{b}_i
= \frac{\sigma^b}{\sigma} z^{b}_i +  \frac{\mu^b - \mu}{\sigma}
}
$$

---

## Comparison of Method A and Method B

Method A output:

$$
z^{b}_i \in \mathbb{R}^{1\times n}
$$

Method B output:

$$
y^{b}_i
= \frac{\sigma^b}{\sigma} z^{b}_i +  \frac{\mu^b - \mu}{\sigma}
\in \mathbb{R}^{1\times n}$$

For the two methods to be identical for all $b,i$, we must have:

$$
\sigma_b = \sigma \quad \text{and} \quad \mu_b = \mu \quad \forall b
$$

---

## Final result

$$
\boxed{
\text{The two procedures are NOT equivalent in general.}
}
$$

They are equivalent **if and only if** every block already has identical feature-wise means and variances.

* **Method A** removes all block-level mean and variance differences before concatenation.
* **Method B** preserves block-level differences and normalizes relative to the pooled distribution.

Block-wise z-scoring and global z-scoring **do not commute**.
. These choices encode different assumptions about whether block identity (e.g., session, subject, condition) should be preserved or discarded. Our choice should be driven by whether block-to-block variability is meaningful signal or nuisance variability in your analysis.

Fun quesiton: What if block size $$t$$ is not the same across all blocks?
