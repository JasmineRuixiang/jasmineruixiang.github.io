---
layout: post
title: Subspace geometry
date: 2026-02-03 21:49:38
description: Subspace geometry, computation
tags: 
    - "Neural Manifold"
    - "Dimensionality Reduction"
categories: 
    - "Brain-Computer Interface"
featured: false
related_posts: true
related_publications: true
citation: true
math: true
toc:
  beginning: true
  sidebar: left
---

## Setup

Suppose you compute PCA on two datasets (e.g., train vs.\ test), and you keep the top-$r$ principal components. Let

$$
U_{\text{train}} \in \mathbb{R}^{d \times r}, 
\qquad
U_{\text{test}} \in \mathbb{R}^{d \times r},
$$

where each matrix has **orthonormal columns** (so each is a basis for an $r$-dimensional subspace of $\mathbb{R}^d$).

Define the **subspace overlap matrix**

$$
S = U_{\text{train}}^{\top} U_{\text{test}} \in \mathbb{R}^{r \times r}.
$$

Each entry is a dot product between basis vectors:

$$
S_{ij} = u_i^{(\text{train})} \cdot u_j^{(\text{test})}.
$$

At first glance, this looks like a direct “basis alignment” comparison.

---

## 1) Why basis-by-basis alignment is unstable

Comparing PCA vectors one-by-one (e.g., “PC1 vs.\ PC1”) is unstable because PCA eigenvectors are **not uniquely defined** in two common cases.

### (i) Sign flips

If $u$ is an eigenvector, then $-u$ is also an eigenvector.

So dot products can flip sign even when the *subspace is identical*.

### (ii) Degenerate / near-degenerate eigenvalues (rotation inside the subspace)

If

$$
\lambda_i \approx \lambda_{i+1},
$$

then the corresponding principal directions inside the 2D span can rotate dramatically under tiny perturbations (noise, finite-sample effects, etc.).

This means that even if the *span* is essentially the same, the individual vectors $u_i$ can change a lot.  
So comparing “PC1 to PC1” is not meaningful.

---

## 2) What singular values do that dot products don’t

The matrix

$$
S = U_{\text{train}}^{\top} U_{\text{test}}
$$

depends on the *chosen bases* inside each subspace. If we change bases within either subspace via orthogonal transformations:

$$
U_{\text{train}} \to U_{\text{train}} R_1,
\qquad
U_{\text{test}} \to U_{\text{test}} R_2,
$$

where $R_1, R_2 \in \mathbb{R}^{r \times r}$ are orthogonal (including sign flips as a special case), then

$$
S \to (U_{\text{train}}R_1)^{\top}(U_{\text{test}}R_2)
= R_1^{\top} S R_2.
$$

So the *entries* of $S$ can change wildly.

### Key fact (invariance)

The **singular values** of $S$ are invariant under left/right orthogonal rotations:

- Left-multiplying by an orthogonal matrix does not change singular values.
- Right-multiplying by an orthogonal matrix does not change singular values.

Therefore, even if PCA “relabels,” flips signs, or rotates the basis vectors within the subspace, the **singular values remain unchanged**.

This means singular values capture a property of the **subspaces**, not of the particular eigenvectors chosen.

---

## 3) Geometric meaning: principal angles

Take the SVD:

$$
S = Q \Sigma R^{\top},
$$

where

$$
\Sigma = \mathrm{diag}(\sigma_1,\dots,\sigma_r).
$$

A fundamental result is:

$$
\sigma_i = \cos(\theta_i),
$$

where $\theta_i$ are the **principal angles** between the two $r$-dimensional subspaces.

Interpretation:

- $\theta_i = 0 \implies$ perfectly aligned direction exists (since $\cos(\theta_i)=1$)
- $\theta_i = 90^\circ \implies$ orthogonal direction (since $\cos(\theta_i)=0$)

So the singular values summarize *how much overlap* the two subspaces have along their best-aligned directions.

---

## 4) Why this is the “stable” comparison

Think of $U_{\text{train}}$ and $U_{\text{test}}$ as **arbitrary coordinate systems** inside their respective subspaces.

A meaningful comparison should ignore that arbitrariness.

Principal angles / singular values do exactly this: they compute the **best possible matching** between directions in the two subspaces.

Instead of comparing “PC1 $\leftrightarrow$ PC1,” we solve an optimal alignment problem:

$$
\max_{\|a\|=\|b\|=1} a^{\top}\bigl(U_{\text{train}}^{\top}U_{\text{test}}\bigr)b,
$$

and the sequence of best matches yields

$$
\sigma_1,\sigma_2,\dots
$$

as the strengths of alignment along the best-aligned directions.

---

## 5) Tiny example intuition (2D case)

Suppose both subspaces are actually the same 2D plane in $\mathbb{R}^d$.

You could pick:

- $U_{\text{train}}$ = standard basis in that plane
- $U_{\text{test}}$ = same plane but rotated by $45^\circ$ inside it

Then $S$ might look like a rotation matrix:

$$
S=
\begin{pmatrix}
\cos 45^\circ & -\sin 45^\circ \\
\sin 45^\circ & \cos 45^\circ
\end{pmatrix}.
$$

The entries are not the identity, so basis-by-basis dot products look “not aligned.”

But the singular values of a rotation matrix are both $1$.

So singular values correctly say: **the subspaces are identical**.

That’s the whole point.

---

## Bottom line

We use singular values of

$$
U_{\text{train}}^{\top}U_{\text{test}}
$$

because:

- ✅ they are invariant to sign flips / rotations / re-ordering of PCA vectors inside the subspace  
- ✅ they define principal angles, which are a true subspace-to-subspace comparison  
- ✅ they give a stable measure of drift even when eigenvectors are not uniquely defined  

---

## Connection to projection distance (optional)

Let $P_{\text{train}}$ and $P_{\text{test}}$ be the orthogonal projection matrices onto the two subspaces:

$$
P_{\text{train}} = U_{\text{train}}U_{\text{train}}^{\top},
\qquad
P_{\text{test}} = U_{\text{test}}U_{\text{test}}^{\top}.
$$

Then one can show the Frobenius-distance relationship:

$$
\|P_{\text{train}} - P_{\text{test}}\|_F^2
= 2r - 2\|U_{\text{train}}^{\top}U_{\text{test}}\|_F^2
= 2\sum_{i=1}^r \sin^2(\theta_i).
$$
