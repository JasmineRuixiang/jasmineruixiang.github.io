---
layout: post
title: Rotational dynamics in neural population
date: 2025-08-16 18:30:16
description: Introduce neural latent dynamics from jPCA 
tags: 
    - "Latent Dynamics"
    - "Neural Manifold"
    - "Dimensionality Reduction"
categories: 
    - "Computational Neuroscience"
featured: true
related_posts: true
related_publications: true
citation: true
math: true
toc:
  beginning: true
  sidebar: left
---

## Preface
> All changed, changed utterly:
> 
> A terrible beauty is born. 
>
>  –––– William Butler Yeats, _Easter, 1916_
>

---

Many years later, when I reflect back on the first quarter of the 21st century studies on computational neuroscience and  brain-computer interface (BCI), among papers on fancy Neural Network based decoders and clinical breakthroughs expanding from cursor and motor BCI to speech and vision, I might still recollect a distant afternoon when I first heard the name Mark Churchland, and more specifically, neural population dynamics and jPCA (Avid readers of _Gabriel García Márquez_ might be smiling or provoked at the blatantly imitated time traversal imbued in this sentence structure). "Terrible beauty", that's the phrase which came to my mind at that time. Since then, such impression has taken its roots only deeper as we observe an ongoing and accelerating burst of more research with similar flavor supporting the dynamical system view of neural population. 

I perhaps first read this paper {% cite jpca %} at least a decade after it got published, but the astonishing finding and the elegance of the algorithm, together with its implicit influences which reshaped my views on interpreting neural population and, also, the current buzzword "emergence", made it a classic enduring the test of time. This post is both a flashback and an exploration, where I extract a few key components from the article, specifically its dynamical systems interpretation and the jPCA algorithm itself, and briefly discuss around a few questions and extend from the paper to some open queries in the end. 


## Dynamical systems perspective
A traditional perspective characeterizes neural activities from the primary motor cortex (M1) as representing the corresponding movement parameters. Equivalently, we could write out a parametric equation:

$$
r_n(t) = f_n(param_{1}(t), param_{2}(t), param_{3}(t), ...) 
$$

where $$r_n(t)$$ is the firing rate for the $$n$$th neuron, tuned by the corresponding function $$f_n$$. Alternatively, instead of a representational model, another perspective based on neural population encoding which reflects behavior parameters not on the single neuron level, but on the population level with a __dynamical system__, could be written as follows: 

$$
\dot{r}(t) = f(r(t)) + u(t)
$$

Here $$f$$ might represent a linear/nonlinear dynamical system, and $$u(t)$$ is an unknown external input. In this view, the dynamics, i.e., the evolution of population response, encodes the movement parameters. Or put differently, within a dynamical system model, each single-unit response should reflect the "dynamical factors" exhibited from each latent state in the latent space, which we aim to identify from the observed high-dimensional neural pouplation recordings.

## Quasi-rhythmic responses 
As made clear in the article, the critical finding of this study is that reaching, a non-oscillatory movement (unlike the swimming leech or a walking monkey), leads to a quasi-oscillatory neural trajectory. More surprisingly, the rotations are distinct not by reaching curvatures but determined from the initial conditions, which are encoded by the __preparatory activities__. We will see that __preparatory activities__ feature as an essential ingredient in Churchland and his colleagues' research, leading to surpising results from analysis on nullspace/output-potent space {% cite nullspace %} and {% cite prep_review %} a few years later. Specifically, as the authors summarized, the trajectories have the following primary properties, which support the dynamical systems perspective:

> 1] Rotation is a ubiquitous phenomenon during behavior;  
>
> 2] Trajectories have the same directions for all rotations;  
>
> 3] Preparatory activities determine the initial conditions which govern trajectories;  
>
> 4] Rotations do not directly correlate with the curvature  


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.liquid loading="eager" path="assets/img/jPCA/jPCA_fig_3.png" class="img-fluid rounded z-depth-1" zoomable=true
        width="65%" %}
    </div>
</div>
<div class="caption">
    Adapted from Fig. 3 in {% cite jpca %}. jPCA projections into the first 2 jPC dimensions from different monkeys on tasks across different conditions and targets. Each trajectory plots the first <strong>200ms</strong> activities following the preparatory state (initial conditions). Colors correpond to projection onto jPC1. 
</div>


An interesting reflection is that with the dynamical systems perspective hypothesis, rotations of neural states should be similar no matter what reaching conditions: only the initial conditions would determine the trajectories, with the underlying $$f$$ being identical. I found this line of thought extremely insightful, which demonstrates how important it is to deeply understand theory to deduce results, though they might look unorthodox at first glance  for example, the neural states were expected be similar even when the reaches are in opposite directions if they share similar preparatory activities, because they are highly correlated with initial conditions. The only question that remains is how to determine initial conditions. Again, this ties by to experimental observations and penetrative reflections/insights into the perhaps daily orthodox phenomena.  

The authors also carried some control (shuffling) analyses, and corroborated that such rotational pattern does explain a significant amount of data variance. Interestingly, the authors also discovered that, although rotations are consistent for all conditions in the same jPCA plane (similar orietations and speeds), such rotations actually exist in multiple jPCA planes. As shown below, all top 3 jPCA planes contain rotations, but with higher-numbered jPCA planes carrying less ordered rotations with slower speed. Perhaps not so surprisingly, both PMd and M1 exhibit such rotational structures, with initial states/prepatory activities better distinguished in PMd (PMd is known for movement planning and with stronger preparatory activities. Eight years later, Russo et al. {% cite russo %} would showcase an obvious distinction between how M1 and PMd encode movement sequence/planning by exploring their corresponding latent trajectories, providing further insights into representational differences between these two brain regions) 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.liquid loading="eager" path="assets/img/jPCA/jPCA_supfig_4.png" class="img-fluid rounded z-depth-1" zoomable=true
        width="55%" %}
    </div>
</div>
<div class="caption">
    Adapted from Supplementary Fig. 4 in {% cite jpca %}. jPCA projections into different planes exhibit similar rotation structures. 
</div>


## jPCA
Based on the dynamical systems perspective, since we focus on time-dependent variations, a naive PCA is not sufficent to extract such temporal structures from neural activities (PCA is not specifically designed for incapsulating dynamical structures). Here Churchland et. al. developed an algorithm called jPCA to reoslve this issue. Specifically, it finds orthonormal axes (thus basis which define linear subspaces) which capture the strongest rotational components from the subspace identified by PCA (to ensure that the rotational dynamics come from subspaces that efficiently "represent" the high dimensional neural space). Conseuqently, this is equivalent to rotating the PCA projections to help viewers better "see" the rotation most clearly (as shown in Supplementary Movie 2 below). In the paper the authors chose the PCA dimension as 6, and the data projected from the 6-D PCA space to the first 2 jPCA components, thus a plane which captures the strongest rotations, is displayed (Adapted Supplementary Movie 2, shown below) to reveal the underlying oscillatory structure. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include video.liquid path="assets/video/jPCA/jPCA_Supp_Video_2.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=true %}
    </div>
</div>
<div class="caption">
    Adapted from Supplementary Movie 2 in {% cite jpca %}, showing jPCA projections are essentially a different view of PCA projections. 
</div>

The core of jPCA lies in the following steps:

> 1] Fit a linear dynamical system for $$\dot{X}_{red} = M_{skew}X_{red}$$, where $$X_{red}$$ is of size $$6 \times ct$$, $$c$$: conditions, $$t$$: time. Notice that this is really just a matrix formulation of $$\dot{x}_{red, i} = M_{skew}x_{red, i}$$, where $$\dot{x}_{red, i}, \; x_{red, i}$$ are column vectors and $$\dot{X}_{red} = [\dot{x}_{red, 1}, \; \dot{x}_{red, 2}, \; ..., \; \dot{x}_{red, (c(t-1))}], X_{red} = [x_{red, 1}, \; x_{red, 2}, \; ..., \; x_{red, c(t-1)}]$$. Since $$\dot{X}_{red}$$ is of size $$6 \times c(t-1)$$ (use discrete time difference as approximations for the differentice), $$M_{skew} \in \mathbb{R}^{6 \times 6}$$. Notice that to be precise, $$X_{red}$$ is also truncated to be $$6 \times c(t-1)$$. 6 is the dimensionlality of a PCA projection before starting jPCA, as mentioned above;   
>
> 2] Since $$M_{skew}$$ is a skew-symmetric matrix, it has pure imaginary eigenvalues and thus captures rotational dynamics;  
> 
> 3] Identify the complex vectors $$V_1$$ and $$V_2$$ corresponding to the largest two imaginary eigenvalues. From these locate the real planes: $$jPC_1 = V_1 + V_2$$, and $$jPC_2 = j(V_1 - V_2)$$;  
>
> 4] The first 2 jPC projection is thus $$X_{jPC} = [jPC_1; jPC_2] \times X_{red}$$. Similar projections for other jPCA planes. 

Note that the above $$[jPC_1; jPC_2] \in \mathbb{R}^{2 \times 6}$$. Also, for a given jPCA plane, the choice of orthogonal basis is arbitruary, so the authors pick $$ jPC_1$$ and $$ jPC_2$$ such that the rotation is anti-clockwise and the preparatory activities spread along most clearly on $$jPC_1$$. 

For the sections below, I'll dive into details of the jPCA algorithm. Readers might want to skip the following if the above information is sufficient. For a more detailed description of jPCA, let's start by reordering $$X$$ as $$X \in \mathbb{R}^{ct \times n}$$, and for instance $$X$$ might be choosen as $$X_{red} \in \mathbb{R}^{ct \times k}$$, for $$k = 6$$ after PCA projection. For simpler and more general notation, for the following I will use $$X$$ for illustration. 

### Dynamical summary of data
For a canonical PCA projection, we would start by finding the covaraince matrix $$\Sigma = X^TX \in \mathbb{R}^{n \times n}$$ (notice that here $$n$$ represents not the number of samples, but the number of features). We assume that $$X$$ is already mean-centered. To capture dynamical structure, we will need a different $$n \times n$$ matrix to summarize the data: consider a time-invariant linear dynamical system: $$\dot{x}(t) = x(t)M, M \in \mathbb{R}^{n \times n}$$ (in accordance with the size of $$X$$, $$x(t)$$ is a row vector: $$x(t) \in \mathbb{R}^{1 \times n}$$). Consequently, this reduces to solving 

$$
M^{*} = \argmin_{M \in \mathbb{R}^{n \times n}} ||\dot{X} - XM||_{F}
$$

Or simply put, $$M^{*} = X \backslash \dot{X}$$. Notice that this optimization has a simple and closed-form solution: 

$$
M^{*} = (X^TX)^{-1}X^T\dot{X}
$$

As the authors argued, the data covariance matrix $$\Sigma$$ captures __an ellipsoid which best fits the data__. It geometrically represents an ellipsoid because quadratic forms from symmetric positive semidefinite matrices naturally yield ellipsoids. Notice that a quadratic form 

$$
q(x) = x^TAx
$$

where $$A$$ is symmetric, could be interpreted as the square of distorted length of x. Consequently, the level set $$x^TAx = c$$ defines the set all points with distorted length (by $$A$$) equal to $$\sqrt{c}$$. If $$A$$ is symmetric and positive definite, then the level set becomes an ellipsoid Specifically, $$\Sigma$$ (with data already demeaned) encodes the spread and correlation of data along different directions. The corresponding eigenvectors point at the principal directions of variation, while the eigenvalues indicate how much the data varies along those directions. Notice that here "best fit" is not in the sense of $$L_2$$, but that the covariance ellipsoid axes align with the data’s main directions of variability, and its radii scale with the spread of the data. In other words, If we project the data onto the directions of eigenvectors/ellipsoid axes, the ellipsoid gives a natural compact summary of how spread out the data are. Consequently, if the data is uncorrelated in all feature dimensions and the variance along each feature is identical (isotropic), then the covariance is $$\Sigma = \sigma I$$ and thus the ellipsoid is a sphere. In summary, $$\Sigma$$ is a geometric label of the second order structure of the data. However, given such benefits, it does not take temporal information into consideration, whereas $$M^*$$ characterizes a linear dynamical system which best fits (linearly) the data $$X$$ (to be fair, here the data $$X$$ already carries with it some dynamical structure). 


### Skew-symmetric specification for rotation
For a general linear dynamical system, the dynamics include both expansions/contractions and rotations. Specifically, if $$M$$ has real eigenvalues $$\lambda_i$$, the system exhibits exponential growth (unstable direction, all $$\lambda_i > 0$$), or exponential decay (stable direction, all $$\lambda <0$$) along the corresponding eigenvectors, or saddle behavior (some $$\lambda_i < 0$$ while some $$\lambda_j > 0 $$). Geometrically, this looks like stretching or shrinking along certain axes. If $$M$$ has complex eigenvalues $$\alpha_i \pm i\beta_i$$ the system combines exponential scaling (from $$\alpha_i$$) with rotation (from $$\beta_i$$). If $$\alpha_i=0$$, it's pure rotation. If $$\alpha_i \neq 0$$, then the dynamics exhibit a spiral (either expansion or contraction depending on the sign of $$\alpha_i$$). Note that the above are local behaviors around fixed pionts, and will be easier to visualize (with fewer combinatorial conditions) in second-order linear dynamical systems since there'll only be 2 eigenvalues.  

In this study, the authors are particularly interested in extracting potentially pure rotational dynamics. Consequently, Churchland and colleagues started with the decomposition that every linear transformation $$M$$ (thus the linear dynamics of $$\dot{X}$$, within the same space) can be dissected into two components:

$$
M = M_{symm} + M_{skew}
$$

where $$M_{symm} = (M + M^T)/2, M_{skew} = (M - M^T)/2$$, so that $$M_{symm} = M_{symm}^T, \; M_{skew} = -M_{skew}^T$$. This raises an interesting point whether all linear transformations can be decomposed into symmetric and skew-symmetric components: well, not quite, since here the formula only exist for $$M$$ being a square matrix. However, even if $$M \in \mathbb{R}^{k \times l}, \; k \neq l$$, it is still a linear transformation. How should we reconcile and think deeper here? I will leave this to the last open queries for further discussions.

With such decomposition, $$M_{symm}$$ has purely real eigenvalues and $$M_{skew}$$ has purely imaginary eigenvalues (in conjugate pairs). They describe expansions/contractions and rotations, correspondingly (This relation is further illustrated in the last section together with matrix exponential and Lie group/algebra).

Consequently, if we specify the set of skew-symmetric matrices as $$\not \mathbb{S}^{n\times n}$$, and 

$$
M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}
$$

### Solution of the constrained optimization
#### Matrix problem rewritten into vector form
For solving the above optimization constrained to only skew-symmetric matrices, Churchland and colleagues did the following modifications: first notice that when solving $$M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}$$, each column of M is independently determined by the corresponding column of $$\dot{X}$$. Consequently, the matrix optimization problem could be written in vector format: unroll $$M \in \mathbb{R}^{n \times n}$$ into $$m = M(:)$$, where $$m \in \mathbb{R}^{n^2}$$, and thus the unconstrained least squares problem could be rewritten as:

$$
m^{*} = \argmin_{m \in \mathbb{R}^{n^2}} ||\dot{x} - \tilde{X}m||_{F}
$$

where $$\dot{x} = \dot{X}(:)$$, and $$\tilde{X}$$ is a block diagonal matrix with $$X$$ repeated on the $$n$$ diagonal blocks. PS: I totally agree that the matrix/vector formats are equivalent, but it does not take $$M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}$$ to realize such format conversion. By inspecting upon $$\dot{X} = XM$$, we should already know that each column of $$\dot{X}$$ is independently informed by the corresponding column of $$M$$. The deeper reason for this rewrittiing is revealed as below: 

#### Skew-symmetric matrix constraint
To add skew-symmetricity constraint to the optimization problem, notice that for $$\not \mathbb{S}^{n \times n}$$, the degrees of freedom is $$n(n-1)/2$$ (the diagonals are automatically 0, thus with $$n$$ less than $$n(n+1)/2$$ as the degrees of freedon/dimensionality in symmetric matrix). The next crucial step is to realize that a skew-symmetric matrices are equivalently represented as vectors of the form $$k \in \mathbb{R}^{n(n-1)/2}$$. I also want to mention that this is fundamentally related to the underlying Lie group structure [TODO] and I'll leave them to the last section for more discussoins.

A side note: the paradigm of setting constraints/descriptions even before fitting the model also reinforces my view that acquiring __geometric priors__, or basically whatever kinds of priors, embedded in the decoding models is alleviating model's "__cognitive burden__" (exactly like a spider offloading its attention onto its web to detect when its prey is coming instead of staying focused all the time (See Chapter 1 of %cite modelsmind %)). It vaguely evokes the flavor of the current trend of speech BCI, which entails large language models for improving RNN decding. Geometric deep learning is also a field with similar guiding principle.  

Back to the central topic, with this vector $$k \in \mathbb{R}^{n(n-1)/2}$$, we could likewise simply form a linear map $$H$$ which transforms $$\mathbb{R}^{n(n-1)/2}$$ into $$\mathbb{R}^{n^2}$$. In a nutshell, $$k$$ encodes the lower/upper triangle of $$M_{skew}$$ and $$H$$ can be hand-coded (not shown here, but sparse and has only $$n(n-1)/2$$ positions of 1 and $$n(n-1)/2$$ positions of -1). 

#### Final closed-form optimization
Notice that now we finally see the benefit of the vector format: $$Hk$$ returns a vector of shape $$\mathbb{R}^{n^2}$$ which equivalently represents $$M_{skew} \in \not \mathbb{S}^{n \times n}$$. Consequently, the original constrained optimzation problem 

$$
M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}
$$

is eventually rewritten as 

$$
k^{*} = \argmin_{k \in \mathbb{R}^{n(n-1)/2}} ||\dot{x} - \tilde{X}Hk||_{2}
$$

Notice that this is similar in form to the OLS problem, and can thus be written in closed form:

$$
k^{*} = (\tilde{X}H)\backslash \dot{x}
$$

and eventually 

$$
M^*_{skew} = R(Hk^*)
$$

where $$Hk^* \in \mathbb{R}$$

The authors also briefly discussed a few other implementation techniques, including solving the constrained optimization problem using a gradient descent based method. However, since this optimization problem has a unique global optimum, they should return the same results. 




## Discussions

### What does it represent to decompose the dynamics?
With $$M^{*}_{skew}$$, the authors could find out the corresponding eigenvectors and eigenvalues and do projections accordingly. Since skew-symmetric matrices only have pure imaginary eigenvalues and the eigenvectors come in __orthogonal, complex conjugate__ pairs (thus could think about the decomposition of $$M_{skew}^{*}$$ as extracting orthogonal planes), the real projection planes $${u_{i, 1}, u_{i, 2}}$$ are calculated from the complex conjugate vector pairs $${v_{i, 1}, v_{i, 2}}$$ by $$u_{i, 1} = v_{i, 1} + v_{i, 2}, \; u_{i, 1} = j(v_{i, 1} - v_{i, 2})$$ with normalization (magnitude 1). Notice that the eigenvector/eigenvalue decomposition logics are different here: For PCA, eigenvectors of the covariance matrix serve as principal components of the underlying data (geometric view: axes of ellipsoid as mentioned above); For jPCA, the eigenvectors stipulate the axes/subspace along which the solutions of the underlying dynamical system show.

To explain this clearly, let's take an example linear dynamical system problem: 

[TODO]

Matrix exponential

(Associate to Suppl.Fig.11 panels d, e)

Projections onto jPC's with largest magnitude of the corresponding eigenvalues (explain what this means) present most significant rotations: higher frequency and more consistency (Supplemantary Figure 4)


### Quasi-oscillation
The extracted neural trajectory is only "quasi-oscillatory", and usually exists for barely 1 cycle. 

For many neurons, the preparatory and movement-related single neuron responses do exhibit some quasi-oscillations which last for about 1 cycle (See in below).

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.liquid loading="eager" path="assets/img/jPCA/jPCA_fig_2.png" class="img-fluid rounded z-depth-1" zoomable=true
        width="70%" %}
    </div>
</div>
<div class="caption">
    Adapted from Fig. 2 in {% cite jpca %}. Trace colors are according to the strengthof preparatory activities (red with greatest preparotory response). It allows clear view of the evolution of preparatory patterns during the movement. 
</div>

Not sure yet if this explains the observed rotation structure at the population level. However, the population level state-space rotations direcly to neural responses at single unit level: notice that each axis of jPCA exhibits similar patterns as single-neuron response(shown below)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.liquid loading="eager" path="assets/img/jPCA/jPCA_supfig_1.png" class="img-fluid rounded z-depth-1" zoomable=true width="60%" %}
    </div>
</div>
<div class="caption">
    Adapted from Supplementary Fig. 1 in {% cite jpca %}. The projections onto jPC's are similar to single neuron response. 
</div>

As Churchland et al argued, this could be explained by the fact that jPC's capture the most conscipuous patterns in the data, and each jPCA projection is a weighted sum/average of individual-neuron resopnses. Likewise, each single neuron could also be interpreted as a combination of all the jPC projected patterns, except that the intertwined relationship is not obvious if just viewed from a single neuron or a pattern itself. 

We might argue that perhaps a sufficient condition for the rotation structures is that neurons exhibit multiphasic responses. However, this turns out to be not necessarily true. The authors also tested on a "complex-kinematic model" and recorded EMG. Though both carry multi-phasic reponses, they do not show clear rotation patterns. In the end, as the authors argued, multiphasic responses are not enough: we also need the multi-phasic patterns to have consistent phases for around 90 degrees apart. 

More exploration on the 90 degrees phase difference here: 

Another core question is how such rotational pattern generates non-oscilatory movement patterns, e.g. reaching. The authors conducted some further simulation analyses and demonstrated that it's possible to reconstruct EMG (deltoid) activities from weighted sum of rotations with different magnitude and phases (they call this the generator-model). It's interesting to see this because EMG does not acquire the latent rotational patterns, yet it could be reconstructed from rotational dynamics. Further quantification analyses also showed that only the neural data and generator-model exhibit rotational patterns, whereas two other models (velocity-tuned, complex-kinematic models (not explained here in this blog)) do not. Since EMG showed only weak rotation, as the authors put, this further illustrates that the latent rotation dynamics do not necessarily result from a muli-phasic reponse, but how such response is constructed. 

One interesting finding is that the speed of movements is not encoded as the speed of the latent trajectories (they keep similar angular velocity). The amplitude of rotation dictates different movement speeds (for example faster or slower reaches). As Churchland et al. argued, this could be nicely explained since rotations with larger amplitude represent more strongly multi-phasic responses, with which EMG frequently entails larger accelerataions. 

The rotational structure is dependent upon the initial states (dynamical systems view) specified by the prepratory activities, which captures the condition-dependent activities (here condition refers to experiment conditions). Indeed, to clearly observe such differences, the cross-condition mean is substracted first for each single neuron before PCA/jPCA. The authors also illustrated the effects of keeping the cross-condition mean, as shown below:

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.liquid loading="eager" path="assets/img/jPCA/jPCA_supfig_11.png" class="img-fluid rounded z-depth-1" zoomable=true width="60%" %}
    </div>
</div>
<div class="caption">
    Adapted from Supplementary Fig. 11 in {% cite jpca %}. 
</div>

The goal of mean subtraction is to preserve only data distinguished in different conditions. In panel a. and b., each trace represents a trial-averaged firing rate for one (experiment) condition, with the same coloring scheme by the strength of preparatory activities. The yellow trace indicates the cross-condition mean for that neuron. Panel b. substracts the mean (yellow line being 0 all along) while panel a. keeps it. If without substraction of cross-condition mean, it's observed that the first jPCA plane captures condition-independent curved trajectories. The authors argued that this is not surprising, since many neurons exhibit similar behaviors across different conditions, which are thus naturally captured by PCA projections. Since jPCA is extracting patterns from PCA space, it's fairly expected that such patterns are condition independent. Notice that even in panel c., these trajectories still carry curvature, though it's difficult to pinpoint how it's theoretically generated. Panel d. displays the jPCA projection onto the second jPC plane ($$jPC3_, \; jPC_4$$) and we do see similar rotational structures dependent upon initial states specified by the preparatory activities. Consequently, to always explore the condition-dependent dynamics, the authors argue to substract cross-condition mean first before PCA/jPCA. 


As the authors put at the end of the [Supplementary Information](https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf), the jPCA method could be easily adapted to find the projections for most significant expansion/contraction.   

Please allow me to presume upon how Churchland et al. came up with this method. 


## Conclusions
In summary, 

> 1] "state-space rotations produce briefly oscillatory temporal patterns that provide an effective basis for producing multiphasic muscle activity, suggesting that non-periodic movements may be generated via neural mechanisms resembling those that generate rhythmic movement";  
> 
> 2] Preparactory encoders the initial conditions for the underlying dynamical system;  

The shift of views from single neuron analysis in pursuit of movement/behavior correlates/representaitons, to a dynamical system analysis on population level, is becoming an increasingly populat domain. As we will see later, the other side of the same story, geometry instead of dynamics, will quickly come into the arena under the name of neural manifold. This, with both aboundance of aesthetic elegance and disatisfying lack of mathematical rigor from either topology or differential geometry, will be an eternal theme for the following research, projects, and blogs of mine. 


## Open Querries
The annotomical circuitry that leads to the dynamical system/latent trajectory is still unclear. 

What happens after the 200ms? As shown in Supp Movie 3 below, how would the observed rotations evolve later in time (notice that all the rotations exist for merely 1~1.5 cylces)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include video.liquid path="assets/video/jPCA/jPCA_Supp_Video_3.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=true %}
    </div>
</div>
<div class="caption">
    Adapted from Supplementary Movie 3 in {% cite jpca %}, showing the evolution of the projected states. 
</div>

Although in the Discussions section, I briefly talked about the reason for existence of the rotational structures, The problem remains as to how individual oscillations of different phases and amplitudes would result in a consistent rotation. 

At the algorithmic level, the jPCA method stems from dissecting the rotation component away from the general linear transformation. Can this be further stripped away from PCA? 

Churchland argued that the dynamics quantified by a skew-symmetric matrix hints at the underlying functionally antisymmetric connectivity. The authors wrote: "a given neural dimension (for example, $$jPC_1$$) positively influences another (for example, $$jPC_2$$), which negatively influences the first. However, it is unclear whether this population-level pattern idrectly reflects a circuit-level dominance of antisymmetric connectivity" 

For jPCA algorithm, apart from $$\Sigma$$ or $$M^*$$, is there other way of capturing the data statistics from different perspectives?

Notice that within jPCA, the authors made the following decomposition: $$M = M_{symm} + M_{skew}$$, which naturally dissects the linear dynamics into expansions/contractions and rotations. I'm wondering whether similar decomposition can be made for general linear transformation $$M \in \mathbb{R}^{k \times l}, \; k \neq l$$. This definitely cannot be a dissection into symmetric and skew-symmetric matrices because this general $$M$$ is not a square matrix. Moreover, if we want to extend to non-square matrix, we are already out of the realm of dynamical system, since that would require the linear matrix to be square: $$\dot{x}(t) = Mx(t)$$. How do we characterize a general linear transformation? 

Why do dynamical systems specified by symmetric and skew-symmetric matrices encode expansions/contractions and rotations? The reasons for that is the solution for linear dynamical systems exhibits as a matrix exponential, which leads to eventual solutions dependent upon the eigenvalues/eigenvectors. Notice that a symmetric matrix has real eigenvalues and skew-symmetrix matrix has imaginative eigenvalues. One related question I have is: can matrix exponential here in jPCA for extrapolation, potentially applied for data later than the first 200ms? How would the extrapolated dynamical trajectories align with the observed data?


### Hermitian, unitary, and normal matrices
Churchland and colleagues put the following at the end of 
[Supplementary Information](https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf):

> "The convenience of being able to eigendecompose a
summary matrix and yield orthogonal vectors belongs to the class of normal matrices, which by definition
are diagonalizable by a unitary matrix. The class of normal matrices includes symmetric and skewsymmetric matrices, among others. This fact suggests a broader class of PCA variants that are a subject
of future work."

In order to understand the inner workings of matrix decomposition, I'd like to review first the following definitions. 

> _Def_: The Hermitian tranpose (or conjugate transpose) of a matrix $$A$$ is denoted as $$A^{\dagger} = \bar{A}^{T}$$, which the tranpose of the complex conjugate (applied to each entry). Equivalent notations are $$A^H, \; A^{*}$$. Notice that for a real matrix $$A \in \mathbb{R}^{n \times n}, \; A^{\dagger} = A^{T}$$. 
>
> _Def_: A matrix is called __Hermitian__ (or __self-adjoint__) if 
> $$
> A = A^{\dagger}
> $$. 
> Notice that A real __Hermitian__ matrix is equivalently __symmetric__.   
>
> _Def_: A matrix $$U$$ is called __unitary__ if
> $$
> U^{\dagger}U = I
> $$
> Notice that a real __unitary__ matrix is equivalently __orthogonal__.   

Notice that we also have the following theorems for the above special matrices (more discussions seen in this [lecture note](https://www.math.purdue.edu/~eremenko/dvi/lect3.26.pdf)):

>_Spectral theorem_ for Hermitian matrices: For a __Hermitian__ matrix,
>
>(i) all eigenvalues are real,  
>
>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,  
>
>(iii) there is an orthonormal basis consisting of eigenvectors.  

and likewise 

>_Spectral theorem_ for unitary matrices: For a __unitary__ matrix,
>
>(i) all eigenvalues have magnitude 1,  
>
>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,  
>
>(iii) there is an orthonormal basis consisting of eigenvectors.  

Consequently, Hermitian and unitary matrices are always __diagonalizable__ (indeed some eigenvalues might be the same). Notice that eigenvectors of any matrix corresponding
to distinct eigenvalues are linearly independent. Here with Hermitian and unitary matrices they are not only linearly indepedent, but also __orthogonal__.

Theorems (i) for both Hermitian and unitary matrices could be proven separately. Since (ii) and (iii) are the same, we might be thinking about looking for a greater class of matrices which include these two types, but carrying with more general properties, entailing (ii) and (iii). Let me introduce the following: 

> _Def_: A normal matrix is a matrix that commutes with its adjoint: 
> $$
> [A, A^{\dagger}] = 0
> $$
> where [B, C] = BC - CB

Note that Hermitian and unitary matrices are special cases of a normal matrix. Also, the previous definitions will be helpful for the next extension into Lie group/algebra. For normal matrices we have the following:

> _Spectral theorem_ for normal matrices: A matrix is __normal__ _if and only if_ there is an orthogonal basis
consisting of eigenvectors.

From the above theorems we could easily see that a symmetric matrix $$A$$ has all real eigenvalues (becuase it's a Hermitian matrix). Consequently, it has the decomposition: 

$$
A = B \Lambda B^{-1}
$$

where $$\Lambda$$ is a real iagonal matrix, B orthogonal.

There's also a famous fact between Hermitian and unitary matrix:

> There exists a 1-1 correponsdence between the set of unitary matrices $$U$$ and the exponental of the set of Hermitian matrices $$H$$, i.e.:
>
> $$
> U = exp(iH)
> $$

The proof is not hard and not displayed here. The message it encodes is important: the unitary matrices are exactly in the format of the exponential map of Hermitian matrices. 

Similarly, we could show the following for orthogonal matrix: Given $$A$$ as a skew-symmetric (real) matrix ($$A^{\dagger} = A^{T} = -A$$), notice first that 

$$
-iA = iA^T = iA^{\dagger} = (-iA)^{\dagger}
$$

Consequently, $$-iA$$ is a Hermitian matrix and thus since

$$
e^{A} = e^{i(-iA)} = e^{i(-iA)^{\dagger}}
$$

Then $$e^{A}$$ is a unitary matrix. Since $$A$$ is real, $$e^{A}$$ has to be real. Consequently, $$e^{A}$$ is an orthogonal matrix. 

However, the exponential map of skew symmetric matrices does not result in all orthogonal matrix:

$$
det(e^A) = e^{trA} = e^{0} = 1
$$

which means that this way only characterizes rotation matrices (determinant equal to 1, unlike reflection which changes the orientation). 




### Relations with Lie group and Lie algebra
Since we are talking about dynamics with symmetric and skew-symmetric matrices, how do they relate to Lie group/algebra?

Also, since this constrained optimization problem has a unique global optimum (how should we know this)? 


### Cartan decomposition of Lie algebra