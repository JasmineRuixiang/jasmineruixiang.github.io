<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Rotational dynamics in neural population | Ruixiang Li </title> <meta name="author" content="Ruixiang Li"> <meta name="description" content="Introduce neural latent dynamics from jPCA"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jasmineruixiang.github.io/blog/2025/jPCA/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ruixiang</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Rotational dynamics in neural population</h1> <p class="post-meta"> Created in August 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/latent-dynamics"> <i class="fa-solid fa-hashtag fa-sm"></i> Latent Dynamics</a>   <a href="/blog/tag/neural-manifold"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Manifold</a>   <a href="/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>     ·   <a href="/blog/category/computational-neuroscience"> <i class="fa-solid fa-tag fa-sm"></i> Computational Neuroscience</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#preface">Preface</a></li> <li class="toc-entry toc-h2"><a href="#dynamical-systems-perspective">Dynamical systems perspective</a></li> <li class="toc-entry toc-h2"><a href="#quasi-rthymic-responses">Quasi-rthymic responses</a></li> <li class="toc-entry toc-h2"> <a href="#jpca">jPCA</a> <ul> <li class="toc-entry toc-h3"><a href="#dynamical-summary-of-data">Dynamical summary of data</a></li> <li class="toc-entry toc-h3"><a href="#skey-symmetric-specification-for-rotation">Skey-symmetric specification for rotation</a></li> <li class="toc-entry toc-h3"> <a href="#solution-of-the-constrained-optimization">Solution of the constrained optimization</a> <ul> <li class="toc-entry toc-h4"><a href="#matrix-problem-rewritten-into-vector-form">Matrix problem rewritten into vector form</a></li> <li class="toc-entry toc-h4"><a href="#skew-symmetric-matrix-constraint">Skew-symmetric matrix constraint</a></li> <li class="toc-entry toc-h4"><a href="#final-closed-form-optimization">Final closed-form optimization</a></li> <li class="toc-entry toc-h4"><a href="#what-does-it-represent-to-decompose-the-dynamics">What does it represent to decompose the dynamics?</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"><a href="#discussions">Discussions</a></li> <li class="toc-entry toc-h2"><a href="#conclusions">Conclusions</a></li> <li class="toc-entry toc-h2"> <a href="#open-querries">Open Querries</a> <ul> <li class="toc-entry toc-h3"><a href="#hermitian-unitary-and-normal-matrices">Hermitian, unitary, and normal matrices</a></li> <li class="toc-entry toc-h3"><a href="#relations-with-lie-group-and-lie-algebra">Relations with Lie group and Lie algebra</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="preface">Preface</h2> <blockquote> <p>All changed, changed utterly:</p> <p>A terrible beauty is born.</p> <p>–––– William Butler Yeats, <em>Easter, 1916</em></p> </blockquote> <hr> <p>Many years later, when I reflect back on the first quarter of the 21st century studies on computational neuroscience and brain-computer interface (BCI), among papers on fancy Neural Network based decoders and clinical breakthroughs expanding from cursor and motor BCI to speech and vision, I might still recollect a distant afternoon when I first heard the name Mark Churchland, and more specifically, neural population dynamics and jPCA. “Terrible beauty”, that’s the phrase which came to my mind at that time. Since then, such impression has taken its roots only deeper as research of similar flavor supported the dynamical system view from neural population.</p> <p>I perhaps first read the paper <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a> at least a decade after it got published, but the astonishing finding and the elegance of the algorithm, together with its implicit influences which reshaped my views on interpreting neural population and thus the currently suring word “emergence”, made it a classic enduring the test of time. This post is both a flashback and an exploration, where I extract a few key components from the article, specifically its dynamical systems interpretation and the jPCA algorithm itself, and raised a few questions and extended from the paper to some open queries in the end.</p> <h2 id="dynamical-systems-perspective">Dynamical systems perspective</h2> <p>A traditional perspective characeterizes neural activities from the primary motor cortex (M1) as representing the corresponding movement parameters. Equivalently, we could write out a parametric equation:</p> \[r_n(t) = f_n(param_{1}(t), param_{2}(t), param_{3}(t), ...)\] <p>where \(r_n(t)\) is the firing rate for the \(n_{th}\) neuron, tuned by the corresponding function \(f_n\). Alternatively, instead of a representational model, another perspective based on neural population encoding which reflects behavior parameters not on the single neuron level, but on the population level with a <strong>dynamical system</strong>, could be written as follows:</p> \[\dot{r}(t) = f(r(t)) + u(t)\] <p>Here \(f\) might represent a linear/nonlinear dynamical system, and \(u(t)\) is an unknown external input. In this view, the dynamics, i.e., the evolution of population response, encodes the movement parameters. Or put differently, within a dynamical system model, each single-unit response should reflect the “dynamical factors” exhibited from each latent state in the latent space, which we aim to identify from the observed high-dimensional neural pouplation recordings.</p> <h2 id="quasi-rthymic-responses">Quasi-rthymic responses</h2> <p>As made clear in the article, the critical finding of this study is that reaching, a non-oscillatory movement (unlike the swimming leech or a walking monkey), leads to a quasi-oscillatory neural trajectory. More surprisingly, the rotations are distinct not by reaching curvatures but determined from the initial conditions, which are encoded by the <strong>preparatory activities</strong>. We will see that <strong>preparatory activities</strong> feature as an essential ingredient in Churchland and colleagues’ research, resulting in surpising analysis on nullspace/output-potent space later <a class="citation" href="#nullspace">(“Cortical Activity in the Null Space: Permitting Preparation without Movement,” 2014)</a><a class="citation" href="#prep_review">(Mark M Churchland, 2024)</a> in the years to. Specifically, as the authors summarized, the trajectories have the following primary properties, which support the dynamical system perspective:</p> <blockquote> <p>1] Rotation is a ubiquitous phenomenon during behavior;</p> <p>2] Trajectories have the same directions for all rotations;</p> <p>3] Preparatory activities determine the initial conditions which govern trajectories;</p> <p>4] Rotations do not directly correlate with the curvature</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_3.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 3 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. jPCA projections into the first 2 dimensions of different monkeys. Each trajectory plots the first <strong>200ms</strong> activities following the preparatory state (initial conditions). Colors correpond to projection onto jPC1. </div> <p>An interesting reflection is that with the dynamical system perspective hypothesis, rotations of neural states should be similar no matter what reaching conditions: only the initial conditions determine the trajectories, with the underlying \(f\) being identical. I found this line of thought extremely insightful, which demonstrates how important it is to deeply understand theory to deduce results, though they might look unorthodox — for example, the neural states were expected be similar even when the reaches are in opposite directions if they share similar preparatory activities, because they are highly correlated wit initial conditions.</p> <p>The authors also carried some control (shuffling) analyses, and corroborated that such rotational pattern does explain a significant amount of data variance. Interestingly, the authors also discovered that, although rotations are consistent for all conditions in the same jPCA plane (similar orietations and speeds), such rotations actually exist in multiple jPCA planes. As shown below, all top 3 jPCA planes contain rotatinos, but with higher-number jPCA planes carrying less ordered rotations with slower speed. Perhaps not so surprisingly, both PMd and M1 exhibit such rotational structures, with initial states/prepatory activities better distinguished in PMd (again not surpising, because PMd is known for movement planning and with stronger preparatory activities. Eight years later, Russo et al. <a class="citation" href="#russo">(“Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation,” 2020)</a> will exhibit an obvious distinction between how M1 and PMd encode movement sequence/planning by exploring corresponding latent trajectories, providing further analyses between these two brain regions)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_4.png" class="img-fluid rounded z-depth-1" width="55%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 4 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. jPCA projections into different planes exhibit similar rotation structures. </div> <h2 id="jpca">jPCA</h2> <p>Based on the dynamical systems perspective, since we focus on time-dependent variations, a naive PCA is not sufficent to extract such temporal structures from neural activities (PCA is not specifically designed for incapsulating dynamical structures). Here Churchland et. al. developed an algorithm called jPCA to reoslve this issue. Specifically, it finds orthonormal axes (thus basis which define linear subspaces) which capture the strongest rotational components from the subspace identified by PCA (to ensure that the rotational dynamics come from subspaces that efficiently “represent” the high dimensional neural space). Conseuqently, this is equivalent to rotating the PCA projections to help viewers better “see” the rotation most clearly (as shown in Supplementary Movie 2 below). In the paper the authors chose the PCA dimension as 6, and the data projected from the 6-D PCA space to the first 2 jPCA components, thus a plane which captures the strongest rotations, is displayed (Adapted Figuer 3) to reveal the underlying oscillatory structure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 2 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>, showing jPCA projections are essentially a different view of PCA projections. </div> <p>The core of jPCA lies in the following steps:</p> <blockquote> <p>1] Fit a linear dynamical system for \(\dot{X}_{red} = M_{skew}X_{red}\), where \(X_{red}\) is of size \(6 \times ct\), \(c\): conditions, \(t\): time. Notice that this is really just a matrix formulation of \(\dot{x}_{red, i} = M_{skew}x_{red, i}\), where \(\dot{x}_{red, i}, \; x_{red, i}\) are column vectors and \(\dot{X}_{red} = [\dot{x}_{red, 1}, \; \dot{x}_{red, 2}, \; ..., \; \dot{x}_{red, (c(t-1))}], X_{red} = [x_{red, 1}, \; x_{red, 2}, \; ..., \; x_{red, c(t-1)}]\). Since \(\dot{X}_{red}\) is of size \(6 \times c(t-1)\) (use discrete time difference as approximations for the differentice), \(M_{skew} \in \mathbb{R}^{6 \times 6}\). Notice that to be precise, \(X_{red}\) is also truncated to be \(6 \times c(t-1)\). 6 is the dimensionlality of a PCA projection before starting jPCA, as mentioned above;</p> <p>2] Since \(M_{skew}\) is a skew-symmetric matrix, ithas pure imaginary eigenvalues and thus captures rotational dynamics;</p> <p>3] Identify the complex vectors \(V_1\) and \(V_2\) corresponding to the largest two imaginary complex values. From these locate the real planes: \(jPC_1 = V_1 + V_2\), and \(jPC_2 = V_1 - V_2\);</p> <p>4] The first jPCA projection is thus \(X_{jPC} = [jPC_1, jPC_2] \times X_{red}\). Similar projections for other jPCA planes.</p> </blockquote> <p>For a given jPCA plane, the choice of orthogonal basis is arbitruary, so the authors pick \(jPC_1\) and \(jPC_2\) such that the rotation is anti-clockwise and the preparatory activities spread along most clearly on \(jPC_1\).</p> <p>For the sections below, I’ll dive into details of the jPCA algorithm. Readers might want to skip the following if the above information is sufficient. For a more detailed description of jPCA, let’s start by reordering \(X\) as \(X \in \mathbb{R}^{ct \times n}\), and for instance \(X\) might be choosen as \(X_{red} \in \mathbb{R}^{ct \times k}\), for \(k = 6\) after PCA projection. For simpler and more general notation, for the following I will use X for illustration.</p> <h3 id="dynamical-summary-of-data">Dynamical summary of data</h3> <p>For a canonical PCA projection, we would start by finding the covaraince matrix \(\Sigma = X^TX \in \mathbb{R}^{n \times n}\) (notice that here n represents not the number of samples, but the number of features). We assume that \(X\) is already mean-centered. To capture dynamical structure, we will need a different \(n \times n\) matrix to summarize the data: consider a time-invariant linear dynamical system: \(\dot{x}(t) = x(t)M, M \in \mathbb{R}^{n \times n}\). Consequently, this reduces to solving</p> \[M^{*} = \argmin_{M \in \mathbb{R}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>Or simply put, \(M^{*} = X \backslash \dot{X}\). Notice that this optimization has a simple and closed-form solution:</p> \[M^{*} = (X^TX)^{-1}X^T\dot{X}\] <p>As the authors argued, the data covariance matrix \(\Sigma\) captures <strong>an ellipsoid which best fits the data</strong> (deep interpretation of this, what ellipsoid and what it represents when it says “best fit”), but with no temporal information, whereas \(M^*\) characterizes a linear dynamical system which best fits the data \(X\).</p> <p>I find this analysis particularly intriguing. In order to stay focus here, I’ll discuss a few reflections in the last section.</p> <h3 id="skey-symmetric-specification-for-rotation">Skey-symmetric specification for rotation</h3> <p>For a general linear dynamical system, the dynamics include both expandions/contractions and rotations. In this study, the authors are particularly interested in dissecting a potentially pure rotational dynamics. The key observation that Churchland and colleagues offered here is that every linear transformation \(M\) can be decomposed into two components:</p> \[M = M_{symm} + M_{skew}\] <p>where \(M_{symm} = (M + M^T)/2, M_{skew} = (M - M^T)/2\), so that \(M_{symm} = M_{symm}^T, \; M_{skew} = -M_{skew}^T\). This raises an interesting point whether all linear transformations can be decomposed into symmetric and skew-symmetric components: well, not quite, since here the formula only exist for \(M\) being a square matrix. However, even if \(M \in \mathbb{R}^{k \times l}, \; k \neq l\), it is still a linear transformation. How should we reconcile and think deeper here? I will leave this to the last section for further discussions.</p> <p>With such decomposition, the benefit it brings is that since \(M_{symm}\) have purely real eigenvalues and \(M_{skew}\) have purely imaginary eigenvalues (in complex conjugate pairs), thery describe expansions/contractions and rotations, correspondingly (the explanation for this and matrix exponential is further illustrated at the last section).</p> <p>Consequently, if we specify the set of skew-symmetric matrices as \(\not \mathbb{S}^{n\times n}\), and</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <h3 id="solution-of-the-constrained-optimization">Solution of the constrained optimization</h3> <h4 id="matrix-problem-rewritten-into-vector-form">Matrix problem rewritten into vector form</h4> <p>For solving the above optimization constrained to only skew-symmetric matrices, Churchland and colleagues did the following modifications: first notice that when solving \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\), each column of M is independently determined by the corresponding column of \(\dot{X}\). Consequently, the matrix optimization problem could be written in vector format: unroll \(M \in \mathbb{R}^{n \times n}\) into \(m = M(:)\), where \(m \in \mathbb{R}^{n^2}\), and thus the unconstrained least squares problem could be rewritten as:</p> \[m^{*} = \argmin_{m \in \mathbb{R}^{n^2}} ||\dot{x} - \tilde{X}m||_{F}\] <p>where \(\dot{x} = \dot{X}(:)\), and \(\tilde{X}\) is a block diagonal matrix with \(X\) repeated on the \(n\) diagonal blocks. I totally agree that the matrix/vector formats are equivalent, but it does not take \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\) to realize such format conversion. By inspecting upon \(\dot{X} = XM\), we should know that each column of \(\dot{X}\) is independently informed by the corresponding column of \(M\).</p> <h4 id="skew-symmetric-matrix-constraint">Skew-symmetric matrix constraint</h4> <p>To add skew-symmetricity constraint to the optimization problem, notice that for \(\not \mathbb{S}^{n \times n}\), the degrees of freedom is \(n(n-1)/2\) (notice that the diagonals are automatically 0, thus \(n\) less than \(n(n+1)/2\) as in symmetric matrix. Could think more on the underlying manifold, Lie group, and Lie algebra, see the last section).</p> <p>The crucial step is to realize that thus a skew-symmetric matrices is equivalently represented as vectors of the form \(k \in \mathbb{R}^{n(n-1)/2}\). This also reinforces my view that acquiring geometric priors, or basically whatever kinds of priors, encoded in the decoding models is alleviating model’s “cognitive burden”. I’d consider the speech BCI, which entails large language models (LLM) for improving RNN decding as adopting a “similar” strategy/line of thinking.</p> <p>Back to the central topic, with this vector \(k \in \mathbb{R}^{n(n-1)/2}\), we could likewise simply form a linear map \(H\) which transforms \(\mathbb{R}^{n(n-1)/2}\) into \(\mathbb{R}^{n^2}\). In a nutshell, \(k\) encodes the lower/upper triangle of \(M_{skew}\) and \(H\) is hand-coded (not shown here, but sparse and has only \(n(n-1)/2\) positions of 1 and \(n(n-1)/2\) positions of -1).</p> <h4 id="final-closed-form-optimization">Final closed-form optimization</h4> <p>Notice that now we finally see the benefit of the vector format: \(Hk\) returns a vector of shape \(\mathbb{R}^{n^2}\) which equivalently represents \(M_{skew} \in \not \mathbb{S}^{n \times n}\). Consequently, the original constrained optimzation problem</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>is eventually rewritten as</p> \[k^{*} = \argmin_{k \in \mathbb{R}^{n(n-1)/2}} ||\dot{x} - \tilde{X}Hk||_{2}\] <p>Notice that this is similar in form to the OLS problem can thus be written in closed form:</p> \[k^{*} = (\tilde{X}H)\backslash \dot{x}\] <p>The authors also briefly discussed a few implementation techniques, including solving using a gradient descent based method. However, since this optimization problem has a unique global optimum, they should return the same results.</p> <h4 id="what-does-it-represent-to-decompose-the-dynamics">What does it represent to decompose the dynamics?</h4> <p>Now with \(M^{*}_{skew}\), the authors found out the corresponding eigenvectors and eigenvalues and do projections like PCA. Since skew-symmetric matrices only have pure imaginary eigenvalues and the eigenvectors come in <strong>orthogonal, complex conjugate</strong> pairs (thus could think about the decomposition of \(M_{skew}\) as extracting orthogonal planes), the authors find the real projection planes \({u_{i, 1}, u_{i, 2}}\) from the complex conjugate vector pairs \({v_{i, 1}, v_{i, 2}}\) by \(u_{i, 1} = v_{i, 1} + v_{i, 2}, \; u_{i, 1} = v_{i, 1} - v_{i, 2}\) with normalization. Notice that the eigenvector/eigenvalue decomposition logics are different here: For PCA, eigenvectors of the covariance matrix serve as principal components of the underlying data (geometric view: ellipsoid); For jPCA, the eigenvectors stipulate the axes along which the solutions of the underlying dynamical system evolve.</p> <p>To explain this clearly, let’s take an example linear dynamical system problem:</p> <p>[TODO]</p> <p>Matrix exponential</p> <p>(Associate to Suppl.Fig.11 panels d, e)</p> <p>Projections onto jPC’s with largest magnitude of the corresponding eigenvalues (explain what this means) present most significant rotations: higher frequency and more consistency (Supplemantary Figure 4)</p> <h2 id="discussions">Discussions</h2> <p>The extracted neural trajectory is only “quasi-oscillatory”, and usually exists for barely 1 cycle.</p> <p>For many neurons, the preparatory and movement-related single neuron responses do exhibit some quasi-oscillations which last for about 1 cycle (See in below).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_2.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 2 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. Trace colors are according to the strengthof preparatory activities (red with greatest preparotory response). It allows clear view of the evolution of preparatory patterns during the movement. </div> <p>Not sure yet if this explains the observed rotation structure at the population level. However, the population level state-space rotations direcly to neural responses at single unit level: notice that each axis of jPCA exhibits similar patterns as single-neuron response(shown below)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_1.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 1 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. The projections onto jPC's are similar to single neuron response. </div> <p>As Churchland et al argued, this could be explained by the fact that jPC’s capture the most conscipuous patterns in the data, and each jPCA projection is a weighted sum/average of individual-neuron resopnses. Likewise, each single neuron could also be interpreted as a combination of all the jPC projected patterns, except that the intertwined relationship is not obvious if just viewed from a single neuron or a pattern itself.</p> <p>We might argue that perhaps a sufficient condition for the rotation structures is that neurons exhibit multiphasic responses. However, this turns out to be not necessarily true. The authors also tested on a “complex-kinematic model” and recorded EMG. Though both carry multi-phasic reponses, they do not show clear rotation patterns. In the end, as the authors argued, multiphasic responses are not enough: we also need the multi-phasic patterns to have consistent phases for around 90 degrees apart.</p> <p>More exploration on the 90 degrees phase difference here:</p> <p>Another core question is how such rotational pattern generates non-oscilatory movement patterns, e.g. reaching. The authors conducted some further simulation analyses and demonstrated that it’s possible to reconstruct EMG (deltoid) activities from weighted sum of rotations with different magnitude and phases (they call this the generator-model). It’s interesting to see this because EMG does not acquire the latent rotational patterns, yet it could be reconstructed from rotational dynamics. Further quantification analyses also showed that only the neural data and generator-model exhibit rotational patterns, whereas two other models (velocity-tuned, complex-kinematic models (not explained here in this blog)) do not. Since EMG showed only weak rotation, as the authors put, this further illustrates that the latent rotation dynamics do not necessarily result from a muli-phasic reponse, but how such response is constructed.</p> <p>One interesting finding is that the speed of movements is not encoded as the speed of the latent trajectories (they keep similar angular velocity). The amplitude of rotation dictates different movement speeds (for example faster or slower reaches). As Churchland et al. argued, this could be nicely explained since rotations with larger amplitude represent more strongly multi-phasic responses, with which EMG frequently entails larger accelerataions.</p> <p>The rotational structure is dependent upon the initial states (dynamical systems view) specified by the prepratory activities, which captures the condition-dependent activities (here condition refers to experiment conditions). Indeed, to clearly observe such differences, the cross-condition mean is substracted first for each single neuron before PCA/jPCA. The authors also illustrated the effects of keeping the cross-condition mean, as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_11.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 11 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. </div> <p>The goal of mean subtraction is to preserve only data distinguished in different conditions. In panel a. and b., each trace represents a trial-averaged firing rate for one (experiment) condition, with the same coloring scheme by the strength of preparatory activities. The yellow trace indicates the cross-condition mean for that neuron. Panel b. substracts the mean (yellow line being 0 all along) while panel a. keeps it. If without substraction of cross-condition mean, it’s observed that the first jPCA plane captures condition-independent curved trajectories. The authors argued that this is not surprising, since many neurons exhibit similar behaviors across different conditions, which are thus naturally captured by PCA projections. Since jPCA is extracting patterns from PCA space, it’s fairly expected that such patterns are condition independent. Notice that even in panel c., these trajectories still carry curvature, though it’s difficult to pinpoint how it’s theoretically generated. Panel d. displays the jPCA projection onto the second jPC plane (\(jPC3_, \; jPC_4\)) and we do see similar rotational structures dependent upon initial states specified by the preparatory activities. Consequently, to always explore the condition-dependent dynamics, the authors argue to substract cross-condition mean first before PCA/jPCA.</p> <p>As the authors put at the end of the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf" rel="external nofollow noopener" target="_blank">Supplementary Information</a>, the jPCA method could be easily adapted to find the projections for most significant expansion/contraction.</p> <h2 id="conclusions">Conclusions</h2> <p>In summary,</p> <blockquote> <p>1] “state-space rotations produce briefly oscillatory temporal patterns that provide an effective basis for producing multiphasic muscle activity, suggesting that non-periodic movements may be generated via neural mechanisms resembling those that generate rhythmic movement”;</p> <p>2] Preparactory encoders the initial conditions for the underlying dynamical system;</p> </blockquote> <p>The shift of views from single neuron analysis in pursuit of movement/behavior correlates/representaitons, to a dynamical system analysis on population level, is becoming an increasingly populat domain. As we will see later, the other side of the same story, geometry instead of dynamics, will quickly come into the arena under the name of neural manifold. This, with both aboundance of aesthetic elegance and disatisfying lack of mathematical rigor from either topology or differential geometry, will be an eternal theme for the following research, projects, and blogs of mine.</p> <h2 id="open-querries">Open Querries</h2> <p>The annotomical circuitry that leads to the dynamical system/latent trajectory is still unclear.</p> <p>What happens after the 200ms? As shown in Supp Movie 3 below, how would the observed rotations evolve later in time (notice that all the rotations exist for merely 1~1.5 cylces)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_3.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 3 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>, showing the evolution of the projected states. </div> <p>Although in the Discussions section, I briefly talked about the reason for existence of the rotational structures, The problem remains as to how individual oscillations of different phases and amplitudes would result in a consistent rotation.</p> <p>At the algorithmic level, the jPCA method stems from dissecting the rotation component away from the general linear transformation. Can this be further stripped away from PCA?</p> <p>Churchland argued that the dynamics quantified by a skew-symmetric matrix hints at the underlying functionally antisymmetric connectivity. The authors wrote: “a given neural dimension (for example, \(jPC_1\)) positively influences another (for example, \(jPC_2\)), which negatively influences the first. However, it is unclear whether this population-level pattern idrectly reflects a circuit-level dominance of antisymmetric connectivity”</p> <p>For jPCA algorithm, apart from \(\Sigma\) or \(M^*\), is there other way of capturing the data statistics from different perspectives?</p> <p>Notice that within jPCA, the authors made the following decomposition: \(M = M_{symm} + M_{skew}\), which naturally dissects the linear dynamics into expansions/contractions and rotations. I’m wondering whether similar decomposition can be made for general linear transformation \(M \in \mathbb{R}^{k \times l}, \; k \neq l\). This definitely cannot be a dissection into symmetric and skew-symmetric matrices because this general \(M\) is not a square matrix. Moreover, if we want to extend to non-square matrix, we are already out of the realm of dynamical system, since that would require the linear matrix to be square: \(\dot{x}(t) = Mx(t)\). How do we characterize a general linear transformation?</p> <p>Why do dynamical systems specified by symmetric and skew-symmetric matrices encode expansions/contractions and rotations? The reasons for that is the solution for linear dynamical systems exhibits as a matrix exponential, which leads to eventual solutions dependent upon the eigenvalues/eigenvectors. Notice that a symmetric matrix has real eigenvalues and skew-symmetrix matrix has imaginative eigenvalues. One related question I have is: can matrix exponential here in jPCA for extrapolation, potentially applied for data later than the first 200ms? How would the extrapolated dynamical trajectories align with the observed data?</p> <h3 id="hermitian-unitary-and-normal-matrices">Hermitian, unitary, and normal matrices</h3> <p>Churchland and colleagues put the following at the end of <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf" rel="external nofollow noopener" target="_blank">Supplementary Information</a>:</p> <blockquote> <p>“The convenience of being able to eigendecompose a summary matrix and yield orthogonal vectors belongs to the class of normal matrices, which by definition are diagonalizable by a unitary matrix. The class of normal matrices includes symmetric and skewsymmetric matrices, among others. This fact suggests a broader class of PCA variants that are a subject of future work.”</p> </blockquote> <p>In order to understand the inner workings of matrix decomposition, I’d like to review first the following definitions.</p> <blockquote> <p><em>Def</em>: The Hermitian tranpose (or conjugate transpose) of a matrix \(A\) is denoted as \(A^{\dagger} = \bar{A}^{T}\), which the tranpose of the complex conjugate (applied to each entry). Equivalent notations are \(A^H, \; A^{*}\). Notice that for a real matrix \(A \in \mathbb{R}^{n \times n}, \; A^{\dagger} = A^{T}\).</p> <p><em>Def</em>: A matrix is called <strong>Hermitian</strong> (or <strong>self-adjoint</strong>) if \(A = A^{\dagger}\). Notice that A real <strong>Hermitian</strong> matrix is equivalently <strong>symmetric</strong>.</p> <p><em>Def</em>: A matrix \(U\) is called <strong>unitary</strong> if \(U^{\dagger}U = I\) Notice that a real <strong>unitary</strong> matrix is equivalently <strong>orthogonal</strong>.</p> </blockquote> <p>Notice that we also have the following theorems for the above special matrices (more discussions seen in this <a href="https://www.math.purdue.edu/~eremenko/dvi/lect3.26.pdf" rel="external nofollow noopener" target="_blank">lecture note</a>):</p> <blockquote> <p><em>Spectral theorem</em> for Hermitian matrices: For a <strong>Hermitian</strong> matrix,</p> <p>(i) all eigenvalues are real,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>and likewise</p> <blockquote> <p><em>Spectral theorem</em> for unitary matrices: For a <strong>unitary</strong> matrix,</p> <p>(i) all eigenvalues have magnitude 1,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>Consequently, Hermitian and unitary matrices are always <strong>diagonalizable</strong> (indeed some eigenvalues might be the same). Notice that eigenvectors of any matrix corresponding to distinct eigenvalues are linearly independent. Here with Hermitian and unitary matrices they are not only linearly indepedent, but also <strong>orthogonal</strong>.</p> <p>Theorems (i) for both Hermitian and unitary matrices could be proven separately. Since (ii) and (iii) are the same, we might be thinking about looking for a greater class of matrices which include these two types, but carrying with more general properties, entailing (ii) and (iii). Let me introduce the following:</p> <blockquote> <p><em>Def</em>: A normal matrix is a matrix that commutes with its adjoint: \([A, A^{\dagger}] = 0\) where [B, C] = BC - CB</p> </blockquote> <p>Note that Hermitian and unitary matrices are special cases of a normal matrix. Also, the previous definitions will be helpful for the next extension into Lie group/algebra. For normal matrices we have the following:</p> <blockquote> <p><em>Spectral theorem</em> for normal matrices: A matrix is <strong>normal</strong> <em>if and only if</em> there is an orthogonal basis consisting of eigenvectors.</p> </blockquote> <p>From the above theorems we could easily see that a symmetric matrix \(A\) has all real eigenvalues (becuase it’s a Hermitian matrix). Consequently, it has the decomposition:</p> \[A = B \Lambda B^{-1}\] <p>where \(\Lambda\) is a real iagonal matrix, B orthogonal.</p> <p>There’s also a famous fact between Hermitian and unitary matrix:</p> <blockquote> <p>There exists a 1-1 correponsdence between the set of unitary matrices \(U\) and the exponental of the set of Hermitian matrices \(H\), i.e.:</p> \[U = exp(iH)\] </blockquote> <p>The proof is not hard and not displayed here. The message it encodes is important: the unitary matrices are exactly in the format of the exponential map of Hermitian matrices.</p> <p>Similarly, we could show the following for orthogonal matrix: Given \(A\) as a skew-symmetric (real) matrix (\(A^{\dagger} = A^{T} = -A\)), notice first that</p> \[-iA = iA^T = iA^{\dagger} = (-iA)^{\dagger}\] <p>Consequently, \(-iA\) is a Hermitian matrix and thus since</p> \[e^{A} = e^{i(-iA)} = e^{i(-iA)^{\dagger}}\] <p>Then \(e^{A}\) is a unitary matrix. Since \(A\) is real, \(e^{A}\) has to be real. Consequently, \(e^{A}\) is an orthogonal matrix.</p> <p>However, the exponential map of skew symmetric matrices does not result in all orthogonal matrix:</p> \[det(e^A) = e^{trA} = e^{0} = 1\] <p>which means that this way only characterizes rotation matrices (determinant equal to 1, unlike reflection which changes the orientation).</p> <h3 id="relations-with-lie-group-and-lie-algebra">Relations with Lie group and Lie algebra</h3> <p>Since we are talking about dynamics with symmetric and skew-symmetric matrices, how do they relate to Lie group/algebra?</p> <p>Also, since this constrained optimization problem has a unique global optimum (how should we know this)?</p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Li, Ruixiang (Aug 2025). Rotational dynamics in neural population. https://jasmineruixiang.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2025rotational-dynamics-in-neural-population</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Rotational dynamics in neural population}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Li, Ruixiang}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Aug}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://jasmineruixiang.github.io/blog/2025/jPCA/}</span>
<span class="p">}</span>
</code></pre></div></div> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="prep_review" class="col-sm-8"> <div class="title">Preparatory activity and the expansive null-space</div> <div class="author"> Krishna V Shenoy Mark M Churchland </div> <div class="periodical"> <em>Nature Reviews Neuroscience</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">prep_review</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Preparatory activity and the expansive null-space}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mark M Churchland, Krishna V Shenoy}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Reviews Neuroscience}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{25}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{213--236}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1038/s41583-024-00796-z}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="russo" class="col-sm-8"> <div class="title">Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation</div> <div class="author"> </div> <div class="periodical"> <em>Neuron</em>, Aug 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">russo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neuron}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{107}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{745--758}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.neuron.2020.05.020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="nullspace" class="col-sm-8"> <div class="title">Cortical activity in the null space: permitting preparation without movement</div> <div class="author"> </div> <div class="periodical"> <em>Nature Neuroscience</em>, Feb 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nullspace</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cortical activity in the null space: permitting preparation without movement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Neuroscience}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{440--448}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1038/nn.3643}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="jpca" class="col-sm-8"> <div class="title">Neural population dynamics during reaching</div> <div class="author"> </div> <div class="periodical"> <em>Nature</em>, Jul 2012 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">jpca</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural population dynamics during reaching}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{487}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{51--56}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1038/nature11129}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Constraint-Learning/">Constraints upon Learning from a Neural Manifold Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/blog-summary/">Blogs syntax summary</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/distill/">a distill-style blog post</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/vega-lite/">a post with vega lite</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/code-diff/">a post with code diff</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ruixiang Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>