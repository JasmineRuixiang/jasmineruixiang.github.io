<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Rotational dynamics in neural population | Ruixiang Li </title> <meta name="author" content="Ruixiang Li"> <meta name="description" content="Introduce neural latent dynamics from jPCA"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?1bba76160cd50d1600475ac5ca76b1a1"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jasmineruixiang.github.io/blog/2025/jPCA/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ruixiang</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/PhiarLitSic/">PhiArLitSic </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Rotational dynamics in neural population</h1> <p class="post-meta"> Created in August 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/latent-dynamics"> <i class="fa-solid fa-hashtag fa-sm"></i> Latent Dynamics</a>   <a href="/blog/tag/neural-manifold"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Manifold</a>   <a href="/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>     ·   <a href="/blog/category/computational-neuroscience"> <i class="fa-solid fa-tag fa-sm"></i> Computational Neuroscience</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#preface">Preface</a></li> <li class="toc-entry toc-h2"><a href="#dynamical-systems-perspective">Dynamical systems perspective</a></li> <li class="toc-entry toc-h2"><a href="#quasi-rhythmic-responses">Quasi-rhythmic responses</a></li> <li class="toc-entry toc-h2"> <a href="#jpca">jPCA</a> <ul> <li class="toc-entry toc-h3"><a href="#dynamical-summary-of-data">Dynamical summary of data</a></li> <li class="toc-entry toc-h3"><a href="#skew-symmetric-specification-for-rotation">Skew-symmetric specification for rotation</a></li> <li class="toc-entry toc-h3"> <a href="#solution-of-the-constrained-optimization">Solution of the constrained optimization</a> <ul> <li class="toc-entry toc-h4"><a href="#matrix-problem-rewritten-into-vector-form">Matrix problem rewritten into vector form</a></li> <li class="toc-entry toc-h4"><a href="#skew-symmetric-matrix-constraint">Skew-symmetric matrix constraint</a></li> <li class="toc-entry toc-h4"><a href="#final-closed-form-optimization">Final closed-form optimization</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#discussions">Discussions</a> <ul> <li class="toc-entry toc-h3"><a href="#what-does-it-represent-to-decompose-the-dynamics">What does it represent to decompose the dynamics?</a></li> <li class="toc-entry toc-h3"><a href="#quasi-similarity-micro-vs-macro--explainers">Quasi-similarity: micro-vs macro- explainers</a></li> <li class="toc-entry toc-h3"><a href="#multiphasic-response-and-phase-difference">Multiphasic response and phase difference</a></li> <li class="toc-entry toc-h3"><a href="#from-oscillation-to-non-oscillation">From oscillation to non-oscillation</a></li> <li class="toc-entry toc-h3"><a href="#speed-vs-amplitude">Speed vs. amplitude</a></li> <li class="toc-entry toc-h3"><a href="#the-effec-of-cross-condition-mean">The effec of cross-condition mean</a></li> <li class="toc-entry toc-h3"><a href="#skew-symmetric-constraint-prior-or-posterior">Skew-symmetric constraint: prior or posterior</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#conclusions">Conclusions</a></li> <li class="toc-entry toc-h2"><a href="#open-querries">Open Querries</a></li> <li class="toc-entry toc-h2"> <a href="#extensions">Extensions</a> <ul> <li class="toc-entry toc-h3"><a href="#hermitian-unitary-and-normal-matrices">Hermitian, unitary, and normal matrices</a></li> <li class="toc-entry toc-h3"><a href="#linear-transformation-and-decomposition">Linear transformation and decomposition</a></li> <li class="toc-entry toc-h3"><a href="#cartan-decomposition-of-lie-algebra">Cartan decomposition of Lie algebra</a></li> <li class="toc-entry toc-h3"><a href="#relations-with-matrix-lie-group-and-lie-algebra">Relations with Matrix Lie group and Lie algebra</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="preface">Preface</h2> <blockquote> <p>All changed, changed utterly:</p> <p>A terrible beauty is born.</p> <p>–––– William Butler Yeats, <em>Easter, 1916</em></p> </blockquote> <hr> <p>Many years later, when I reflect back on the first quarter of the 21st century studies on computational neuroscience and brain-computer interface (BCI), among papers on fancy Neural Network based decoders and clinical breakthroughs expanding from cursor and motor BCI to speech and vision, I might still recollect a distant afternoon when I first heard the name Mark Churchland, and more specifically, neural population dynamics and jPCA (Avid readers of <em>Gabriel García Márquez</em> might be smiling or provoked at the blatantly imitated time traversal imbued in this sentence structure). “Terrible beauty”, that’s the phrase which came to my mind at that time. Since then, such impression has taken its roots only deeper as we observe an ongoing and accelerating burst of more research with similar flavor supporting the dynamical system view of neural population.</p> <p>I perhaps first read this paper <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a> at least a decade after it got published, but the astonishing finding and the elegance of the algorithm, together with its implicit influences which reshaped my views on interpreting neural population and, also, the current buzzword “emergence”, made it a classic enduring the test of time. This post is both a flashback and an exploration, where I extract a few key components from the article, specifically its dynamical systems interpretation and the jPCA algorithm itself, and briefly discuss around a few questions and extend from the paper to some open queries in the end.</p> <h2 id="dynamical-systems-perspective">Dynamical systems perspective</h2> <p>A traditional perspective characeterizes neural activities from the primary motor cortex (M1) as representing the corresponding movement parameters. Equivalently, we could write out a parametric equation:</p> \[r_n(t) = f_n(param_{1}(t), param_{2}(t), param_{3}(t), ...)\] <p>where \(r_n(t)\) is the firing rate for the \(n\)th neuron, tuned by the corresponding function \(f_n\). Alternatively, instead of a representational model, another perspective based on neural population encoding which reflects behavior parameters not on the single neuron level, but on the population level with a <strong>dynamical system</strong>, could be written as follows:</p> \[\dot{r}(t) = f(r(t)) + u(t)\] <p>Here \(f\) might represent a linear dynamical system (linear dynamical system, or can be nonlinear) and \(u(t)\) is an unknown external input. In this view, the dynamics, i.e., the evolution of population response, encodes the movement parameters. Or put differently, within a dynamical system model, each single-unit response should reflect the “dynamical factors” exhibited from each latent state in the latent space, which we aim to identify from the observed high-dimensional neural pouplation recordings.</p> <h2 id="quasi-rhythmic-responses">Quasi-rhythmic responses</h2> <p>As made clear in the article, the critical finding of this study is that reaching, a non-oscillatory movement (unlike the swimming leech or a walking monkey), leads to a quasi-oscillatory neural trajectory. More surprisingly, the rotations are distinct not by reaching curvatures but determined from the initial conditions, which are encoded by the <strong>preparatory activities</strong>. We will see that <strong>preparatory activities</strong> feature as an essential ingredient in Churchland and his colleagues’ research, leading to surpising results from analysis on nullspace/output-potent space <a class="citation" href="#nullspace">(“Cortical Activity in the Null Space: Permitting Preparation without Movement,” 2014)</a> and <a class="citation" href="#prep_review">(Mark M Churchland, 2024)</a> a few years later. Specifically, as the authors summarized, the trajectories have the following primary properties, which support the dynamical systems perspective:</p> <blockquote> <p>1] Rotation is a ubiquitous phenomenon during behavior;</p> <p>2] Trajectories have the same directions for all rotations;</p> <p>3] Preparatory activities determine the initial conditions which govern trajectories;</p> <p>4] Rotations do not directly correlate with the curvature</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_3.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 3 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. jPCA projections into the first 2 jPC dimensions from different monkeys on tasks across different conditions and targets. Each trajectory plots the first <strong>200ms</strong> activities following the preparatory state (initial conditions). Colors correpond to projection onto jPC1. </div> <p>An interesting reflection is that with the dynamical systems perspective hypothesis, rotations of neural states should be similar no matter what reaching conditions: only the initial conditions would determine the trajectories, with the underlying \(f\) being identical. I found this line of thought extremely insightful, which demonstrates how important it is to deeply understand theory to deduce results, though they might look unorthodox at first glance for example, the neural states were expected be similar even when the reaches are in opposite directions if they share similar preparatory activities, because they are highly correlated with initial conditions. The only question that remains is how to determine initial conditions. Again, this ties by to experimental observations and penetrative reflections/insights into the perhaps daily orthodox phenomena.</p> <p>The authors also carried some control (shuffling) analyses, and corroborated that such rotational pattern does explain a significant amount of data variance. Interestingly, the authors also discovered that, although rotations are consistent for all conditions in the same jPCA plane (similar orietations and speeds), such rotations actually exist in multiple jPCA planes. As shown below, all top 3 jPCA planes contain rotations, but with higher-numbered jPCA planes carrying less ordered rotations with slower speed. Perhaps not so surprisingly, both PMd and M1 exhibit such rotational structures, with initial states/prepatory activities better distinguished in PMd (PMd is known for movement planning and with stronger preparatory activities. Eight years later, Russo et al. <a class="citation" href="#russo">(“Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation,” 2020)</a> would showcase an obvious distinction between how M1 and PMd encode movement sequence/planning by exploring their corresponding latent trajectories, providing further insights into representational differences between these two brain regions)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_4.png" class="img-fluid rounded z-depth-1" width="55%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 4 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. jPCA projections into different planes exhibit similar rotation structures. </div> <h2 id="jpca">jPCA</h2> <p>Based on the dynamical systems perspective, since we focus on time-dependent variations, a naive PCA is not sufficent to extract such temporal structures from neural activities (PCA is not specifically designed for incapsulating dynamical structures). Here Churchland et. al. developed an algorithm called jPCA to reoslve this issue. Specifically, it finds orthonormal axes (thus basis which define linear subspaces) which capture the strongest rotational components from the subspace identified by PCA (to ensure that the rotational dynamics come from subspaces that efficiently “represent” the high dimensional neural space). Conseuqently, this is equivalent to rotating the PCA projections to help viewers better “see” the rotation most clearly (as shown in Supplementary Movie 2 below). In the paper the authors chose the PCA dimension as 6, and the data projected from the 6-D PCA space to the first 2 jPCA components, thus a plane which captures the strongest rotations, is displayed (Adapted Supplementary Movie 2, shown below) to reveal the underlying oscillatory structure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 2 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>, showing jPCA projections are essentially a different view of PCA projections. </div> <p>The core of jPCA lies in the following steps:</p> <blockquote> <p>1] Fit a linear dynamical system (linear dynamical system) for \(\dot{X}_{red} = M_{skew}X_{red}\), where \(X_{red}\) is of size \(6 \times ct\), \(c\): conditions, \(t\): time. Notice that this is really just a matrix formulation of \(\dot{x}_{red, i} = M_{skew}x_{red, i}\), where \(\dot{x}_{red, i}, \; x_{red, i}\) are column vectors and \(\dot{X}_{red} = [\dot{x}_{red, 1}, \; \dot{x}_{red, 2}, \; ..., \; \dot{x}_{red, (c(t-1))}], X_{red} = [x_{red, 1}, \; x_{red, 2}, \; ..., \; x_{red, c(t-1)}]\). Since \(\dot{X}_{red}\) is of size \(6 \times c(t-1)\) (use discrete time difference as approximations for the differentice), \(M_{skew} \in \mathbb{R}^{6 \times 6}\). Notice that to be precise, \(X_{red}\) is also truncated to be \(6 \times c(t-1)\). 6 is the dimensionlality of a PCA projection before starting jPCA, as mentioned above;</p> <p>2] Since \(M_{skew}\) is a skew-symmetric matrix, it has pure imaginary eigenvalues and thus captures rotational dynamics;</p> <p>3] Identify the complex vectors \(V_1\) and \(V_2\) corresponding to the largest two imaginary eigenvalues. From these locate the real planes: \(jPC_1 = V_1 + V_2\), and \(jPC_2 = j(V_1 - V_2)\);</p> <p>4] The first 2 jPC projection is thus \(X_{jPC} = [jPC_1; jPC_2] \times X_{red}\). Similar projections for other jPCA planes.</p> </blockquote> <p>Note that the above \([jPC_1; jPC_2] \in \mathbb{R}^{2 \times 6}\). Also, for a given jPCA plane, the choice of orthogonal basis is arbitruary, so the authors pick \(jPC_1\) and \(jPC_2\) such that the rotation is anti-clockwise and the preparatory activities spread along most clearly on \(jPC_1\).</p> <p>For the sections below, I’ll dive into details of the jPCA algorithm. Readers might want to skip the following if the above information is sufficient. For a more detailed description of jPCA, let’s start by reordering \(X\) as \(X \in \mathbb{R}^{ct \times n}\), and for instance \(X\) might be choosen as \(X_{red} \in \mathbb{R}^{ct \times k}\), for \(k = 6\) after PCA projection. For simpler and more general notation, for the following I will use \(X\) for illustration.</p> <h3 id="dynamical-summary-of-data">Dynamical summary of data</h3> <p>For a canonical PCA projection, we would start by finding the covaraince matrix \(\Sigma = X^TX \in \mathbb{R}^{n \times n}\) (notice that here \(n\) represents not the number of samples, but the number of features). We assume that \(X\) is already mean-centered. To capture dynamical structure, we will need a different \(n \times n\) matrix to summarize the data: consider a time-invariant linear dynamical system: \(\dot{x}(t) = x(t)M, M \in \mathbb{R}^{n \times n}\) (in accordance with the size of \(X\), \(x(t)\) is a row vector: \(x(t) \in \mathbb{R}^{1 \times n}\)). Consequently, this reduces to solving</p> \[M^{*} = \argmin_{M \in \mathbb{R}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>Or simply put, \(M^{*} = X \backslash \dot{X}\). Notice that this optimization has a simple and closed-form solution:</p> \[M^{*} = (X^TX)^{-1}X^T\dot{X}\] <p>As the authors argued, the data covariance matrix \(\Sigma\) captures <strong>an ellipsoid which best fits the data</strong>. It geometrically represents an ellipsoid because quadratic forms from symmetric positive semidefinite matrices naturally yield ellipsoids. Notice that a quadratic form</p> \[q(x) = x^TAx\] <p>where \(A\) is symmetric, could be interpreted as the square of distorted length of x. Consequently, the level set \(x^TAx = c\) defines the set all points with distorted length (by \(A\)) equal to \(\sqrt{c}\). If \(A\) is symmetric and positive definite, then the level set becomes an ellipsoid Specifically, \(\Sigma\) (with data already demeaned) encodes the spread and correlation of data along different directions. The corresponding eigenvectors point at the principal directions of variation, while the eigenvalues indicate how much the data varies along those directions. Notice that here “best fit” is not in the sense of \(L_2\), but that the covariance ellipsoid axes align with the data’s main directions of variability, and its radii scale with the spread of the data. In other words, If we project the data onto the directions of eigenvectors/ellipsoid axes, the ellipsoid gives a natural compact summary of how spread out the data are. Consequently, if the data is uncorrelated in all feature dimensions and the variance along each feature is identical (isotropic), then the covariance is \(\Sigma = \sigma I\) and thus the ellipsoid is a sphere. In summary, \(\Sigma\) is a geometric label of the second order structure of the data. However, given such benefits, it does not take temporal information into consideration, whereas \(M^*\) characterizes a linear dynamical system which best fits (linearly) the data \(X\) (to be fair, here the data \(X\) already carries with it some dynamical structure).</p> <h3 id="skew-symmetric-specification-for-rotation">Skew-symmetric specification for rotation</h3> <p>For a general linear dynamical system, the dynamics include both expansions/contractions and rotations. Specifically, if \(M\) has real eigenvalues \(\lambda_i\), the system exhibits exponential growth (unstable direction, all \(\lambda_i &gt; 0\)), or exponential decay (stable direction, all \(\lambda &lt;0\)) along the corresponding eigenvectors, or saddle behavior (some \(\lambda_i &lt; 0\) while some \(\lambda_j &gt; 0\)). Geometrically, this looks like stretching or shrinking along certain axes. If \(M\) has complex eigenvalues \(\alpha_i \pm i\beta_i\) the system combines exponential scaling (from \(\alpha_i\)) with rotation (from \(\beta_i\)). If \(\alpha_i=0\), it’s pure rotation. If \(\alpha_i \neq 0\), then the dynamics exhibit a spiral (either expansion or contraction depending on the sign of \(\alpha_i\)). Note that the above are local behaviors around fixed pionts, and will be easier to visualize (with fewer combinatorial conditions) in second-order linear dynamical system since there’ll only be 2 eigenvalues.</p> <p>In this study, the authors are particularly interested in extracting potentially pure rotational dynamics. Consequently, Churchland and colleagues started with the decomposition that every linear transformation \(M\) (thus the linear dynamics of \(\dot{X}\), within the same space) can be dissected into two components:</p> \[M = M_{symm} + M_{skew}\] <p>where \(M_{symm} = (M + M^T)/2, M_{skew} = (M - M^T)/2\), so that \(M_{symm} = M_{symm}^T, \; M_{skew} = -M_{skew}^T\). This raises an interesting point whether all linear transformations can be decomposed into symmetric and skew-symmetric components: well, not quite, since here the formula only exist for \(M\) being a square matrix. However, even if \(M \in \mathbb{R}^{k \times l}, \; k \neq l\), it is still a linear transformation. How should we reconcile and think deeper here? I will leave this to the last open queries for further discussions.</p> <p>With such decomposition, \(M_{symm}\) has purely real eigenvalues and \(M_{skew}\) has purely imaginary eigenvalues (in conjugate pairs). They describe expansions/contractions and rotations, correspondingly (This relation is further illustrated in the last section together with matrix exponential and Lie group/algebra).</p> <p>Consequently, if we specify the set of skew-symmetric matrices as \(\not \mathbb{S}^{n\times n}\), and</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <h3 id="solution-of-the-constrained-optimization">Solution of the constrained optimization</h3> <h4 id="matrix-problem-rewritten-into-vector-form">Matrix problem rewritten into vector form</h4> <p>For solving the above optimization constrained to only skew-symmetric matrices, Churchland and colleagues did the following modifications: first notice that when solving \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\), each column of M is independently determined by the corresponding column of \(\dot{X}\). Consequently, the matrix optimization problem could be written in vector format: unroll \(M \in \mathbb{R}^{n \times n}\) into \(m = M(:)\), where \(m \in \mathbb{R}^{n^2}\), and thus the unconstrained least squares problem could be rewritten as:</p> \[m^{*} = \argmin_{m \in \mathbb{R}^{n^2}} ||\dot{x} - \tilde{X}m||_{F}\] <p>where \(\dot{x} = \dot{X}(:)\), and \(\tilde{X}\) is a block diagonal matrix with \(X\) repeated on the \(n\) diagonal blocks. PS: I totally agree that the matrix/vector formats are equivalent, but it does not take \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\) to realize such format conversion. By inspecting upon \(\dot{X} = XM\), we should already know that each column of \(\dot{X}\) is independently informed by the corresponding column of \(M\). The deeper reason for this rewrittiing is revealed as below:</p> <h4 id="skew-symmetric-matrix-constraint">Skew-symmetric matrix constraint</h4> <p>To add skew-symmetricity constraint to the optimization problem, notice that for \(\not \mathbb{S}^{n \times n}\), the degrees of freedom is \(n(n-1)/2\) (the diagonals are automatically 0, thus with \(n\) less than \(n(n+1)/2\) as the degrees of freedon/dimensionality in symmetric matrix). The next crucial step is to realize that a skew-symmetric matrices are equivalently represented as vectors of the form \(k \in \mathbb{R}^{n(n-1)/2}\). I also want to mention that this is fundamentally related to the underlying Lie group structure [TODO] and I’ll leave them to the last section for more discussoins.</p> <p>A side note: the paradigm of setting constraints/descriptions even before fitting the model also reinforces my view that acquiring <strong>geometric priors</strong>, or basically whatever kinds of priors, embedded in the decoding models is alleviating model’s “<strong>cognitive burden</strong>” (exactly like a spider offloading its attention onto its web to detect when its prey is coming instead of staying focused all the time (See Chapter 1 of %cite modelsmind %)). It vaguely evokes the flavor of the current trend of speech BCI, which entails large language models for improving RNN decding. Geometric deep learning is also a field with similar guiding principle.</p> <p>Back to the central topic, with this vector \(k \in \mathbb{R}^{n(n-1)/2}\), we could likewise simply form a linear map \(H\) which transforms \(\mathbb{R}^{n(n-1)/2}\) into \(\mathbb{R}^{n^2}\). Specifically,</p> \[m = Hk\] <p>where \(H \in \mathbb{R}^{n^2 \times n(n-1)/2}\), and \(m \in \mathbb{R}^{n^2}\). There’s no unique way to design \(H\); one simple method might be the following:</p> \[H = \begin{bmatrix} I_{n(n-1)/2} \\ 0_{n \times n(n-1)/2} \\ -I_{n(n-1)/2} \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; \cdots &amp; 1 \\ 0 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; \cdots &amp; 0 \\ -1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; -1 &amp; \cdots &amp; 0 \\ \vdots &amp; 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; \cdots &amp; -1 \\ \end{bmatrix}\] <p>In a nutshell, \(k\) encodes the lower/upper triangle of \(M_{skew}\) and \(H\) can be hand-coded (not shown here, but sparse and has only \(n(n-1)/2\) positions of 1 and \(n(n-1)/2\) positions of -1).</p> <h4 id="final-closed-form-optimization">Final closed-form optimization</h4> <p>Notice that now we finally see the benefit of the vector format: \(Hk\) returns a vector of shape \(\mathbb{R}^{n^2}\) which equivalently represents \(M_{skew} \in \not \mathbb{S}^{n \times n}\). Consequently, the original constrained optimzation problem</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>is eventually rewritten as</p> \[k^{*} = \argmin_{k \in \mathbb{R}^{n(n-1)/2}} ||\dot{x} - \tilde{X}Hk||_{2}\] <p>Notice that this is similar in form to the OLS problem, and can thus be written in closed form:</p> \[k^{*} = (\tilde{X}H)\backslash \dot{x}\] <p>and eventually</p> \[M^*_{skew} = R(Hk^*)\] <p>where \(R\) is a reshaping map from \(\mathbb{R}^{n^2}\) to \(\mathbb{R}^{n \times n}\).</p> <p>The authors also briefly discussed a few other implementation techniques, including solving the constrained optimization problem using a gradient descent based method. However, since this optimization problem has a unique global optimum, they should return the same results.</p> <h2 id="discussions">Discussions</h2> <h3 id="what-does-it-represent-to-decompose-the-dynamics">What does it represent to decompose the dynamics?</h3> <p>With \(M^{*}_{skew}\), the authors could find out the corresponding eigenvectors and eigenvalues and do projections accordingly. Since skew-symmetric matrices only have pure imaginary eigenvalues and the eigenvectors come in <strong>orthogonal, complex conjugate</strong> pairs (thus could think about the decomposition of \(M_{skew}^{*}\) as extracting orthogonal planes), the real projection planes \({u_{i, 1}, u_{i, 2}}\) are calculated from the complex conjugate vector pairs \({v_{i, 1}, v_{i, 2}}\) (with the corresponding eigenvalues \(\pm \beta_i\)) by \(u_{i, 1} = v_{i, 1} + v_{i, 2}, \; u_{i, 1} = j(v_{i, 1} - v_{i, 2})\) with normalization (magnitude 1). Why do we eigendecompose the summary matrix (either \(\Sigma\) or \(M\))? Or more specifically, what do these eigenvectors/eigenvalues inform us? Notice that the eigenvector/eigenvalue decomposition logics are different here: For PCA, eigenvectors of the covariance matrix \(\Sigma\) serve as principal components of the underlying data (geometric view: axes of ellipsoid as mentioned above); For jPCA, the eigenvectors stipulate the axes/subspace along which the solutions of the underlying dynamical system show rotational patterns. Projections onto jPC’s with the largest magnitude of the corresponding eigenvalues present the most significant rotation: higher frequency and more consistency (also shown in Supplemantary Video 4).</p> <p>Theoretically, this results from the fact that the magnitude of the pure imaginary eigenvalues encodes the frequency of oscillation (rotation speed). In our case here, since each conjugate pair \(\pm \beta_i\) corresponds to a 2D <strong>invariant subspace</strong> in which the flow is a rotation at frequency \(\beta_i\), the 6D dynamics are essentially a <strong>direct sum</strong> of 2D oscillatory patterns, each rotating at its own frequency. The real basis (\(u_{i, 1}, u_{i, 2}\)) is formed using the above calculation (why would the above calculation make sense? A question for readers), and they form an real 2D invariant subspace where the projection exhibits a circle/ellipse trajectory at the frequency \(\beta_i\). Moreover, there’s no expansion nor contraction and the magnitude of \(\beta_i\) indicates the <strong>oscillation frequency/rotation speed</strong>. Why does the magnitude of eigenvalues encode rotational frequency? This can be seen from the solution to the linear dynamical system.</p> <p>Let me give a very simple exmaple (perhaps the simplest in this case): Assume for our LDS: \(\dot{x}(t) = Ax(t), \; x(t) \in \mathbb{R}^2\), where</p> \[A = \begin{bmatrix} 0 &amp; \omega \\ -\omega &amp; 0 \end{bmatrix}\] <p>with \(\omega &gt; 0\). The eigenvalues are easily calculated to be \(\lambda = \pm i\omega\), and thus the solution is</p> \[x(t) = e^{At}x(0)\] <p>where</p> \[e^{At} = \begin{bmatrix} \cos(\omega t) &amp; -\sin(\omega t) \\ \sin(\omega t) &amp; \cos(\omega t) \end{bmatrix}\] <p>Consequently, it’s obvious that \(e^{At}\) is a rotation matrix, applied on \(x(0)\) with pure rotation with angular speed \(\omega\). Notice that in the above I also utilize the fact that the general solution for LDS entails matrix exponential. I’m currently also writing another blog on exponential maps not only on its application in linear differential equations, but also in differential geometry/Lie group. It turns out to be much more fundamental than how contrived it might appear at first glane. A fantastic object.</p> <p>Now let me also show the solution for a general LDS with pure imaginary eigenvalues [TODO].</p> <h3 id="quasi-similarity-micro-vs-macro--explainers">Quasi-similarity: micro-vs macro- explainers</h3> <p>One caveat from this paper is that the extracted neural trajectory is only “quasi-oscillatory”, not always a complete rotation: the oscillation usually exists for barely 1 cycle. It’s a little unclear how the activities would evolve if more than \(200ms\) of neural population activities, recorded outside of this structured delay paradigm, are projected using jPCA. At the same time, for many neurons, the preparatory and movement-related single neuron responses do exhibit some quasi-oscillations which last for about 1 cycle (See in below).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_2.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 2 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. Trace colors are according to the strength of preparatory activities (red with greatest preparotory response). It allows clear view of the evolution of preparatory patterns during the movement. </div> <p>I mentioned the word <strong>emergence</strong> at the beginning a little bit. I don’t think anyone ever doubts that the population level behavior is composed of, or resulting from, micro-level patterns from units in the population. The key question is what is the <strong>correct</strong> first level of explanation for the corresponding object of the selected problem. Will the microstates encode the “fundamental truth”, if any, or the emergent structures provide the most fundamental explanation. You probably can tell that I’m being very vague here. Actually, this debate, or the gradual shift from reductionism to emergence, exists in many fields nowadays across computational/systems/cognitive neuroscience, LLM from deep learning, and of course physics, etc. In this paper the authors did not give a clear explanation of how such single unit level multiphasic response produces the observed rotation structure at the population level. However, it was indeed observed that the jPCA projection onto each axis does exhibits similar patterns as single-neuron response(shown below)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_1.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 1 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. The projections onto jPC's are similar to single neuron response. </div> <p>The quasi-similarities (I think this might nicely capture the fuzzy feeling here) of signal patterns through time (shown by the 2 figures above) in the population level and the single-neuron level could be explained by the fact that jPC’s capture the most conscipuous patterns in the data among the recorded neural population, or that each jPCA projection is a weighted sum/average of individual-neuron resopnses. Likewise, each single neuron could also be interpreted as a combination of all the jPC projected patterns, except that the intertwined relationship is not obvious if just viewed from a single neuron or a pattern itself.</p> <h3 id="multiphasic-response-and-phase-difference">Multiphasic response and phase difference</h3> <p>A multiphasic response means that a single neuron’s activity might rise and fall (or exhibit multiple peaks and troughs) over time during a reaching movement. Indeed, from the above figures we could observe that motor cortex neurons often exhibit these complex, multiphasic temporal patterns. From there we might argue that perhaps a sufficient condition for the rotation structures is that neurons exhibit multiphasic responses. However, this turns out to be not necessarily true. Such complexity at the level of individual neurons, while intriguing, doesn’t automatically translate into coherent rotational dynamics at the level of the population. The authors also tested on a “complex-kinematic model” and recorded EMG. Though both carry multi-phasic reponses, they do not show clear rotation patterns.</p> <p>In the end, as the authors argued, multiphasic responses are not enough: we also need the multi-phasic patterns to have consistent phases for around 90 degrees apart. This is an intriguing math argument. Indeed, for the population to trace out a rotational trajectory, at least 2 temporal patterns need to be phase-shifted by <strong>90 degrees</strong> (if there’s amplitude change then the trajectory should be an ellipse) with the same frequency (if different frequencies or time-dependent frequencies, might be more complex). Observed rotation implies the existence of a complex eigenvector pair and a 2D subspace whose coordinates are quadrature components (\(90\) degrees apart) with common frequency after appropriate linear transform/projection, since such 2D subspace might not show itself in high D space. On the contrary, wwo components in \(90\) degrees phase lag with the common frequency produce an elliptical rotation in their plane (circle if with the same amplitude). If frequency is different or with drifting phase/amplitude, a clean rotation need not appear. In reality, among high dimensional noise and condition-dependent variance, even a clean rotation might be burried in non-oscillatory trajectories.</p> <h3 id="from-oscillation-to-non-oscillation">From oscillation to non-oscillation</h3> <p>Another core question is how such rotational pattern generates non-oscillatory movement patterns, e.g. reaching. The authors conducted some further simulation analyses and demonstrated that it’s possible to reconstruct EMG (deltoid) activities from weighted sum of rotations with different magnitude and phases (they call this the generator-model). It’s interesting to see this because EMG does not acquire the latent rotational patterns, yet it could be reconstructed from rotational dynamics. Further quantification analyses also showed that only the neural data and generator-model exhibit rotational patterns, whereas two other models (velocity-tuned, complex-kinematic models) do not. Since EMG showed only weak rotation, as the authors put, this further illustrates that the latent rotation dynamics do not necessarily result from a muli-phasic reponse, but how such response is constructed.</p> <h3 id="speed-vs-amplitude">Speed vs. amplitude</h3> <p>One interesting finding is that the speed of movements is not encoded as the speed of the latent trajectories (they have similar angular velocity). The amplitude of rotation dictates different movement speeds (for example faster or slower reaches). As Churchland et al. argued, this could be nicely explained since rotations with larger amplitude represent more strongly multi-phasic responses, with which EMG frequently entails larger accelerations.</p> <h3 id="the-effec-of-cross-condition-mean">The effec of cross-condition mean</h3> <p>The rotational structure is dependent upon the initial states (dynamical systems view) specified by the prepratory activities, which captures the condition-dependent activities (here condition refers to experiment conditions). Indeed, to clearly observe such differences, the cross-condition mean is substracted first for each single neuron before PCA/jPCA. The authors also illustrated the effects of keeping the cross-condition mean, as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_11.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 11 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. </div> <p>The goal of mean subtraction is to preserve only data distinguished in different conditions. In panel a. and b., each trace represents a trial-averaged firing rate for one (experiment) condition, with the same coloring scheme indicating the strength of preparatory activities. The yellow trace indicates the cross-condition mean for that neuron. Panel b. substracts the mean (yellow line being 0 all along) while panel a. keeps it. If without substraction of cross-condition mean, the first jPCA plane would preserve condition-independent curved trajectories. The authors argued that this is not surprising, since many neurons exhibit similar behaviors across different conditions, which are thus naturally captured by PCA projections. Since jPCA is extracting patterns from PCA space, it’s expected that such patterns would be kept. Interestingly, even in panel c., these trajectories still carry some curvature, though it’s difficult to pinpoint how it’s theoretically generated. Panel d. displays the jPCA projection onto the second jPC plane (\(jPC3_, \; jPC_4\)) and we do see similar rotational structures dependent upon initial states specified by the preparatory activities. Consequently, to always explore the condition-dependent dynamics, the authors argued to substract cross-condition mean first before PCA/jPCA.</p> <h3 id="skew-symmetric-constraint-prior-or-posterior">Skew-symmetric constraint: prior or posterior</h3> <p>The authors stressed that because they are interested in only in rotational linear dynamical systems, the original least square problem is solved under constrained optimization. However, I’m a little confused why not fitting a general linear dynamical system and then extract out the <strong>unique</strong> skew-symmetric component, onto which the neural data could be projected to observe the rotational effects. In this manner, the fitted \(M^{*}\) instead of \(M_{skew}^{*}\) might capture more subtle dynamics and would not distort the inherent dynamical components.</p> <h2 id="conclusions">Conclusions</h2> <p>In summary,</p> <blockquote> <p>1] M1 reaching behavior supports the dynamical systems perspective</p> <p>2] Rotation is a ubiquitous phenomenon, and does not relate to movement curvature and orientation</p> <p>2] Preparatory activities encode the initial conditions for the underlying dynamical system</p> </blockquote> <p>The shift of views from single neuron analysis in pursuit of movement/behavior correlates/representations, to a dynamical system analysis at the neural population level, is becoming an increasingly populat domain. As we will see later, the other side of the same story, geometry instead of dynamics, will quickly come into the arena under the name of neural manifold. This notion, with both the aboundance of aesthetic elegance and disatisfying lack of mathematical rigor from either topology or differential geometry, will be an eternal theme for the following research, projects, and blogs of mine.</p> <h2 id="open-querries">Open Querries</h2> <p>1] The annotomical circuitry that leads to the dynamical system/latent trajectory is still unclear.</p> <p>2] What happens after the 200ms? As shown in Supp Movie 3 below, how would the observed rotations evolve later in time (notice that all the rotations exist for merely 1~1.5 cylces)? Do they end up in the same positions (or just same in this subspace)?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_3.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 3 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>, showing the evolution of the projected states. </div> <p>3] Although in the Discussions section, I briefly talked about the reason for existence of the rotational structures, The problem still remains as to how individual oscillations of different phases and amplitudes would result in a consistent rotation.</p> <p>4] Inspired from the jPCA algorithm, apart from \(\Sigma\) (covariance of demeaned data) or \(M^*\), is there other way of capturing the data statistics from different perspectives?</p> <h2 id="extensions">Extensions</h2> <h3 id="hermitian-unitary-and-normal-matrices">Hermitian, unitary, and normal matrices</h3> <p>Churchland and colleagues put the following at the end of the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf" rel="external nofollow noopener" target="_blank">Supplementary Information</a>:</p> <blockquote> <p>“The convenience of being able to eigendecompose a summary matrix and yield orthogonal vectors belongs to the class of normal matrices, which by definition are diagonalizable by a unitary matrix. The class of normal matrices includes symmetric and skew-symmetric matrices, among others. This fact suggests a broader class of PCA variants that are a subject of future work.”</p> </blockquote> <p>In order to understand the inner workings of matrix decomposition, I’d like to review first the following definitions.</p> <blockquote> <p><em>Def</em>: The Hermitian tranpose (or conjugate transpose) of a matrix \(A\) is denoted as \(A^{\dagger} = \bar{A}^{T}\), which is tranpose of the complex conjugate (applied to each entry). Equivalent notations are \(A^H, \; A^{*}\). Notice that for a real matrix \(A \in \mathbb{R}^{n \times n}, \; A^{\dagger} = A^{T}\).</p> <p><em>Def</em>: A matrix is called <strong>Hermitian</strong> (or <strong>self-adjoint</strong>) if \(A = A^{\dagger}\). Notice that if A is a real <strong>Hermitian</strong> matrix, then it is equivalently <strong>symmetric</strong>.</p> <p>. <em>Def</em>: A matrix \(U\) is called <strong>unitary</strong> if \(U^{\dagger}U = I\) Notice that a real <strong>unitary</strong> matrix is equivalently <strong>orthogonal</strong>.</p> </blockquote> <p>We also have the following theorems for the above special matrices (more discussions seen in this <a href="https://www.math.purdue.edu/~eremenko/dvi/lect3.26.pdf" rel="external nofollow noopener" target="_blank">note</a>):</p> <blockquote> <p><em>Spectral theorem</em> for Hermitian matrices: For a <strong>Hermitian</strong> matrix,</p> <p>(i) all eigenvalues are real,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>and likewise</p> <blockquote> <p><em>Spectral theorem</em> for unitary matrices: For a <strong>unitary</strong> matrix,</p> <p>(i) all eigenvalues have magnitude 1,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>Consequently, Hermitian and unitary matrices are always <strong>diagonalizable</strong> (indeed some eigenvalues might be the same). Notice that eigenvectors of any matrix corresponding to distinct eigenvalues are linearly independent. Here with Hermitian and unitary matrices they are not only linearly indepedent, but also <strong>orthogonal</strong>.</p> <p>Theorems (i) for both Hermitian and unitary matrices could be proven separately. Since (ii) and (iii) are the same, we might be thinking about looking for a greater class of matrices which include these two types, while carrying more general properties, entailing (ii) and (iii). Let me introduce the following:</p> <blockquote> <p><em>Def</em>: A normal matrix is a matrix that commutes with its adjoint: \([A, A^{\dagger}] = 0\) where [B, C] = BC - CB</p> </blockquote> <p>Note that Hermitian and unitary matrices are special cases of a normal matrix. <strong>Skew-Hermitian</strong> (\(A^{\dagger} = -A\); in real case, skew-symmetric) matrices are also normal matrices. Also, the previous definitions will be helpful for the next extension into Lie group/algebra. For normal matrices we have the following:</p> <blockquote> <p><em>Spectral theorem</em> for normal matrices: A matrix is <strong>normal</strong> <em>if and only if</em> there is an orthogonal basis consisting of eigenvectors.</p> </blockquote> <p>The above is equivalent to saying that a normal matrix \(A\) is <strong>unitarily diagonalizable</strong>:</p> \[A = U\Lambda U^{\dagger}\] <p>where \(U\) is unitary (orthogonal if \(A\) is real), and \(\Lambda\) is diagonal. From the above theorem we could easily see that a symmetric matrix \(A\) has all real eigenvalues (becuase it’s a Hermitian matrix). Consequently, it has the decomposition:</p> \[A = B \Lambda B^{-1}\] <p>where \(\Lambda\) is a real diagonal matrix, B orthogonal.</p> <p>The speicality of the spectral theorem on normal matrices is that for non-normal matrices, you might still have eigenvectors, but they don’t form an orthogonal basis (sometimes they don’t even span the whole space). For example, \(A = \begin{bmatrix}1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}\), which has identical eigenvector so even not a full basis. Normal matrices are exactly those that can be diagonalized by an orthogonal (or unitary) change of basis, with nice orthogonal eigenvectors spanning the whole space.</p> <p>There’s also a famous fact between Hermitian and unitary matrix:</p> <blockquote> <p>There exists a 1-1 correponsdence between the set of unitary matrices \(U\) and the exponental of the set of Hermitian matrices \(H\), i.e.:</p> \[U = exp(iH)\] </blockquote> <p>The proof is not hard and not displayed here. The message it encodes is important: the unitary matrices are exactly in the format of the exponential map (same name as in LDS, different meaning, but could be interpreted similarly; see the future blog) of Hermitian matrices.</p> <p>Similarly, we could show the following for orthogonal matrices: Given \(A\) as a skew-symmetric (real) matrix (\(A^{\dagger} = A^{T} = -A\)), notice first that</p> \[-iA = iA^T = iA^{\dagger} = (-iA)^{\dagger}\] <p>Consequently, \(-iA\) is a Hermitian matrix and thus since</p> \[e^{A} = e^{i(-iA)} = e^{i(-iA)^{\dagger}}\] <p>Then \(e^{A}\) is a unitary matrix. Since \(A\) is real, \(e^{A}\) has to be real. Consequently, \(e^{A}\) is an orthogonal matrix. In other words, the exponential map of <strong>skew-symmetric</strong> matrices leads to an <strong>orthogonal</strong> matrix. However, this mapping does not result in all orthogonal matrices:</p> \[det(e^A) = e^{trA} = e^{0} = 1\] <p>which means that this mapping only characterizes rotation matrices (determinant equal to 1, unlike reflection which changes the orientation). Again, we arrive at the logic of fitting a skew-symmetric matrix for rotational dynamics, from pure matrix calculation (a little bit of Lie group/algebra flavor, but without resorting to the fact that the solution of LDS is matrix exponential mapping; Or maybe there’s a deeper connection?).</p> <h3 id="linear-transformation-and-decomposition">Linear transformation and decomposition</h3> <p>Notice that within jPCA, the authors made the following decomposition: \(M = M_{symm} + M_{skew}\), which naturally dissects the linear dynamics into expansions/contractions and rotations. I’m wondering whether similar decomposition can be made for general linear transformation (not necessarily in the linear dynamical systems framework) \(A \in \mathbb{R}^{k \times k}\) for \(y = Ax, \; x,y \in \mathbb{R}^{k}\).</p> <p>Again, any real square matrix \(A\) can be uniquely decomposed into:</p> \[A = S + K\] <p>where \(S = (A + A^T)/2\) is symmetric, and \(K = (A - A^T)/2\) is skew-symmetric. Consequently, \(y = Ax = (S + K)x = Sx + Kx\). Let’s interpret each component separately.</p> <p>For the symmetric part \(Sx\), based on the above <strong>spctral theorem for Hermitian matrices</strong>, because \(S\) is diagonalizable with real eigenvalues and orthogonal eigenvectors, its effect is to scale space (stretch or compress) along mutually orthogonal directions. An example will be to turning a circle into an ellipse (pure stretching).</p> <p>For the skew-symmetric part \(Kx\), it represents the “rotation-like” part of the transformation. In fact, \(\forall x, \; x^T(Kx) = 0\) (simple proof). Consequently, \(Kx\) is always orthogonal to \(x\). While \(K\) teleports \(x\) to orthogonal positions, it might also scale it (according to its matrix elements). A simple example below: the 2D skew-symmetric matrix is of the form:</p> \[\begin{bmatrix} 0 &amp; -\alpha \\ \alpha &amp; 0 \end{bmatrix}\] <p>if \(\alpha \neq 0\), then alongside the orthogonal teleportation there’s indeed some scaling. A deeper interpretation of the decomposed skew-symmetric matrix here is still to relate it with the exponential map and rotation matrix: unsurprisingly, the exact reason it’s associated with “rotation” is that the matrix exponential of a skew-symmetric matrix \(e^{tK}\) is always a rotation. Although I proved this statement above, we could also draw some intuition if we begin with some infinitesimal analysis:</p> <p>If we look at rotation by a small angle \(\theta\) (still in 2D):</p> \[R(\theta) \approx I + K\theta\] <p>where</p> \[K = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}\] <p>Here \(R(\theta)\) is finite rotation, and \(K\) is the infinitesimal generator: the first-order linear part of the rotation near identity (Lie algebra?). This connection is exact through the matrix exponential:</p> \[R(\theta) = e^{\theta K}\] <p>For any skew-symmetric \(K\), \(e^{\theta K}\) is a true rotation matrix, and that’s why we call \(K\) a generator: by exponentiating it we recover a roataion matrix. If we take the first order Tyler expansion on \(e^{\theta K}\) (really, just truncate the first two terms \(I, K\theta\) based on the matrix exponential definition), we get the exact approximation of \(R(\theta)\) as above. In the end, \(K\) by itself is not a rotation (since it’s not orthogonal), but it encodes the infinitesimal direction of change: skew-symmetric matrices are the infinitestimal generators of rotations.</p> <p>Geometrically, we could also think about rotate a vector in a small time step \(\Delta t\):</p> \[v(t + \Delta t) \approx v(t) + \Delta Kv(t)\] <p>where \(Kv(t)\) is perpendicular to \(v(t)\), thus encoding a vector tangent to the movement trajectory (thus a circle \(\rightarrow\) rotation).</p> <p>Then a next question is: how about \(M \in \mathbb{R}^{k \times l}, k \neq l\)? This definitely cannot be a dissection into symmetric and skew-symmetric matrices because this general \(M\) is not a square matrix at all. Moreover, if we want to extend to non-square matrix, we are absolutely out of the realm of dynamical systems. How do we characterize a general linear transformation?</p> <h3 id="cartan-decomposition-of-lie-algebra">Cartan decomposition of Lie algebra</h3> <h3 id="relations-with-matrix-lie-group-and-lie-algebra">Relations with Matrix Lie group and Lie algebra</h3> <p>Since we are talking about symmetric and skew-symmetric matrices, they are famously linked in Lie group/algebra in the following way:</p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Li, Ruixiang (Aug 2025). Rotational dynamics in neural population. https://jasmineruixiang.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2025rotational-dynamics-in-neural-population</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Rotational dynamics in neural population}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Li, Ruixiang}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Aug}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://jasmineruixiang.github.io/blog/2025/jPCA/}</span>
<span class="p">}</span>
</code></pre></div></div> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="prep_review" class="col-sm-8"> <div class="title">Preparatory activity and the expansive nullspace</div> <div class="author"> Krishna V Shenoy Mark M Churchland </div> <div class="periodical"> <em>Nature Reviews Neuroscience</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">prep_review</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Preparatory activity and the expansive nullspace}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mark M Churchland, Krishna V Shenoy}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Reviews Neuroscience}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{25}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{213--236}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1038/s41583-024-00796-z}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="russo" class="col-sm-8"> <div class="title">Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation</div> <div class="author"> </div> <div class="periodical"> <em>Neuron</em>, Aug 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">russo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neuron}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{107}</span><span class="p">,</span>
  <span class="na">issue</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{745--758}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.neuron.2020.05.020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="nullspace" class="col-sm-8"> <div class="title">Cortical activity in the null space: permitting preparation without movement</div> <div class="author"> </div> <div class="periodical"> <em>Nature Neuroscience</em>, Feb 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nullspace</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cortical activity in the null space: permitting preparation without movement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature Neuroscience}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{17}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{440--448}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1038/nn.3643}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="jpca" class="col-sm-8"> <div class="title">Neural population dynamics during reaching</div> <div class="author"> </div> <div class="periodical"> <em>Nature</em>, Jul 2012 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">jpca</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural population dynamics during reaching}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Nature}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{487}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{51--56}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1038/nature11129}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Constraint-Learning/">Learning and constraints (in progress)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Exponential-map/">Geom/Topo/Dynam Mumble (1): Exponential map (in progress)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Manifold(1)/">Differentiable Manifold (1): Rigorous definition (in progress)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Equivalence/">Equivalence: What does "being equal" represent? (in progress)</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ruixiang Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>