<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="setup">Setup</h2> <p>Suppose we compute PCA on two datasets (e.g., train vs.\ test), and we keep the top-$r$ principal components. Let</p> \[U_{\text{train}} \in \mathbb{R}^{d \times r}, \qquad U_{\text{test}} \in \mathbb{R}^{d \times r},\] <p>where each matrix has <strong>orthonormal columns</strong> (so each is a basis for an $r$-dimensional subspace of $\mathbb{R}^d$).</p> <p>Our first obvious intuition might be to define the <strong>subspace overlap matrix</strong> as the following:</p> \[S = U_{\text{train}}^{\top} U_{\text{test}} \in \mathbb{R}^{r \times r}.\] <p>So, each entry is a dot product between basis vectors:</p> \[S_{ij} = u_i^{(\text{train})} \cdot u_j^{(\text{test})}.\] <p>At first glance, this looks like a direct “basis alignment” comparison, which is exactly what we aim for. How else could we characterize subspace change other than looking at pairs-wise relationships among two sets of basis vectors? Well, there’re a few caveats…</p> <hr> <h2 id="1-why-basis-by-basis-alignment-is-unstable">1] Why basis-by-basis alignment is unstable</h2> <p>Comparing PCA vectors one-by-one (e.g., “PC1 vs.\ PC1”) is unstable because PCA eigenvectors are <strong>not uniquely defined</strong> in two common cases. The following problems might pop up —</p> <h3 id="i-sign-flips">(i) Sign flips</h3> <p>If \(u\) is an eigenvector, then \(-u\) is also an eigenvector.</p> <p>So dot products can flip sign even when the <em>subspace is identical</em>.</p> <h3 id="ii-degenerate--near-degenerate-eigenvalues-rotation-inside-the-subspace">(ii) Degenerate / near-degenerate eigenvalues (rotation inside the subspace)</h3> <p>If</p> \[\lambda_i \approx \lambda_{i+1},\] <p>then the corresponding principal directions inside the 2D span can rotate dramatically under tiny perturbations (noise, finite-sample effects, etc.).</p> <p>This means that even if the <em>span</em> is essentially the same, the individual vectors $u_i$ can change a lot.<br> So comparing “PC1 to PC1” is not meaningful.</p> <hr> <h2 id="2-what-singular-values-do-that-dot-products-dont">2] What singular values do that dot products don’t</h2> <p>The matrix</p> \[S = U_{\text{train}}^{\top} U_{\text{test}}\] <p>depends on the <em>chosen bases</em> inside each subspace. If we change bases within either subspace via orthogonal transformations:</p> \[U_{\text{train}} \to U_{\text{train}} R_1, \qquad U_{\text{test}} \to U_{\text{test}} R_2,\] <p>where $R_1, R_2 \in \mathbb{R}^{r \times r}$ are orthogonal (including sign flips as a special case), then</p> \[S \to (U_{\text{train}}R_1)^{\top}(U_{\text{test}}R_2) = R_1^{\top} S R_2.\] <p>So the <em>entries</em> of $S$ can change wildly.</p> <h3 id="key-fact-invariance">Key fact (invariance)</h3> <blockquote> <p>The <strong>singular values</strong> of $S$ are invariant under left/right orthogonal rotations:</p> </blockquote> <ul> <li>Left-multiplying by an orthogonal matrix does not change singular values.</li> <li>Right-multiplying by an orthogonal matrix does not change singular values.</li> </ul> <p>Therefore, even if PCA “relabels,” flips signs, or rotates the basis vectors within the subspace, the <strong>singular values remain unchanged</strong>.</p> <p>This means singular values capture a property of the <strong>subspaces</strong>, not of the particular eigenvectors chosen.</p> <hr> <h2 id="3-geometric-meaning-principal-angles">3] Geometric meaning: principal angles</h2> <p>Take the SVD:</p> \[S = Q \Sigma R^{\top},\] <p>where</p> \[\Sigma = \mathrm{diag}(\sigma_1,\dots,\sigma_r).\] <p>A fundamental result is:</p> \[\sigma_i = \cos(\theta_i),\] <p>where $\theta_i$ are the <strong>principal angles</strong> between the two $r$-dimensional subspaces.</p> <p>Interpretation:</p> <ul> <li>$\theta_i = 0 \implies$ perfectly aligned direction exists (since $\cos(\theta_i)=1$)</li> <li>$\theta_i = 90^\circ \implies$ orthogonal direction (since $\cos(\theta_i)=0$)</li> </ul> <p>So the singular values summarize <em>how much overlap</em> the two subspaces have along their best-aligned directions.</p> <hr> <h2 id="4-why-this-is-the-stable-comparison">4] Why this is the “stable” comparison</h2> <p>Think of $U_{\text{train}}$ and $U_{\text{test}}$ as <strong>arbitrary coordinate systems</strong> inside their respective subspaces.</p> <p>A meaningful comparison should ignore that arbitrariness.</p> <p>Principal angles / singular values do exactly this: they compute the <strong>best possible matching</strong> between directions in the two subspaces.</p> <p>Instead of comparing “PC1 $\leftrightarrow$ PC1,” we solve an optimal alignment problem:</p> \[\max_{\|a\|=\|b\|=1} a^{\top}\bigl(U_{\text{train}}^{\top}U_{\text{test}}\bigr)b,\] <p>and the sequence of best matches yields</p> \[\sigma_1,\sigma_2,\dots\] <p>as the strengths of alignment along the best-aligned directions.</p> <hr> <h2 id="5-tiny-example-intuition-2d-case">5] Tiny example intuition (2D case)</h2> <p>Suppose both subspaces are actually the same 2D plane in $\mathbb{R}^d$.</p> <p>You could pick:</p> <ul> <li>$U_{\text{train}}$ = standard basis in that plane</li> <li>$U_{\text{test}}$ = same plane but rotated by $45^\circ$ inside it</li> </ul> <p>Then $S$ might look like a rotation matrix:</p> \[S= \begin{pmatrix} \cos 45^\circ &amp; -\sin 45^\circ \\ \sin 45^\circ &amp; \cos 45^\circ \end{pmatrix}.\] <p>The entries are not the identity, so basis-by-basis dot products look “not aligned.”</p> <p>But the singular values of a rotation matrix are both $1$.</p> <p>So singular values correctly say: <strong>the subspaces are identical</strong>.</p> <p>That’s the whole point.</p> <hr> <h2 id="bottom-line">Bottom line</h2> <p>We use singular values of</p> \[U_{\text{train}}^{\top}U_{\text{test}}\] <p>because:</p> <ul> <li>✅ they are invariant to sign flips / rotations / re-ordering of PCA vectors inside the subspace</li> <li>✅ they define principal angles, which are a true subspace-to-subspace comparison</li> <li>✅ they give a stable measure of drift even when eigenvectors are not uniquely defined</li> </ul> <hr> <h2 id="a-sidenote-connection-to-projection-distance">A sidenote: Connection to projection distance</h2> <p>Let $P_{\text{train}}$ and $P_{\text{test}}$ be the orthogonal projection matrices onto the two subspaces:</p> \[P_{\text{train}} = U_{\text{train}}U_{\text{train}}^{\top}, \qquad P_{\text{test}} = U_{\text{test}}U_{\text{test}}^{\top}.\] <p>Then one can show the Frobenius-distance relationship:</p> \[\|P_{\text{train}} - P_{\text{test}}\|_F^2 = 2r - 2\|U_{\text{train}}^{\top}U_{\text{test}}\|_F^2 = 2\sum_{i=1}^r \sin^2(\theta_i).\] </body></html>