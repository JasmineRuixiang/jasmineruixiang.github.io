<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring | Ruixiang Li </title> <meta name="author" content="Ruixiang Li"> <meta name="description" content="Zscoring, block vs session level comparisons"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?1bba76160cd50d1600475ac5ca76b1a1"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jasmineruixiang.github.io/blog/2026/zscore/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ruixiang</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/PhiarLitSic/">PhiArLitSic </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring</h1> <p class="post-meta"> Created in February 06, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/brain-computer-interface"> <i class="fa-solid fa-hashtag fa-sm"></i> Brain Computer Interface</a>     ·   <a href="/blog/category/brain-computer-interface"> <i class="fa-solid fa-tag fa-sm"></i> Brain-Computer Interface</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#1-problem-setup">1] Problem setup</a></li> <li class="toc-entry toc-h2"><a href="#2-notation-for-a-single-feature">2] Notation for a single feature</a></li> <li class="toc-entry toc-h2"> <a href="#3-method-a-block-wise-z-scoring-concatenate-and-then-global-z-scoring">3] Method A: Block-wise z-scoring, concatenate, and then global z-scoring</a> <ul> <li class="toc-entry toc-h3"><a href="#step-3a-z-score-within-each-block">Step 3a]: Z-score within each block</a></li> <li class="toc-entry toc-h3"> <a href="#step-3b-concatenate-all-normalized-blocks">Step 3b]: Concatenate all normalized blocks</a> <ul> <li class="toc-entry toc-h4"><a href="#global-mean">Global mean</a></li> <li class="toc-entry toc-h4"><a href="#global-variance">Global variance</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#step-3c-second-z-scoring">Step 3c]: Second z-scoring?</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#4-method-b-concatenate-first-then-global-z-scoring">4] Method B: Concatenate first, then global z-scoring</a> <ul> <li class="toc-entry toc-h3"><a href="#step-4a-concatenate-raw-data">Step 4a]: Concatenate raw data</a></li> <li class="toc-entry toc-h3"><a href="#step-4b-compute-global-mean-and-standard-deviation">Step 4b]: Compute global mean and standard deviation</a></li> <li class="toc-entry toc-h3"><a href="#step-4c-compute-global-variance">Step 4c]: Compute global variance</a></li> <li class="toc-entry toc-h3"><a href="#step-4d-global-z-scoring">Step 4d]: Global z-scoring</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#comparison-of-method-a-and-method-b">Comparison of Method A and Method B</a></li> <li class="toc-entry toc-h2"><a href="#final-result">Final result</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>This short blog provides a detailed, self-contained computational analysis of whether two z-scoring procedures applied to block-structured neural data are equivalent, and if indeed different how.</p> <hr> <h2 id="1-problem-setup">1] Problem setup</h2> <p>Let’s say that we have neural data organized into blocks:</p> <ul> <li>Number of blocks: \(B\)</li> <li>Each block has data matrix in shape: \((x_{i, j}^b) \in \mathbb{R}^{t \times n}, \quad b = 1, \dots, B\)</li> <li>\(t\) = number of time points (samples)</li> <li>\(n\) = number of neural features</li> </ul> <p>Z-scoring is performed <strong>feature-wise</strong>, so all derivations below consider <strong>one fixed feature</strong> (column) at a time. The argument applies independently to every feature.</p> <hr> <h2 id="2-notation-for-a-single-feature">2] Notation for a single feature</h2> <p>For a fixed feature \(j\):</p> <ul> <li> <p>Let \(x_{i,j}^{b} \in \mathbb{R}^{1}\) denote the data at time \(i\) for the feature \(j\) in block \(b\). For the following paragraphs, I will simplify \(x_{:,j}^{b}\) into \(x_{j}^{b} \in \mathbb{R}^{t\times 1}\), and \(x_{i,:}^{b}\) into \(x_{i}^{b} \in \mathbb{R}^{1\times n}\). Naturally, \(x^b = (x_{ij}^b) \in \mathbb{R}^{t\times n}\) with the same shape for all blocks. Basically, \(i\) corresponds to the index of time \(t\), and \(j\) the index of the number of neurons \(n\).</p> </li> <li> <p>Block-wise mean: \(\mu_{j}^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i, j} \in \mathbb{R}^{1}\),</p> <p>\(\mu ^{b} = [\mu_{1}^{b}, \cdots, \mu_{n}^{b}] \in \mathbb{R}^{1 \times n}\).</p> <p>Or, we could simply write \(\mu ^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i} \in \mathbb{R}^{1 \times n}\)</p> </li> <li> <p>Block-wise variance: \((\sigma_{j}^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i, j} - \mu_{j}^b)^2 \in \mathbb{R}^{1}\),</p> <p>\((\sigma ^{b})^2 = [(\sigma_{1}^{b})^2, \cdots, (\sigma_{n}^{b})^2] \in \mathbb{R}^{1 \times n}\),</p> <p>Or, we could simplify the above as \((\sigma ^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i} - \mu^b) \odot (x^{b}_{i} - \mu^b) \in \mathbb{R}^{1\times n}\)</p> <p>where \(\odot\) is the Hadamard product between two vectors (or just elementwise multiplication), defined as \(x \odot y = diag(x)y = (x_i y_i)_i \in \mathbb{R}^{1\times n},\) where \(x,y \in \mathbb{R}^{1\times n}\)</p> </li> </ul> <hr> <h2 id="3-method-a-block-wise-z-scoring-concatenate-and-then-global-z-scoring">3] Method A: Block-wise z-scoring, concatenate, and then global z-scoring</h2> <h3 id="step-3a-z-score-within-each-block">Step 3a]: Z-score within each block</h3> <p>Each block is normalized independently:</p> \[z^{b}_{i} = \frac{x^{b}_i - \mu^b}{\sigma^b} \in \mathbb{R}^{1\times n}\] <p>Notice that this is element-wise division (to not over-complicate the symbols, I’ll use this abuse of notation for the following).</p> <p>By construction, for every block \(b\):</p> \[\frac{1}{t}\sum_{i=1}^t z^{b}_{i} = \vec{0} \in \mathbb{R}^{1\times n}, \qquad \frac{1}{t}\sum_{i=1}^t (z^{b}_{i} - \vec{0}) \odot (z^{b}_{i} - \vec{0}) = \vec{1} \in \mathbb{R}^{1\times n},\] <p>Consequently, each feature in each block has mean 0 and standard deviation 1. \(z^b\) observes the same notation rule as I described above for \(x^b\).</p> <hr> <h3 id="step-3b-concatenate-all-normalized-blocks">Step 3b]: Concatenate all normalized blocks</h3> <p>Concatenate all \(z^{b}\) into a single vector of length \(Bt\).</p> <h4 id="global-mean">Global mean</h4> \[\frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B \vec{0} = \vec{0} \in \mathbb{R}^{1\times n}\] <h4 id="global-variance">Global variance</h4> \[\frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) \\ = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) = \frac{1}{B}\sum_{b=1}^B\vec{1} = \vec{1} \in \mathbb{R}^{1\times n}\] <hr> <h3 id="step-3c-second-z-scoring">Step 3c]: Second z-scoring?</h3> <p>Since the concatenated data already has zero mean and unit variance, adding any other layers of z-scoring has no effect.</p> <p><strong>Final output of Method A:</strong></p> \[\boxed{z^{b} \in \mathbb{R}^{t\times n}}\] <p>for each block. So method A simply returns the block-wise standardized data.</p> <hr> <h2 id="4-method-b-concatenate-first-then-global-z-scoring">4] Method B: Concatenate first, then global z-scoring</h2> <h3 id="step-4a-concatenate-raw-data">Step 4a]: Concatenate raw data</h3> <p>Concatenate all blocks \(x^{b}\) into a single matrix \(X \in \mathbb{R}^{Bt \times n}\).</p> <hr> <h3 id="step-4b-compute-global-mean-and-standard-deviation">Step 4b]: Compute global mean and standard deviation</h3> \[\mu = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t x^{b}_i = \frac{1}{B}\sum_{b=1}^B \frac{1}{t}\sum_{i=1}^{t}x_i^b \\ = \frac{1}{B}\sum_{b = 1}^{B} \mu^b \in \mathbb{R}^{1\times n}\] <p>The global mean is the average of block-wise means (it’s not hard to show that if each block has different samples, this average will become <em>weighted average</em> by the ratio of the amount of each block’s data to total data amount).</p> <hr> <h3 id="step-4c-compute-global-variance">Step 4c]: Compute global variance</h3> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (x^{b}_i - \mu) \odot (x^{b}_i - \mu) \in \mathbb{R}^{1 \times n}\] <p>Expand the product term:</p> \[(x^{b}_i - \mu) \odot (x^{b}_i - \mu) \\ = (x^{b}_i - \mu^b + \mu^b - \mu) \odot (x^{b}_i - \mu^b + \mu^b - \mu) \\ = (x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + 2(x^{b}_i - \mu^b) \odot (\mu^b - \mu) \\ + (\mu^b - \mu) \odot (\mu^b - \mu) \\\] <p>Let’s look at each of the terms more closely. Notice that when summing over \(i\) in \(\sigma^2\), the cross term vanishes:</p> \[\frac{1}{t}\sum_{i=1}^t ((x^{b}_i - \mu^b) \odot (\mu^b - \mu))\\ = (\frac{1}{t}\sum_{i=1}^t (x^{b}_i - \mu^b)) \odot (\mu^b - \mu)\\ = (\mu^b - \mu^b) \odot (\mu^b - \mu) \\ = \vec{0}\] <p>Thus,</p> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t((x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + (\mu^b - \mu) \odot (\mu^b - \mu)) \\ = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (\mu^b - \mu) \odot (\mu^b - \mu) \\ = \frac{1}{B}\sum_{1}^{B}\frac{1}{t}\sum_{i=1}^{t}(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\\ = \frac{1}{B}\sum_{1}^{B}(\sigma^b)^2 + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\] <p>This is a <strong>variance decomposition</strong> into:</p> <ul> <li>average within-block variance</li> <li>variance of block means (between-block variance)</li> </ul> <p>which meets our intuitive expectation (what else could it be anyway…).</p> <hr> <h3 id="step-4d-global-z-scoring">Step 4d]: Global z-scoring</h3> <p>Each sample is normalized as:</p> \[y^{b}_i = \frac{x^{b}_i - \mu}{\sigma} \in \mathbb{R}^{1\times n}\] <p>By the nature of z-scoring, if we now calculate the global mean and standard deviation on concatenated \(y^b\), as in 3b] for each feature, we will obtain 0 and 1 respectively.</p> <p>Now if we rewrite the above using block-wise z-scores, since</p> <p>\(z_i^b = \frac{x_i^b - \mu^b}{\sigma^b} \rightarrow x_i^b = \sigma^bz_i^b + \mu^b\), then:</p> \[\boxed{ y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} }\] <p>which reinforces the idea that these all are just linear transformations: Transformations being linear, linear into one another.</p> <hr> <h2 id="comparison-of-method-a-and-method-b">Comparison of Method A and Method B</h2> <p>Method A output:</p> \[z^{b}_i \in \mathbb{R}^{1\times n},\; \forall i, b\] <p>Method B output:</p> \[y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} \in \mathbb{R}^{1\times n}, \; \forall i, b\] <p>For the two methods to be identical \(\forall b,i\), we must have:</p> \[\sigma_b = \sigma \quad \text{and} \quad \mu_b = \mu \quad \forall b\] <p>And again, if we concatenate all \(y^b\) and \(z^b\) together separately into \(Y, Z\), they <strong>BOTH</strong> have feature-wise 0 mean and std 1.</p> <hr> <h2 id="final-result">Final result</h2> \[\boxed{ \begin{array}{l} \text{The two procedures are NOT equivalent in general, even though} \\ \text{both yield global mean } 0 \text{ and std } 1 \text{ after transformations}. \end{array} }\] <p>They are equivalent <strong>if and only if</strong> every block already has identical feature-wise means and variances.</p> <ul> <li> <strong>Method A</strong> removes all block-level mean and variance differences before concatenation.</li> <li> <strong>Method B</strong> preserves block-level differences and normalizes relative to the pooled distribution.</li> </ul> <p>Block-wise z-scoring and global z-scoring <strong>do not commute</strong>. . These choices encode different assumptions about whether block identity (e.g., session, subject, condition) should be preserved or discarded. Our choice should be driven by whether block-to-block variability is meaningful signal or nuisance variability in your analysis.</p> <p>Fun quesitons:</p> <ul> <li>1] What if block size \(t\) is not the same across all blocks?</li> <li>2] What are other (useful/effective) ways of normalization which also return a fixed mean/std (0/1, e.g.)?</li> </ul> <p>Practical question: In practice, how much do the statistics from these two methods actually differ? How should we interpret such differences?</p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Li, Ruixiang (Feb 2026). High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring. https://jasmineruixiang.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2026high-dimensional-nitty-gritty-1-equivalence-or-lack-thereof-between-block-wise-and-global-z-scoring</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Li, Ruixiang}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Feb}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://jasmineruixiang.github.io/blog/2026/zscore/}</span>
<span class="p">}</span>
</code></pre></div></div> <h2>References</h2> <div class="publications"> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Constraint-Learning/">Learning and Constraints (in progress)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jPCA/">Rotational Dynamics in Neural Population</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/covariance/">High Dimensional Nitty-gritty (2): Z-scoring before PCA?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/geodesics/">Geom/Topo/Dynam Mumble(4): from Geodesics Strip to Gauss Theorem Egregium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Manifold(1)/">Manifold and Riemannian Geometry (1): Rigorous Definition (in progress)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/Manifold(3)/">Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Manifold(2)/">Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ruixiang Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>