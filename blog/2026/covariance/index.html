<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> High Dimensional Nitty-gritty (2): Z-scoring before PCA? | Ruixiang Li </title> <meta name="author" content="Ruixiang Li"> <meta name="description" content="Zscoring, covariance, PCA"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.png?1bba76160cd50d1600475ac5ca76b1a1"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jasmineruixiang.github.io/blog/2026/covariance/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ruixiang</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/PhiarLitSic/">PhiArLitSic </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">High Dimensional Nitty-gritty (2): Z-scoring before PCA?</h1> <p class="post-meta"> Created in February 09, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/brain-computer-interface"> <i class="fa-solid fa-hashtag fa-sm"></i> Brain Computer Interface</a>     ·   <a href="/blog/category/brain-computer-interface"> <i class="fa-solid fa-tag fa-sm"></i> Brain-Computer Interface</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"><a href="#0-problem-setup">0] Problem Setup</a></li> <li class="toc-entry toc-h3"><a href="#1-what-pca-actually-does">1] What PCA actually does</a></li> <li class="toc-entry toc-h3"><a href="#2-what-does-z-scoring-do">2] What does z-scoring do?</a></li> <li class="toc-entry toc-h3"> <a href="#3-a-geometric-way-to-think-about-this">3] A geometric way to think about this</a> <ul> <li class="toc-entry toc-h4"><a href="#31-pca-as-a-variational-problem">3.1] PCA as a Variational Problem</a></li> <li class="toc-entry toc-h4"><a href="#32-pca-after-z-scoring">3.2] PCA After Z-Scoring</a></li> <li class="toc-entry toc-h4"><a href="#33-equivalent-reformulation-changing-the-constraint">3.3] Equivalent Reformulation (Changing the Constraint)</a></li> <li class="toc-entry toc-h4"><a href="#34-geometric-interpretation">3.4] Geometric Interpretation</a></li> <li class="toc-entry toc-h4"><a href="#35-multi-dimensional-pca-k-components">3.5] Multi-Dimensional PCA (k Components)</a></li> <li class="toc-entry toc-h4"><a href="#36-generalized-eigenvalue-interpretation">3.6] Generalized Eigenvalue Interpretation</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#4-what-geometry-are-we-preserving">4] What geometry are we preserving?</a></li> <li class="toc-entry toc-h3"><a href="#5-which-one-preserves-global-geometry">5] Which one preserves global geometry?</a></li> <li class="toc-entry toc-h3"><a href="#6-summary">6] Summary</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>This blog originates from a daily discussion of neural signal (pre)processing with my mentors and peers. People utilize z-scoring and PCA all the time, and it’s a little shameful to admit by hindsight that I haven’t dwelled on the following question deep enough. Again, we reencounter the conundrum in high dimensional observations haunted by irreducible noise, under which lies our ambitious intent to extract robust and effective information.</p> <hr> <h3 id="0-problem-setup">0] Problem Setup</h3> <p>Let’s say we have a collection of neural data in the format \(X \in \mathbb{R}^{n \times d}\), where \(n\) is the number of samples, and \(d\) the number of features. These are raw features each coming from a single electrode. For simplicity let’s assume that there is only 1 kind of feature, like threshold crossing. Let’s further say that we want to visualize the low dimensional structure of this collection of data, preserving its global geometry as much as possible (we’ll clarify this later).</p> <p>Now we’d like to apply PCA on it for the first try, meaning to transform from \(\mathbb{R}^{n \times d}\) into \(\mathbb{R}^{n \times d'}\), where \(d'\) might be just \(3\), for example. Many methods would start by z-scoring \(X\) for each feature (so for each column of \(X\), after z-scoring would be mean 0 and standard deviation 1; for more discussions please refer to the <a href="https://jasmineruixiang.github.io/blog/2026/zscore/">previous blog</a> of this series) before applying PCA.</p> <p>I’m a little unsure about the motivation behind it. There’re of course many but I cannot pinpoint a conclusive answer.</p> <p>More importantly, I’m naively concerned that if we apply this feature-wise normalization, whether that would still preserve the covariance matrix between the original features (the diagonal will be 1, but I’m wondering how the off-diagonal terms would possibly change, or … would they).</p> <p>Let’s begin.</p> <hr> <h3 id="1-what-pca-actually-does">1] What PCA actually does</h3> <p>The very first step: a quick review of what PCA is doing:</p> <blockquote> <p>Standard PCA:</p> <ol> <li>Center the data (subtracts column-wise/global means across samples; For the following, \(X\) will denote the centered data, if without specifications)</li> <li>Compute the covariance matrix: \(\Sigma = \frac{1}{n}X^TX\)</li> <li>Find eigenvectors of \(\Sigma\)</li> </ol> </blockquote> <p>PCA finds directions of maximal variance in the original coordinate system. We could also interpret PCA as finding minimization of reconstruction error, but that’s not explicitly helpful for deepening our interpretation here. However, I’ll provide another useful perspective in section 3] from the angle of constrained optimization, but this covariance interpretation is what we will grapple with now for this section —</p> <p>Because it already makes it obvious that</p> <blockquote> <p>PCA is sensitive to feature scale.</p> </blockquote> <p>If one electrode has variance 100 and another has variance 1, the first will dominate the principal components — even if its structure might not be more meaningful. And that’s exactly where z-scoring makes a huge distinction.</p> <hr> <h3 id="2-what-does-z-scoring-do">2] What does z-scoring do?</h3> <p>Column-wise z-scoring transforms</p> \[\tilde{X}_{ij} = \frac{X_{ij} - \mu_j}{\sigma_j}\] <p>(Here’s a bit of abuse of notation, as \(\tilde{X}_{ij}, X_{ij}\) are scalars while \(\mu_j, \sigma_j \in \mathbb{R}^{1\times d}\))</p> <p>If we use matrix notation, \(D = diag{(\sigma_1, \cdots, \sigma_n)}\), then the above could be simplified into (again, assume it’s already centered):</p> \[\tilde{X} = XD^{-1}\] <p>Now if we look at the new covariance matrix:</p> \[\tilde{\Sigma} = \frac{1}{n}\tilde{X}^T\tilde{X} = \frac{1}{n}D^{-1}X^TXD^{-1} = D^{-1}\Sigma D^{-1}\] <p>This is obviously <strong>not the same</strong> <code class="language-plaintext highlighter-rouge">covariance matrix</code>.</p> <p>In fact, it’s not hard to see that, from simple linear algebra:</p> \[\tilde{\Sigma}_{ij} = \frac{\Sigma_{ij}}{\sigma_i \sigma_j}\] <p>And this turns out to be exactly the <code class="language-plaintext highlighter-rouge">correlation matrix</code>.</p> <p>So:</p> <blockquote> <p>PCA on z-scored data = PCA on the correlation matrix PCA on raw centered data = PCA on the covariance matrix</p> </blockquote> <p>So back to one of our original questions: does normalization preserve covariance between original features?</p> <p>No. The off-diagonal terms change as:</p> \[\Sigma_{ij} \leftarrow \frac{\Sigma_{ij}}{\sigma_i \sigma_j}\] <p>So they become correlations (or we could say that the correlations are preserved). The important distinction is that <em>covariance</em> measures co-variation in physical units, while <em>correlation</em> measures co-variation relative to each variable’s scale. So z-scoring does not preserve the original covariance geometry: It preserves the <strong>correlation structure</strong> instead.</p> <p>But this leads to the next natural question, or the question —</p> <hr> <h3 id="3-a-geometric-way-to-think-about-this">3] A geometric way to think about this</h3> <p>The above covariance calculation is clear, yet we could reinterpret PCA in a different way by variational characterization of eigenvectors, i.e. the Rayleigh quotient formulation of PCA (depending on your views, these two migth be considered the exact same thing; but even as an explanation for why we care about eigenvectors of the covariance, let me explain below).</p> <h4 id="31-pca-as-a-variational-problem">3.1] PCA as a Variational Problem</h4> <p>Let \(X \in \mathbb{R}^{n \times d}\) be the centered data and the sample covariance matrix</p> \[\Sigma = \frac{1}{n} X^\top X\] <p>If we project the data onto a direction \(v \in \mathbb{R}^d\), the projected variance is:</p> \[\mathrm{Var}(Xv) = \frac{1}{n} \|Xv\|^2 = v^\top \Sigma v\] <p>Therefore, the first principal component solves:</p> \[\max_{\|v\| = 1} v^\top \Sigma v\] <p>This is the Rayleigh quotient, and the solution is the top eigenvector of \(\Sigma\) (not proved here; many other sources exist online).</p> <h4 id="32-pca-after-z-scoring">3.2] PCA After Z-Scoring</h4> <p>Suppose we z-score each feature (column-wise normalization). Again, let me reiterate from above that</p> \[D = \mathrm{diag}(\sigma_1, \dots, \sigma_d)\] <p>and the transformed data is:</p> \[\tilde{X} = X D^{-1}\] <p>The new covariance matrix becomes:</p> \[\tilde{\Sigma} = \frac{1}{n} \tilde{X}^\top \tilde{X} = D^{-1} \Sigma D^{-1}\] <p>PCA on z-scored data solves:</p> \[\max_{\|v\| = 1} v^\top D^{-1} \Sigma D^{-1} v\] <hr> <h4 id="33-equivalent-reformulation-changing-the-constraint">3.3] Equivalent Reformulation (Changing the Constraint)</h4> <p>Now let’s do a simple trick. Let:</p> \[w = D^{-1} v \quad \text{so that} \quad v = D w\] <p>Substitute into the objective:</p> \[v^\top D^{-1} \Sigma D^{-1} v = (Dw)^\top D^{-1} \Sigma D^{-1} (Dw) = w^\top \Sigma w\] <p>Now examine the constraint:</p> \[\|v\|^2 = 1\] \[v^\top v = (Dw)^\top (Dw) = w^\top D^2 w\] <p>So the optimization becomes:</p> \[\max_{w^\top D^2 w = 1} w^\top \Sigma w\] <hr> <h4 id="34-geometric-interpretation">3.4] Geometric Interpretation</h4> <p>The raw PCA solves:</p> \[\max_{v^\top v = 1} v^\top \Sigma v\] <p>Now, the Z-scored PCA solves:</p> \[\max_{w^\top D^2 w = 1} w^\top \Sigma w\] <p>So at a glance, z-scoring rescales the covariance matrix into the correlation matrix.</p> <p>But viewed from a different perspective, it <strong>changes the metric constraint</strong>. Instead of using the standard Euclidean norm:</p> \[v^\top v\] <p>we now use a weighted norm:</p> \[w^\top D^2 w\] <p>This means:</p> <ul> <li>Raw PCA assumes the standard Euclidean inner product.</li> <li>Z-scored PCA uses a different inner product induced by $D^2$.</li> </ul> <hr> <h4 id="35-multi-dimensional-pca-k-components">3.5] Multi-Dimensional PCA (k Components)</h4> <p>Up to this point, you might object that the above is just to find one single direction. Usually we do multiple components. Well, there isn’t too much effort for extension.</p> <p>Raw PCA solves:</p> \[\max_{V^\top V = I} \mathrm{Tr}(V^\top \Sigma V)\] <p>where $V \in \mathbb{R}^{d \times k}$. The solution is the top-\(k\) eigenvectors of \(\Sigma\). Compared with 4.3], this is a natural extension, and I’ll leave out the formal proof.</p> <p>After z-scoring, we solve:</p> \[\max_{V^\top V = I} \mathrm{Tr}(V^\top D^{-1} \Sigma D^{-1} V)\] <p>Using the substitution $V = D W$, this naturally becomes (similar to 4.3]):</p> \[\max_{W^\top D^2 W = I} \mathrm{Tr}(W^\top \Sigma W)\] <hr> <h4 id="36-generalized-eigenvalue-interpretation">3.6] Generalized Eigenvalue Interpretation</h4> <p>What does this mean geometrically?</p> <ul> <li>For the raw PCA: <ul> <li>Orthonormal basis in standard <strong>Euclidean</strong> metric</li> <li>Maximizes variance</li> </ul> </li> <li>For Z-scored PCA: <ul> <li>Orthonormal basis under <strong>weighted</strong> metric \(D^2\)</li> <li>This is equivalent to solving a <strong>generalized eigenvalue</strong> problem:</li> </ul> </li> </ul> \[\Sigma w = \lambda D^2 w\] <p>I’ll also skip the details of why the solution is equivalent to finding the generalized eigenvalues/eigenvectors. However, this fact informs us of the fundamental framework of PCA:</p> <p>the big geometric insight is that PCA always solves:</p> \[\max_{W^\top G W = I} \mathrm{Tr}(W^\top \Sigma W)\] <p>where \(G\) defines the metric.</p> <ul> <li>Raw PCA: \(G = I\)</li> <li>Z-scored PCA: \(G = D^2\)</li> </ul> <p>So z-scoring means that we are not trusting that Euclidean length in raw coordinates is meaningful. We redefine what unit length means.</p> <p>Therefore:</p> <ul> <li>Raw PCA preserves covariance geometry.</li> <li>Z-scored PCA preserves correlation geometry.</li> </ul> <hr> <h3 id="4-what-geometry-are-we-preserving">4] What geometry are we preserving?</h3> <p>To some extent, this is the real conceptual issue.</p> <p>If we do PCA without z-scoring, it preserves the Euclidean geometry in the original feature space. Variance magnitude is meaningful because we indeed keep such information. We’d hold the underlying premise that</p> <blockquote> <p>Electrodes with larger variance are considered more important.</p> </blockquote> <p>This suggests that this methood is good if variance magnitude reflects real neural signal strength or the feature scale is physically meaningful.</p> <p>On the other hand, if we do PCA after z-scoring, then it would preserves geometry under a reweighted metric and all dimensions are treated equally. Each electrode is thus given equal prior importance.</p> <p>This should work if variance differences are arbitrary (e.g., electrode gain differences) and we care about patterns of co-variation, not absolute magnitude.</p> <p>Since neural data often has:</p> <ul> <li>Different firing rates across electrodes</li> <li>Different noise levels</li> <li>Different dynamic ranges</li> </ul> <p>If we don’t z-score:</p> <blockquote> <p>High firing-rate neurons dominate PCA.</p> </blockquote> <p>whereas if we do z-score:</p> <blockquote> <p>Each neuron contributes equally in variance units.</p> </blockquote> <hr> <h3 id="5-which-one-preserves-global-geometry">5] Which one preserves global geometry?</h3> <p>This depends on what geometry we think is meaningful.</p> <p>If our raw space is: \(\mathbb{R}^d\) with standard Euclidean metric, then PCA without z-scoring preserves global geometry better.</p> <p>If we believe that true geometry should not depend on firing rate scale, then z-scoring defines a more appropriate metric (as elaborated in section 3):</p> \[&lt;x, y&gt;_D = x^T(D^{-1})^{2}y\] <p>which means we could alternatively interpret this as keeping the underlying space unchanged but essentially altering the metric before doing PCA.</p> <p>Usually in systems neuroscience people often z-score across time and then do PCA. The reason behind is that neural manifold studies often care about relative population patterns, not which neuron fires more</p> <p>If we want true population variance magnitude, then we should not z-score. If we intend to obtain population structure independent of scale, then z-score.</p> <hr> <h3 id="6-summary">6] Summary</h3> <p>In conclusion, if we do z-scoring before PCA, then</p> <ul> <li>Z-scoring does NOT preserve the covariance matrix.</li> <li>It converts covariance to correlation.</li> <li>Off-diagonal terms are divided by product of standard deviations.</li> <li>Equivalently, we are changing the metric of the space.</li> <li>PCA result can change dramatically depending on scaling.</li> </ul> <p>Finally, in practice we often have more than one kind of features. For example, we could obtain both threshold crossings and spike band power from each electrode at the same. However, these two measures have drastically different scales. In this scenario, of course we could look into them separately, but if combined, the spike power would dominate. Consequently, z-scoring also helps to re-weight the feature importance apriori.</p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Li, Ruixiang (Feb 2026). High Dimensional Nitty-gritty (2): Z-scoring before PCA?. https://jasmineruixiang.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2026high-dimensional-nitty-gritty-2-z-scoring-before-pca</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{High Dimensional Nitty-gritty (2): Z-scoring before PCA?}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Li, Ruixiang}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Feb}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://jasmineruixiang.github.io/blog/2026/covariance/}</span>
<span class="p">}</span>
</code></pre></div></div> <h2>References</h2> <div class="publications"> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Constraint-Learning/">Learning and Constraints (in progress)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jPCA/">Rotational Dynamics in Neural Population</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/geodesics/">Geom/Topo/Dynam Mumble(4): from Geodesics Strip to Gauss Theorem Egregium</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Manifold(1)/">Manifold and Riemannian Geometry (1): Rigorous Definition (in progress)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/Manifold(3)/">Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/zscore/">High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Manifold(2)/">Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ruixiang Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>