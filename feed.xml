<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jasmineruixiang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasmineruixiang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-09T06:22:53+00:00</updated><id>https://jasmineruixiang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">BCI nitty-gritty (2): Zscoring before PCA?</title><link href="https://jasmineruixiang.github.io/blog/2026/covariance/" rel="alternate" type="text/html" title="BCI nitty-gritty (2): Zscoring before PCA?"/><published>2026-02-09T01:22:40+00:00</published><updated>2026-02-09T01:22:40+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/covariance</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/covariance/"><![CDATA[<p>Zero vector-holer here</p> <p>Will add here soon.</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Brain Computer Interface"/><summary type="html"><![CDATA[Zscoring, covariance, PCA]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(4): from Geodesics Strip to Gauss Theorem Egregium</title><link href="https://jasmineruixiang.github.io/blog/2026/geodesics/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(4): from Geodesics Strip to Gauss Theorem Egregium"/><published>2026-02-08T14:58:02+00:00</published><updated>2026-02-08T14:58:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/geodesics</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/geodesics/"><![CDATA[<p>This weekend I was (re)thinking about the geometric connotations of geodesics, and that reminds me of a brilliant illustration with an intuitive method the eloquent mathematician Tristan Needham noted down in his remarkable and mind-numbing book <em>Visual Differential Geometry and Forms</em>. I thought just a little more, and only to recover a long-lasting misconception about Gauss Theorem Egregium which I was both shameful to admit for possessing so long and excited to have it cleared out of my mind.</p> <p>However, as I dive more into Needham‚Äôs method, I discovered a potential and simple failure mode, which naturally echoes another important statement: The Gauss-Bonnet Theorem. This deepens my grasp of the tension between <strong>local isometry</strong> (the peeling method, see below) and <strong>global topology</strong> (the area enclosed).</p> <p>Again, since this is the mumble series, I‚Äôll not give all definitions and assume you have already known some flavor of the basics. Let‚Äôs begin.</p> <hr/> <h2 id="0-the-problem-confusion-setup">0] The Problem (Confusion) Setup</h2> <p>On Page 12-13, Tristan introduced a method to easily and intuitively construct geodesics on a curved surface (see figure [1.11] below). He claimed that ‚ÄúIf a narrow strip surrounding a segment G of a geodesic is cut out of a surface and laid flat in the plane, then G becomes a segment of a straight line.‚Äù Well, he presented an intuitive proof (figure [1.12] below), but I‚Äôm immediately reminded of <strong>Gauss‚Äô Theorem Egregium</strong> and there seemed to be something inconsistent (I‚Äôll omit other information and count on you to look up the basics).</p> <p>I was thinking: the peeling is an (local) isometry without doubt, so geodesics have to be preserved. That‚Äôs why geodesics ‚Äúpeeled‚Äù off from the surface (the fruit/vegetable used in the illustration) has to be geodesics in Eucliean 2D space, which is straight according to the normal definitions. However, as I‚Äôm thinking a bit more deeply, the Gaussian curvature does change (before it‚Äôs nonzero, on 2D it‚Äôs zero, which contradicts Theorem Egregium), which leads to me to think backwards towards using this Theorem Egregium again. Immediately I realize that Gaussian theorem egregium is applied only to 2D surfaces (not to 1D curve since there‚Äôs no ‚ÄúGaussian curvature‚Äù for a curve). Ah, shame to have blundered upon conceptual confusion. But treating this as an opportunity for an upgrade overhaul of my conceptual framework, let‚Äôs sort this out step by step and see what we might also dabble into.</p> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/VDGF/VDGF_1.11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/VDGF/VDGF_1.12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig [1.11] and [1.12] in <a class="citation" href="#VDGF">(Needham, 2021)</a>. </div> <h2 id="1-the-peeling-intuition">1] The ‚ÄúPeeling‚Äù Intuition</h2> <p>Let‚Äôs make it clear: in Figure [1.11] of Needham‚Äôs text, he introduces the following powerful intuitive tool:</p> <blockquote> <p>‚ÄúIf a narrow strip surrounding a segment $G$ of a geodesic is cut out of a surface and laid flat in the plane, then $G$ becomes a segment of a straight line.‚Äù</p> </blockquote> <p>At first glance, this indeed seems to challenge Gauss‚Äôs <strong>Theorema Egregium</strong>. Since Gaussian curvature (\(K\)) is an intrinsic invariant, how can a patch of a curved surface (\(K \neq 0\)) be laid flat (\(K = 0\)) without stretching or tearing?</p> <p>Well, the resolution lies in the <strong>limit</strong>. Gaussian curvature is a property of a 2D area. By taking a ‚Äúnarrow strip,‚Äù we are effectively reducing the 2D surface to a 1D curve and its immediate neighborhood. In the limit, as the width of the strip goes to zero, the ‚Äúarea‚Äù of the surface being considered goes to zero. In other words, the Gaussian curvature of the surface does not change because we aren‚Äôt flattening the whole surface; we are only flattening an ‚Äúinfinitesimally‚Äù narrow strip.</p> <hr/> <h2 id="2-geodesic-curvature-vs-gaussian-curvature">2] Geodesic Curvature vs. Gaussian Curvature</h2> <p>The Theorema Egregium applies to <strong>isometries</strong>. While we cannot map a patch of a sphere to a plane isometrically, we can map a curve and its first-order neighborhood to a plane isometrically (often called ‚Äúdeveloping‚Äù the strip; or just developable).</p> <p>We could think of it this way:</p> <blockquote> <p>The Surface: Has intrinsic curvature \(K\). The Strip: Because it is ‚Äúinfinitesimally narrow,‚Äù the Gaussian curvature doesn‚Äôt ‚Äú<strong>trap</strong>‚Äù the strip. We aren‚Äôt forcing the 2D relationships across the strip to remain the same; we are only preserving the lengths along the geodesic \(G\).</p> </blockquote> <p>Consequently, the core of the questions (why such an intuition makes sense, beyond the simple proof shown in Fig.[1.12]) does not lie in Gauss‚Äôs Theorem, but in another concept: the geodesic curvature: The Theorema Egregium is about the surface (\(K\)), but the property of being a ‚Äústraight line‚Äù is about Geodesic Curvature (\(\kappa_g\)):</p> <blockquote> <p>Gaussian Curvature (\(K\)) reflects the property of the surface. Geodesic Curvature (\(\kappa_g\)): A property of a curve relative to the surface. It measures how much the curve bends within the surface.</p> </blockquote> <p>The ‚Äústraightness‚Äù of the peeled strip is governed by <strong>Geodesic Curvature ($\kappa_g$)</strong>, not Gaussian Curvature ($K$).</p> <ul> <li><strong>Gaussian Curvature ($K$):</strong> A property of the surface itself ($K = \kappa_1 \kappa_2$). It dictates whether a 2D patch can be flattened.</li> <li><strong>Geodesic Curvature ($\kappa_g$):</strong> A property of a curve <em>relative</em> to the surface. It measures how much the curve ‚Äúveers‚Äù to the left or right within the surface.</li> </ul> <p>Recall that a geodesic is defined as a curve where $\kappa_g = 0$ everywhere (Will do a another blog on the computation of geodesics from the perspective of modern (Riemannian) manifold). Because $\kappa_g$ is an intrinsic property (it can be measured by flatlanders like intelligent ants living on the surface), it must be preserved under isometry.</p> <p>With this, we could demystify the underpinning logic of the peel:</p> <ol> <li>The act of peeling the strip is a local isometry along the curve.</li> <li>The $\kappa_g$ of the curve on the surface was 0 (by definition of a geodesic).</li> <li>Therefore, the $\kappa_g$ of the curve on the flat plane must also be 0 .</li> <li>In the Euclidean plane, a curve with zero curvature is a straight line.</li> </ol> <p>Notice that the 3rd step above relies on the fact that isometry preserves intrinsic properties and here the geodesic curvature, but again this is NOT Gauss Theorem Egregium (same fundamental fact but applied to different objects).</p> <p>Gaussian curvature is not defined for a 1D curve. A curve only has curvature (how much it bends in space) and geodesic curvature (how much it bends relative to the surface it‚Äôs on). By narrowing the strip until the 2D ‚Äúsurface‚Äù nature of the paper effectively vanishes, we are bypassing the restriction of the Theorema Egregium regarding the 2D area, while retaining the intrinsic measurement of the curve‚Äôs straightness. We are essentially ‚Äúcheating‚Äù the theorem by reducing the 2D surface to a 1D line where the concept of Gaussian Curvature has no grip.</p> <p>The Theorem Egregium is a negative constraint (it tells us what we can‚Äôt do with a 2D patch), but the Geodesic Curvature ($\kappa_g$) is the positive proof. It yet emphasizes again that it‚Äôs important to separate the topological ‚Äúbudget‚Äù of the surface (the Gaussian Curvature) from the local behavior of the curve (the Geodesic Curvature).</p> <p>There‚Äôs only one last piece left to make the story rigorous.</p> <hr/> <h2 id="3-physical-approximation">3] Physical Approximation</h2> <p>Let me paraphrase again in another way. This narrow strip, in practice, since it cannot be infinitesimally narrow, by Theorem Egregium, <strong>CANNOT</strong> lie flat on the plane, but the more narrow we could get, the better aligned it is with respect to the plane. If it goes to limit, then Gaussian Theorem Egregium does not apply because this limit 1D curve is no longer constrained by this theorem.</p> <p>On the other hand, even in practice we assume that after peeling it ‚Äúis‚Äù flat, this appeal to Theorem Egregium still does not ‚Äúprove‚Äù the fact that once laid down the strip would become straight; we still need to assort to the preserved geodesic curvature, which is covered above.</p> <p>Now, back to the reality. In practice, a strip of finite width \(w\) cannot lie perfectly flat if \(K \neq 0\). The ‚Äúerror‚Äù or distortion required to flatten it is proportional to the area of the strip ($Area \approx Length \times w$). As $w \to 0$:</p> <ul> <li>The area vanishes.</li> <li>The constraint of the Theorema Egregium vanishes.</li> <li>The 1D ‚Äústraightness‚Äù remains perfectly preserved.</li> </ul> <p>Another way to view Needham‚Äôs trick is that the strip isn‚Äôt just being ‚Äúflattened‚Äù; it is being identified with the unrolled <strong>tangent developable</strong> of the geodesic. If we imagine a sequence of tangent planes along the geodesic, they form a ‚Äúribbon‚Äù that has zero Gaussian curvature (because it‚Äôs a developable surface, like a cylinder or a cone). Because this ribbon has \(K=0\), it can be laid perfectly flat in a plane without any distortion at all. Needham‚Äôs ‚Äúnarrow strip‚Äù is essentially a physical approximation of this developable ribbon.</p> <hr/> <h2 id="4-extension-of-the-original-trick-connection-to-gauss-bonnet">4] Extension of the original trick: Connection to Gauss-Bonnet</h2> <h3 id="41-failure-mode-of-a-strip-when-in-closed-loop">4.1] Failure mode of a strip when in closed loop</h3> <p>Now with the above puzzle cleared out, let‚Äôs think a little deeper into the next single, perhaps most natural topic: How about peeling off not just a single strip segment, but instead a strip which loops back into itself?</p> <p>Think about the physical reality of Needham‚Äôs experiment. If we cut a strip along a great circle (a closed geodesic) of a sphere:</p> <ul> <li>1] The Segment: If we cut just a small arc (say 30¬∞), the strip is essentially a tiny rectangle. Because it‚Äôs so narrow, the ‚Äúsurface tension‚Äù of the sphere‚Äôs curvature isn‚Äôt strong enough to prevent us from pressing it flat.</li> <li>2] The Loop: If we try to cut the entire great circle, we get a ‚Äúring‚Äù or a ‚Äúhoop.‚Äù On the sphere, this hoop has a specific circumference (\(C = 2\pi R\)).</li> <li>3] The Failure: If we try to lay that hoop perfectly flat on a table without stretching it, we‚Äôll find it impossible. To lie flat in Euclidean space as a circle, the relationship between its radius and circumference must be \(C = 2\pi r\). But on the sphere, the ‚Äúradius‚Äù (the distance from the pole to the equator) is an arc length. <strong>The geometry of the 2D area inside the hoop ‚Äúlocks‚Äù the hoop‚Äôs shape</strong>.</li> </ul> <p>Or maybe another perhaps more intuitive example ‚Äî</p> <blockquote> <p>The ‚ÄúPaper Cone‚Äù Analogy:</p> </blockquote> <p>Think of a paper cone (like a party hat).</p> <ul> <li> <p>1] Local Flattening: You can cut a narrow strip from the cone running from the tip to the base. You can lay this strip flat on the table perfectly. In fact, you can lay any part of the cone flat.</p> </li> <li> <p>2] Global Failure: But if you try to flatten the <em>entire</em> cone at once, you can‚Äôt. You have to make a cut. When you flatten it, the cut edges don‚Äôt meet; there is a angular gap.</p> </li> </ul> <p>Yet perhaps another thought experiment:</p> <blockquote> <p>The ‚ÄúTrain Track‚Äù Experiment</p> </blockquote> <p>Imagine the ‚Äúnarrow strip‚Äù as a set of flexible but straight train tracks.</p> <p>On the Sphere: You lay the tracks along the equator. They go all the way around and connect perfectly at the start.</p> <p>The ‚ÄúPeeling‚Äù (Transfer to Plane): Now you transfer these tracks to a flat Euclidean floor. Because the tracks are geodesics (straight), you must lay them down as a straight line on the floor. You keep laying them down, inch by inch.</p> <p>Now the Problem: On the floor, a straight line goes on forever. It never comes back to start.</p> <p>The Contradiction: To make the tracks close a loop on the floor, you would have to bend them (add Geodesic Curvature). But we know geodesics are straight!</p> <p>So, the ‚Äúnarrow strip‚Äù of a closed geodesic loop on a sphere becomes an infinite straight line on the plane. It loses its ‚Äúloop-ness‚Äù entirely.</p> <p>Later we will make it clear that this ‚Äúangular gap‚Äù is precisely what the Gauss-Bonnet Theorem calculates (\(\iint K dA\)). The curvature \(K\) inside the loop on the sphere is responsible for ‚Äúturning‚Äù the geometry so that it closes. The flat plane (\(K=0\)) lacks this ‚Äúturning power,‚Äù so the strip simply runs away in a straight line.</p> <p>But anyway, for now, in short: We can flatten a line because a line has no ‚Äúinside.‚Äù We cannot flatten a closed loop without accounting for the gap (which is what we will show later as the holonomy, or ‚Äúequivalently‚Äù integration of the Gaussian curvature of the area it encloses.)</p> <h3 id="42-the-formula-and-geodesic-loop">4.2] The formula and geodesic-loop</h3> <p>We resort to The Gauss-Bonnet Theorem, which almost fits in immediately, since it bridges the gap between the local straightness of the geodesic and the global curvature of the surface.</p> <p>Well, the Gauss-Bonnet Theorem is essentially a ‚Äúbudgeting‚Äù equation. It tells us exactly how much ‚Äústraightness‚Äù we have to give up to account for the curvature of the surface. Let‚Äôs see if we could intuitively see its effect from the closed loop peeling failure.</p> <p>Let me state the theorem:</p> <p>For a simply connected region \(R\) bounded by a curve \(C\), the theorem states:</p> \[\iint_R K \, dA + \oint_C \kappa_g \, ds + \sum \alpha_i = 2\pi\] <p>\(K\) is the Gaussian curvature, \(\kappa_g\) is the geodesic curvature, \(\alpha_i\) are the exterior angles at any corners.</p> <p>Moreover, if we create a closed loop (like a triangle) using only geodesic segments, then $\kappa_g = 0$ along the edges by definition. The middle term of the equation vanishes, leaving a direct relationship between the ‚Äúarea-integral of curvature‚Äù and how much the geodesics had to ‚Äúturn‚Äù at the corners to close the loop:</p> \[\iint_R K \, dA = 2\pi - \sum \alpha_i\] <p>Furthermore, if we use a smooth closed geodesic, there is no geodesic curvature (\(\kappa_g = 0\)) and also no corners (\(\sum \alpha_i = 0\)). The equation thus becomes:</p> \[\iint_R K \, dA = 2\pi\] <p>This tells us that for a smooth closed geodesic to exist, the total Gaussian curvature of the area it encloses must equal \(2\pi\) (the ‚Äúangle‚Äù of a full circle). This is why you can have a closed geodesic on a sphere (\(K &gt; 0\)), but you can never have a simple closed geodesic on a flat plane (\(K=0\)) or a saddle (\(K&lt;0\))‚Äîthe ‚Äúcurvature budget‚Äù doesn‚Äôt add up to \(2\pi\).</p> <h3 id="43-how-does-the-strip-fit-in">4.3] How does the ‚Äústrip‚Äù fit in?</h3> <p>Now, imagine applying Needham‚Äôs ‚Äúpeeling‚Äù trick to each side of a geodesic triangle.</p> <blockquote> <ol> <li>On the surface: We would have three ‚Äústraight‚Äù paths (geodesics) that enclose a region of curvature \(K\). Because of that \(K\), the interior angles sum to more than \(\pi\) (on a sphere).</li> <li><strong>The ‚ÄúPeeling‚Äù Conflict</strong>: If we tried to peel a ‚Äúnarrow strip‚Äù that followed the entire boundary of the triangle and lay it flat in one go, we would encounter a physical gap or an overlap where the ends meet.</li> </ol> </blockquote> <p>The Gauss-Bonnet Theorem effectively measures this ‚Äúgap.‚Äù The amount of Gaussian curvature ‚Äútrapped‚Äù inside the triangle is exactly equal to the ‚Äúholonomy‚Äù‚Äîthe amount a vector rotates when transported around that loop:</p> <h3 id="44-holonomy-parallel-transport-and-the-gap">4.4] Holonomy, Parallel Transport, and the ‚ÄúGap‚Äù</h3> <p>There‚Äôs a simple and intuitive relationship between holonomy, parallel transport, and exterior angles.</p> <p>Let‚Äôs start with a simple thought experiment about parallel transport (you might have seen this everywhere):</p> <p>Imagine walking along a geodesic triangle on a sphere, carrying a spear (a vector) pointing straight ahead.</p> <ul> <li>Along the edge: Because you are on a geodesic, you never turn your ‚Äústeering wheel.‚Äù The spear stays parallel to your path.</li> <li>At the corner: You stop and turn your body by an exterior angle (\(\alpha_i\)). You do not turn the spear; it still points where it was pointing.</li> <li>Back at the start: When you complete the loop, you compare the spear‚Äôs current direction to its starting direction. They won‚Äôt match. This net rotation of the vector after the full trip, the ‚Äúerror‚Äù in direction, is called the <strong>Holonomy (\(\Delta \theta\))</strong>.</li> </ul> <p>The above way of sliding a vector along a geodesic while keeping it ‚Äúparallel‚Äù is called <strong>Parallel Transport</strong>.</p> <p>At the same time, just by some simple calculation we would know that for the total turn on a flat plane, our total change in heading (sum of exterior angles \(\sum \alpha_i\)) must be \(2\pi\) <em>to close a loop</em>. In other words, the holonomy \(\Delta \theta = 0\). However, on a curved surface, the amount we actually turned, the holonomy, is \(2\pi - \sum \alpha_i \neq 0\).</p> <p>Also notice that if we do parallel transport along the ‚Äúpeeled‚Äù flat strip, the vector remains parallel in the Euclidean sense because the strip is a straight line.</p> <p>However, when we close the loop,</p> <ul> <li>On the flat plane, the vector would return to its start pointing in the original direction.</li> <li>On the curved surface, the vector returns rotated by an angle \(\Delta \theta\).</li> </ul> <p>The theorem tells us that this holonomy $\Delta \theta$ is precisely the integral of the Gaussian curvature over the area we bypassed:</p> \[\Delta \theta = 2\pi - \sum\alpha_i = \iint_R K \, dA\] <p>Intuition Check: If you are on a flat plane, \(K=0\). Therefore, \(\iint K dA = 0\). This means \(0 = 2\pi - \sum \alpha_i\), or \(\sum \alpha_i = 2\pi\). This is just some high-school geometry rule that the exterior angles of any polygon sum to 360¬∞. On a sphere, the curvature \(K\) ‚Äúhelps‚Äù us turn, so we don‚Äôt need as much ‚Äúexterior angle‚Äù to close the loop.</p> <p>Consequently, if we apply Needham‚Äôs trick, i.e., peel off the strip of geodesic loop, we would find that</p> <blockquote> <p>While we can peel a <strong>single</strong> geodesic segment and lay it <strong>flat</strong> perfectly, we <strong>cannot</strong> peel a <strong>closed loop</strong> of geodesics and lay the resulting ‚Äúframe‚Äù flat in the plane without a gap or overlap.</p> </blockquote> <p>The ‚Äúangle‚Äù of that gap is the <strong>holonomy</strong>. The Gauss-Bonnet theorem tells us that this gap is exactly equal to the total Gaussian curvature ‚Äútrapped‚Äù inside the area we just cut out.</p> <ul> <li><strong>On a Sphere ($K&gt;0$):</strong> The geodesics turn ‚Äútoward‚Äù each other, and the interior angles sum to $&gt;\pi$.</li> <li><strong>On a Saddle ($K&lt;0$):</strong> The geodesics flare ‚Äúaway‚Äù from each other, and the interior angles sum to $&lt;\pi$.</li> </ul> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Needham‚Äôs trick works because a 1D line has no ‚Äúarea‚Äù to trap curvature. The ‚Äústraightness‚Äù we see on the paper is the physical manifestation of zero geodesic curvature, an intrinsic property that survives the transition from the fruit‚Äôs skin to the flat desk.</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Manifold"/><category term="Differential Geometry"/><summary type="html"><![CDATA[Geodescis and its construction, Egregium, Gauss-Bonnet and Chern]]></summary></entry><entry><title type="html">BCI nitty-gritty (1): Equivalence (or Lack Thereof) between Block-wise and Global Z-Scoring</title><link href="https://jasmineruixiang.github.io/blog/2026/zscore/" rel="alternate" type="text/html" title="BCI nitty-gritty (1): Equivalence (or Lack Thereof) between Block-wise and Global Z-Scoring"/><published>2026-02-06T11:16:09+00:00</published><updated>2026-02-06T11:16:09+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/zscore</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/zscore/"><![CDATA[<p>This short blog provides a detailed, self-contained computational analysis of whether two z-scoring procedures applied to block-structured neural data are equivalent, and if different how.</p> <hr/> <h2 id="1-problem-setup">1] Problem setup</h2> <p>Let‚Äôs say that we have neural data organized into blocks:</p> <ul> <li>Number of blocks: \(B\)</li> <li>Each block has data matrix in shape: \((x_{i, j}^b) \in \mathbb{R}^{t \times n}, \quad b = 1, \dots, B\)</li> <li>\(t\) = number of time points (samples)</li> <li>\(n\) = number of neural features</li> </ul> <p>Z-scoring is performed <strong>feature-wise</strong>, so all derivations below consider <strong>one fixed feature</strong> (column) at a time. The argument applies independently to every feature.</p> <hr/> <h2 id="2-notation-for-a-single-feature">2] Notation for a single feature</h2> <p>For a fixed feature \(j\):</p> <ul> <li> <p>Let \(x_{i,j}^{b} \in \mathbb{R}^{1}\) denote the data at time \(i\) for the feature \(j\) in block \(b\). For the following paragraphs, I will simplify \(x_{:,j}^{b}\) into \(x_{j}^{b} \in \mathbb{R}^{t\times 1}\), and \(x_{i,:}^{b}\) into \(x_{i}^{b} \in \mathbb{R}^{1\times n}\). Naturally, \(x^b \in \mathbb{R}^{t\times n}\) The same for other data matrices. Basically, \(i\) corresponds to the time \(t\), and \(j\) to the number of neurons \(n\).</p> </li> <li> <p>Block-wise mean: \(\mu_{j}^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i, j} \in \mathbb{R}^{1}\) \(\mu ^{b} = [\mu_{1}^{b}, \cdots, \mu_{n}^{b}] \in \mathbb{R}^{1 \times n}\). Or, we could simply write \(\mu ^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i} \in \mathbb{R}^{1}\)</p> </li> <li> <p>Block-wise variance: \((\sigma_{j}^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i, j} - \mu_{j}^b)^2 \in \mathbb{R}^{1}\) \((\sigma ^{b})^2 = [(\sigma_{1}^{b})^2, \cdots, (\sigma_{n}^{b})^2] \in \mathbb{R}^{1 \times n}\) Or, we could simplify the above as \((\sigma ^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i} - \mu^b) \odot (x^{b}_{i} - \mu^b) \in \mathbb{R}^{1}\) where \(\odot\) is the Hadamard product between two vectors (or just elementwise multiplication), defined as \(x \odot y = diag(x)y = (x_i y_i)_i \in \mathbb{R}^{1\times n},\) where \(x,y \in \mathbb{R}^{1\times n}\)</p> </li> </ul> <hr/> <h2 id="3-method-a-block-wise-z-scoring-concatenate-and-then-global-z-scoring">3] Method A: Block-wise z-scoring, concatenate, and then global z-scoring</h2> <h3 id="step-3a-z-score-within-each-block">Step 3a]: Z-score within each block</h3> <p>Each block is normalized independently:</p> \[z^{b}_{i} = \frac{x^{b}_i - \mu^b}{\sigma^b} \in \mathbb{R}^{1\times n}\] <p>Notice that this is element-wise division (to not complicate the symbols, I‚Äôll use this abuse of notation for the following).</p> <p>By construction, for every block \(b\):</p> \[\frac{1}{t}\sum_{i=1}^t z^{b}_{i} = \vec{0} \in \mathbb{R}^{1\times n}, \qquad \frac{1}{t}\sum_{i=1}^t &lt;z^{b}_{i} - \vec{0}, z^{b}_{i} - \vec{0}&gt; = \vec{1} \in \mathbb{R}^{1\times n},\] <p>Consequently, each feature in each block has mean 0 and standard deviation 1. Notice that \(z^b\) observes the same notation rule as I described above.</p> <hr/> <h3 id="step-3b-concatenate-all-normalized-blocks">Step 3b]: Concatenate all normalized blocks</h3> <p>Concatenate all \(z^{b}\) into a single vector of length \(Bt\).</p> <h4 id="global-mean">Global mean</h4> \[\mu' = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B \vec{0} = \vec{0} \in \mathbb{R}^{1\times n}\] <h4 id="global-variance">Global variance</h4> \[(\sigma')^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t &lt;z^{b}_i - \vec{0}, z^{b}_i - \vec{0}&gt; \\ = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) = \frac{1}{B}\sum_{b=1}^B\vec{1} = \vec{1} \in \mathbb{R}^{1\times n}\] <hr/> <h3 id="step-3c-second-z-scoring">Step 3c]: Second z-scoring?</h3> <p>Since the concatenated data already has zero mean and unit variance, adding any other layers of z-scoring has no effect.</p> <p><strong>Final output of Method A:</strong></p> \[\boxed{z^{b} \in \mathbb{R}^{t\times n}}\] <p>for each block. So method A simply returns the block-wise standardized data.</p> <hr/> <h2 id="4-method-b-concatenate-first-then-global-z-scoring">4] Method B: Concatenate first, then global z-scoring</h2> <h3 id="step-4a-concatenate-raw-data">Step 4a]: Concatenate raw data</h3> <p>Concatenate all blocks \(x^{b}\) into a single matrix \(X \in \mathbb{R}^{Bt \times n}\).</p> <hr/> <h3 id="step-4b-compute-global-mean-and-standard-deviation">Step 4b]: Compute global mean and standard deviation</h3> \[\mu = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t x^{b}_i = \frac{1}{B}\sum_{b=1}^B \frac{1}{t}\sum_{i=1}^{t}x_i^b \\ = \frac{1}{B}\sum_{b = 1}^{B} \mu^b\] <p>The global mean is the average of block-wise means (it‚Äôs not hard to show that if each block has different samples, this average will become <em>weighted average</em> by the ratio of the amount of each block‚Äôs data to total data amount).</p> <hr/> <h3 id="step-4c-compute-global-variance">Step 4c]: Compute global variance</h3> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (x^{b}_i - \mu) \odot (x^{b}_i - \mu)\] <p>Expand the product term:</p> \[(x^{b}_i - \mu) \odot (x^{b}_i - \mu) \\ = (x^{b}_i - \mu^b + \mu^b - \mu) \odot (x^{b}_i - \mu^b + \mu^b - \mu) \\ = (x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + 2(x^{b}_i - \mu^b) \odot (\mu^b - \mu) \\ + (\mu^b - \mu) \odot (\mu^b - \mu) \\\] <p>Let‚Äôs see into each of the terms. Notice that when summing over \(i\) in \(\sigma^2\), the cross term vanishes:</p> \[\frac{1}{t}\sum_{i=1}^t ((x^{b}_i - \mu^b) \odot (\mu^b - \mu))\\ = (\frac{1}{t}\sum_{i=1}^t (x^{b}_i - \mu^b)) \odot (\mu^b - \mu)\\ = (\mu^b - \mu^b) \odot (\mu^b - \mu) \\ = \vec{0}\] <p>Thus,</p> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t((x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + (\mu^b - \mu) \odot (\mu^b - \mu)) \\ = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (\mu^b - \mu) \odot (\mu^b - \mu) \\ = \frac{1}{B}\sum_{1}^{B}\frac{1}{t}\sum_{i=1}^{t}(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\\ = \frac{1}{B}\sum_{1}^{B}(\sigma^b)^2 + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\] <p>This is a <strong>variance decomposition</strong> into:</p> <ul> <li>average within-block variance</li> <li>variance of block means (between-block variance)</li> </ul> <p>which meets our intuitive expectation (what else could it be anyway‚Ä¶)</p> <hr/> <h3 id="step-4d-global-z-scoring">Step 4d]: Global z-scoring</h3> <p>Each sample is normalized as:</p> \[y^{b}_i = \frac{x^{b}_i - \mu}{\sigma} \in \mathbb{R}^{1\times n}\] <p>Now if we rewrite the above using block-wise z-scores, since</p> <p>\(z_i^b = \frac{x_i^b - \mu^b}{\sigma^b} \rightarrow x_i^b = \sigma^bz_i^b + \mu^b\), then:</p> \[\boxed{ y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} }\] <p>which reinforces the idea that these all are just linear transformations: Transformations being linear, linear into one another.</p> <hr/> <h2 id="comparison-of-method-a-and-method-b">Comparison of Method A and Method B</h2> <p>Method A output:</p> \[z^{b}_i \in \mathbb{R}^{1\times n}\] <p>Method B output:</p> \[y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} \in \mathbb{R}^{1\times n}\] <p>For the two methods to be identical for all $b,i$, we must have:</p> \[\sigma_b = \sigma \quad \text{and} \quad \mu_b = \mu \quad \forall b\] <p>On the other hand, notice that if we concatenate all \(y^b\) and \(z^b\) together separately into \(Y, Z\), they <strong>BOTH</strong> have feature-wise 0 mean and std 1.</p> <hr/> <h2 id="final-result">Final result</h2> \[\boxed{ \begin{array}{l} \text{The two procedures are NOT equivalent in general, even though} \\ \text{both yield global mean } 0 \text{ and std } 1 \text{ after transformations}. \end{array} }\] <p>They are equivalent <strong>if and only if</strong> every block already has identical feature-wise means and variances.</p> <ul> <li><strong>Method A</strong> removes all block-level mean and variance differences before concatenation.</li> <li><strong>Method B</strong> preserves block-level differences and normalizes relative to the pooled distribution.</li> </ul> <p>Block-wise z-scoring and global z-scoring <strong>do not commute</strong>. . These choices encode different assumptions about whether block identity (e.g., session, subject, condition) should be preserved or discarded. Our choice should be driven by whether block-to-block variability is meaningful signal or nuisance variability in your analysis.</p> <p>Fun quesitons:</p> <ul> <li>1] What if block size \(t\) is not the same across all blocks?</li> <li>2] What are other (useful/effective) ways of normalization which also return a fixed mean/std (0/1, e.g.)?</li> </ul> <p>Practical question: In practice, how much do the statistics from these two methods actually differ? How should we interpret such differences?</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Brain Computer Interface"/><summary type="html"><![CDATA[Zscoring, block vs session level comparisons]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(3): Subspace geometry</title><link href="https://jasmineruixiang.github.io/blog/2026/subspace/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(3): Subspace geometry"/><published>2026-02-03T21:49:38+00:00</published><updated>2026-02-03T21:49:38+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/subspace</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/subspace/"><![CDATA[<h2 id="setup">Setup</h2> <p>Suppose we compute PCA on two datasets (e.g., train vs.\ test), and we keep the top-$r$ principal components. Let</p> \[U_{\text{train}} \in \mathbb{R}^{d \times r}, \qquad U_{\text{test}} \in \mathbb{R}^{d \times r},\] <p>where each matrix has <strong>orthonormal columns</strong> (so each is a basis for an $r$-dimensional subspace of $\mathbb{R}^d$).</p> <p>Our first obvious intuition might be to define the <strong>subspace overlap matrix</strong> as the following:</p> \[S = U_{\text{train}}^{\top} U_{\text{test}} \in \mathbb{R}^{r \times r}.\] <p>So, each entry is a dot product between basis vectors:</p> \[S_{ij} = u_i^{(\text{train})} \cdot u_j^{(\text{test})}.\] <p>At first glance, this looks like a direct ‚Äúbasis alignment‚Äù comparison, which is exactly what we aim for. How else could we characterize subspace change other than looking at pairs-wise relationships among two sets of basis vectors? Well, there‚Äôre a few caveats‚Ä¶</p> <hr/> <h2 id="1-why-basis-by-basis-alignment-is-unstable">1] Why basis-by-basis alignment is unstable</h2> <p>Comparing PCA vectors one-by-one (e.g., ‚ÄúPC1 vs.\ PC1‚Äù) is unstable because PCA eigenvectors are <strong>not uniquely defined</strong> in two common cases. The following problems might pop up ‚Äî</p> <h3 id="i-sign-flips">(i) Sign flips</h3> <p>If \(u\) is an eigenvector, then \(-u\) is also an eigenvector.</p> <p>So dot products can flip sign even when the <em>subspace is identical</em>.</p> <h3 id="ii-degenerate--near-degenerate-eigenvalues-rotation-inside-the-subspace">(ii) Degenerate / near-degenerate eigenvalues (rotation inside the subspace)</h3> <p>If</p> \[\lambda_i \approx \lambda_{i+1},\] <p>then the corresponding principal directions inside the 2D span can rotate dramatically under tiny perturbations (noise, finite-sample effects, etc.).</p> <p>This means that even if the <em>span</em> is essentially the same, the individual vectors $u_i$ can change a lot.<br/> So comparing ‚ÄúPC1 to PC1‚Äù is not meaningful.</p> <hr/> <h2 id="2-what-singular-values-do-that-dot-products-dont">2] What singular values do that dot products don‚Äôt</h2> <p>The matrix</p> \[S = U_{\text{train}}^{\top} U_{\text{test}}\] <p>depends on the <em>chosen bases</em> inside each subspace. If we change bases within either subspace via orthogonal transformations:</p> \[U_{\text{train}} \to U_{\text{train}} R_1, \qquad U_{\text{test}} \to U_{\text{test}} R_2,\] <p>where $R_1, R_2 \in \mathbb{R}^{r \times r}$ are orthogonal (including sign flips as a special case), then</p> \[S \to (U_{\text{train}}R_1)^{\top}(U_{\text{test}}R_2) = R_1^{\top} S R_2.\] <p>So the <em>entries</em> of $S$ can change wildly.</p> <h3 id="key-fact-invariance">Key fact (invariance)</h3> <blockquote> <p>The <strong>singular values</strong> of \(S\) are invariant under left/right orthogonal rotations:</p> </blockquote> <ul> <li>Left-multiplying by an orthogonal matrix does not change singular values.</li> <li>Right-multiplying by an orthogonal matrix does not change singular values.</li> </ul> <p>Therefore, even if PCA ‚Äúrelabels,‚Äù flips signs, or rotates the basis vectors within the subspace, the <strong>singular values remain unchanged</strong>.</p> <p>This means singular values capture a property of the <strong>subspaces</strong>, not of the particular eigenvectors chosen.</p> <hr/> <h2 id="3-geometric-meaning-principal-angles">3] Geometric meaning: principal angles</h2> <p>Take the SVD:</p> \[S = Q \Sigma R^{\top},\] <p>where</p> \[\Sigma = \mathrm{diag}(\sigma_1,\dots,\sigma_r).\] <p>A fundamental result is:</p> \[\sigma_i = \cos(\theta_i),\] <p>where $\theta_i$ are the <strong>principal angles</strong> between the two $r$-dimensional subspaces.</p> <p>Interpretation:</p> <ul> <li>$\theta_i = 0 \implies$ perfectly aligned direction exists (since $\cos(\theta_i)=1$)</li> <li>$\theta_i = 90^\circ \implies$ orthogonal direction (since $\cos(\theta_i)=0$)</li> </ul> <p>So the singular values summarize <em>how much overlap</em> the two subspaces have along their best-aligned directions.</p> <hr/> <h2 id="4-why-this-is-the-stable-comparison">4] Why this is the ‚Äústable‚Äù comparison</h2> <p>Think of $U_{\text{train}}$ and $U_{\text{test}}$ as <strong>arbitrary coordinate systems</strong> inside their respective subspaces.</p> <p>A meaningful comparison should ignore that arbitrariness.</p> <p>Principal angles / singular values do exactly this: they compute the <strong>best possible matching</strong> between directions in the two subspaces.</p> <p>Instead of comparing ‚ÄúPC1 $\leftrightarrow$ PC1,‚Äù we solve an optimal alignment problem:</p> \[\max_{\|a\|=\|b\|=1} a^{\top}\bigl(U_{\text{train}}^{\top}U_{\text{test}}\bigr)b,\] <p>and the sequence of best matches yields</p> \[\sigma_1,\sigma_2,\dots\] <p>as the strengths of alignment along the best-aligned directions.</p> <hr/> <h2 id="5-tiny-example-intuition-2d-case">5] Tiny example intuition (2D case)</h2> <p>Suppose both subspaces are actually the same 2D plane in $\mathbb{R}^d$.</p> <p>You could pick:</p> <ul> <li>$U_{\text{train}}$ = standard basis in that plane</li> <li>$U_{\text{test}}$ = same plane but rotated by $45^\circ$ inside it</li> </ul> <p>Then $S$ might look like a rotation matrix:</p> \[S= \begin{pmatrix} \cos 45^\circ &amp; -\sin 45^\circ \\ \sin 45^\circ &amp; \cos 45^\circ \end{pmatrix}.\] <p>The entries are not the identity, so basis-by-basis dot products look ‚Äúnot aligned.‚Äù</p> <p>But the singular values of a rotation matrix are both $1$.</p> <p>So singular values correctly say: <strong>the subspaces are identical</strong>.</p> <p>That‚Äôs the whole point.</p> <hr/> <h2 id="bottom-line">Bottom line</h2> <p>We use singular values of</p> \[U_{\text{train}}^{\top}U_{\text{test}}\] <p>because:</p> <ul> <li>‚úÖ they are invariant to sign flips / rotations / re-ordering of PCA vectors inside the subspace</li> <li>‚úÖ they define principal angles, which are a true subspace-to-subspace comparison</li> <li>‚úÖ they give a stable measure of drift even when eigenvectors are not uniquely defined</li> </ul> <hr/> <h2 id="a-sidenote-connection-to-projection-distance">A sidenote: Connection to projection distance</h2> <p>Let $P_{\text{train}}$ and $P_{\text{test}}$ be the orthogonal projection matrices onto the two subspaces:</p> \[P_{\text{train}} = U_{\text{train}}U_{\text{train}}^{\top}, \qquad P_{\text{test}} = U_{\text{test}}U_{\text{test}}^{\top}.\] <p>Then one can show the Frobenius-distance relationship:</p> \[\|P_{\text{train}} - P_{\text{test}}\|_F^2 = 2r - 2\|U_{\text{train}}^{\top}U_{\text{test}}\|_F^2 = 2\sum_{i=1}^r \sin^2(\theta_i).\]]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Subspace Geometry and Computation]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (4): Connections (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (4): Connections (in progress)"/><published>2026-01-15T15:48:02+00:00</published><updated>2026-01-15T15:48:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(4)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/"><![CDATA[<p>This is episode 4 on the smooth manifold series. Today we will be exploring more on tangent vectors, and another key concept related to tangent spaces for different tangent planes: connections.</p> <h2 id="intuition-directional-derivatives">Intuition: Directional Derivatives</h2> <p>üåê The Connection: Bridging Derivatives from $\mathbb{R}^3$ to Curved Manifolds The concept of a connection is the necessary tool that allows us to perform differential calculus on curved spaces (manifolds), such as the surface of a sphere. It generalizes the familiar idea of the directional derivative from flat Euclidean space ($\mathbb{R}^3$).</p> <ol> <li>Directional Derivatives in Euclidean Space ($\mathbb{R}^3$) In $\mathbb{R}^3$ with Cartesian coordinates $(x, y, z)$, the directional derivative provides a simple way to measure change. The basis vectors $\left{ \mathbf{i}, \mathbf{j}, \mathbf{k} \right}$ (or the equivalent operators $\left{ \frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z} \right}$) are constant, allowing us to define derivatives simply as component-wise partial derivatives. Let $p=(1, 2, 0)$ be a point, $X=(y, -x, 3x)$ be the direction vector field, and $V=(xz, y^2, -2x)$ be a vector field. A. Derivative of a Scalar Function ($D_X f$) This is the directional derivative of a smooth scalar function $f$ in the direction $X$.</li> </ol> <p>Perspective Formula Example Result for f(x,y,z)=xy2+z at p Vector-Based (Calculus) $D_X f = \nabla f \cdot X$ $D_X f(p) = \langle 4, 4, 1 \rangle \cdot \langle 2, -1, 3 \rangle = \mathbf{7}$ Point Derivation (Geometry) $X[f] = X^i \frac{\partial f}{\partial x^i}$ $Xf = (y^3 - 2x^2y + 3x)\big</p> <p>This confirms that in differential geometry, a tangent vector $X$ is rigorously defined as a point derivation‚Äîan operator that mimics the directional derivative by satisfying the Leibniz rule. B. Derivative of a Vector Field ($D_X V$) This derivative measures how the vector field $V$ changes as we move in the direction $X$. In $\mathbb{R}^3$, this is calculated by taking the directional derivative of each component of $V$. Using the vector fields $X$ and $V$: The $k$-th component of the resulting vector $D_X V$ is $(D_X V)^k = \sum_{i} X^i \frac{\partial V^k}{\partial x^i}$. Component 1 (i.e., $k=1$): $(D_X V)^1 = yz + 3x^2$ Component 2 (i.e., $k=2$): $(D_X V)^2 = -2xy$ Component 3 (i.e., $k=3$): $(D_X V)^3 = -2y$ Evaluating at $p=(1, 2, 0)$ gives:</p> <p>\(D_X V(p) = \langle 3, -4, -4 \rangle\)</p> <ol> <li>The General Connection: The Covariant Derivative ($\nabla_X V$) The formula for $D_X V$ fails on a curved manifold $M$ because the tangent spaces $T_p M$ and $T_q M$ at nearby points $p$ and $q$ are distinct. We cannot simply subtract the vector $V(p)$ from $V(q)$. A Connection ($\nabla$) is the rule that provides the necessary ‚Äúcorrection‚Äù to define the derivative intrinsically on $M$. The resulting derivative is called the covariant derivative $\nabla_X V$. A. Definition and Axioms The connection is an operator $\nabla: C^{\infty}(M) \times C^{\infty}(M) \to C^{\infty}(M)$ that maps two vector fields, $X$ and $V$, to a new vector field $\nabla_X V$, satisfying: Linearity over Functions in $X$: $\nabla_{fX} V = f \nabla_X V$ Linearity in $V$: $\nabla_X (aV + bW) = a \nabla_X V + b \nabla_X W$ Leibniz Rule: $\nabla_X (fV) = (Xf) V + f \nabla_X V$ (where $Xf$ is the directional derivative of $f$) B. The Coordinate Form and Christoffel Symbols In local coordinates, the covariant derivative $\nabla_X V$ is defined using the Christoffel symbols ($\Gamma^k_{ij}$), which represent the rate of change of the coordinate basis vectors $\left{ \frac{\partial}{\partial x^i} \right}$:</li> </ol> <p>\((\nabla_X V)^k = \underbrace{X^i \frac{\partial V^k}{\partial x^i}}_{\text{I. Flat-Space Derivative Term}} + \underbrace{X^i \Gamma^k_{ij} V^j}_{\text{II. Curvature Correction Term}}\) The Christoffel symbols $\Gamma^k_{ij}$ are defined by the action of the connection on the basis vectors:</p> <p>\(\nabla_{\frac{\partial}{\partial x^i}} \frac{\partial}{\partial x^j} = \sum_{k} \Gamma^k_{ij} \frac{\partial}{\partial x^k}\) Difference from Euclidean Case: In $\mathbb{R}^3$ with Cartesian coordinates, $\Gamma^k_{ij} = 0$, and the second term vanishes, resulting in $\nabla_X V = D_X V$. On a curved manifold, $\Gamma^k_{ij} \neq 0$, and the correction term is essential.</p> <ol> <li>The Levi-Civita Connection In Riemannian Geometry, a Riemannian metric $g$ is introduced to measure lengths and angles. The Levi-Civita Connection is the unique connection that respects this metric structure. It is defined by two crucial properties: Metric Compatibility: The connection must preserve the metric $g$ under parallel transport.</li> </ol> <p>\(X(g(V, W)) = g(\nabla_X V, W) + g(V, \nabla_X W)\) Zero Torsion: The connection must satisfy:</p> \[\nabla_X Y - \nabla_Y X = [X, Y]\] <p>where $[X, Y]$ is the Lie bracket. The Christoffel symbols of the Levi-Civita Connection are thus entirely determined by the components of the metric $g_{ij}$ and their first derivatives:</p> \[\Gamma^k_{ij} = \frac{1}{2} g^{k\ell} \left( \frac{\partial g_{j\ell}}{\partial x^i} + \frac{\partial g_{i\ell}}{\partial x^j} - \frac{\partial g_{ij}}{\partial x^\ell} \right)\] <h2 id="connections">Connections</h2> <h2 id="christoffel-symbols">Christoffel Symbols</h2> <h2 id="levi-civita-connections">Levi-Civita Connections</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Exploration of Connection]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)"/><published>2026-01-14T20:22:25+00:00</published><updated>2026-01-14T20:22:25+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(3)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/"><![CDATA[<p>This is episode 3 on the smooth manifold series. Today we will be diving into the properties of maps between manifolds. I will first summarize how to understand and compute the differential of a smooth map between manifolds, both abstractly and concretely, culminating in the matrix-valued example<br/> \(F(A) = A^\top A.\)</p> <hr/> <h2 id="1-differential-canonical-definition">1] Differential: canonical definition</h2> <h3 id="11-differential-of-a-smooth-map-intrinsic-definition">1.1 Differential of a Smooth Map (Intrinsic Definition)</h3> <p>Let \(F : M \to N\) be a smooth map between smooth manifolds.</p> <p>For any point $ p \in M $, the <strong>differential</strong> is a linear map which is a <strong>pushforward of derivations</strong>. \(dF_p : T_p M \longrightarrow T_{F(p)} N.\)</p> <h3 id="derivation-based-definition">Derivation-based definition</h3> <p>If $ v \in T_p M $ is a tangent vector viewed as a derivation, then \((dF_p v)(g) := v(g \circ F), \qquad g \in C^\infty(N).\)</p> <p>This definition is <strong>coordinate-free</strong>.</p> <p>It might appear at first both unnecessarily abstract and underestimated as to its computation. We might claim that it is essentially just a Jacobian matrix in local coordinates. However, the essence of this concept resides on its definition to be conceptually a coordinate-independent linear map between tangent spaces.</p> <hr/> <h3 id="12-coordinate-representation-and-the-jacobian">1.2 Coordinate Representation and the Jacobian</h3> <p>To compute $ dF_p $ in practice:</p> <ol> <li>Choose a chart $ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>Choose a chart $ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ol> <p>Define the coordinate expression: \(\tilde F = \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Then: \(\boxed{ dF_p \;\text{is represented by}\; D\tilde F(\varphi(p)) }\)</p> <p>That is, <strong>the Jacobian matrix is the coordinate representation of the differential</strong>.</p> <blockquote> <p>The Jacobian depends on coordinates; the linear map $ dF_p $ does not.</p> </blockquote> <hr/> <h3 id="13-why-this-is-not-just-the-jacobian">1.3 Why this is not ‚Äújust‚Äù the Jacobian</h3> <p>The Jacobian depends on coordinates;<br/> $ dF_p $ does not.</p> <p>More precisely:</p> <ul> <li>$ dF_p $ is a <strong>geometric linear map</strong></li> <li>The Jacobian is a <strong>matrix representation</strong> of that map in chosen bases: \(\frac{\partial \bigl(\psi^1 \circ F,\;\dots,\;\psi^n \circ F\bigr)} {\partial \bigl(x^1,\;\dots,\;x^m\bigr)}\)</li> </ul> <p>If you change charts, the matrix changes by:</p> \[\boxed{ J_{\text{new}} = D\psi\,\cdot\, J_{\text{old}} \,\cdot\, (D\varphi^{-1}) }\] <p>but the underlying linear map $ dF_p $ stays the same.</p> <hr/> <h2 id="2-differential-alternative-interpretation">2] Differential: alternative interpretation</h2> <h3 id="21-curve-based-definition">2.1 Curve-based definition</h3> <p>This is also coordinate-free:</p> <p>If<br/> \(\gamma : (-\varepsilon,\varepsilon) \to M\) is a smooth curve with \(\gamma(0) = p \quad \text{and} \quad \gamma'(0) = v \in T_p M,\) then \(\boxed{ dF_p(v) = (F \circ \gamma)'(0) \in T_{F(p)} N. }\)</p> <p>No coordinates anywhere. This viewpoint is often the most intuitive and is fully equivalent to the derivation definition.</p> <hr/> <h3 id="22-curves-in-local-coordinates">2.2 Curves in local coordinates</h3> <p>Choose charts:</p> <ul> <li>$ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>$ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ul> <p>Define the coordinate representation: \(\tilde F := \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Now define the coordinate curve: \(\tilde\gamma := \varphi \circ \gamma : (-\varepsilon,\varepsilon) \to \mathbb{R}^m.\)</p> <p>Consequently, in coordinates the statement is the following:</p> \[\boxed{ D\tilde F(\varphi(p)) \cdot \tilde\gamma'(0) = D(\psi \circ F \circ \varphi^{-1})(\varphi(p)) \cdot (\varphi \circ \gamma)'(0). }\] <p>In the coordinate formula, \(\gamma'(0) \quad \text{really means} \quad \tilde\gamma'(0) = (\varphi \circ \gamma)'(0).\)</p> <p>Here‚Äôs a diagram that makes everything explicit</p> \[\begin{array}{ccc} T_p M &amp; \xrightarrow{dF_p} &amp; T_{F(p)} N \\ \downarrow d\varphi_p &amp; &amp; \uparrow d\psi^{-1}_{\psi(F(p))} \\ \mathbb{R}^m &amp; \xrightarrow{D\tilde F(\varphi(p))} &amp; \mathbb{R}^n \end{array}\] <p>Thus:</p> <ul> <li>$ \gamma‚Äô(0) $ lives in $ T_p M $</li> <li>$ (\varphi \circ \gamma)‚Äô(0) = d\varphi_p(\gamma‚Äô(0)) \in \mathbb{R}^m $</li> <li>$ D\tilde F(\varphi(p)) $ acts on that coordinate vector</li> </ul> <hr/> <h3 id="23-sidenote-graph-differential">2.3 Sidenote: Graph Differential</h3> <p>The <strong>graph</strong> of $ F $ is \(\Gamma_F = \{ (p, F(p)) \mid p \in M \} \subset M \times N.\)</p> <p>Define the graph map: \(\Phi : M \to M \times N, \quad \Phi(p) = (p, F(p)).\)</p> <p>Its differential is: \(\boxed{ d\Phi_p(v) = (v, dF_p(v)). }\)</p> <p>This is what is often called the <strong>graph differential</strong>.</p> <hr/> <h2 id="3-special-case-maps-between-vector-spaces">3] Special Case: Maps Between Vector Spaces</h2> <h3 id="31-a-great-simplification">3.1 A great simplification</h3> <p>If $ M = \mathbb{R}^m $, $ N = \mathbb{R}^n $, then: \(T_p M \cong \mathbb{R}^m, \quad T_{F(p)} N \cong \mathbb{R}^n.\)</p> <p>In this case:</p> <ul> <li>$ dF_p $ is a linear map $ \mathbb{R}^m \to \mathbb{R}^n $</li> <li>Its matrix is exactly the <strong>Jacobian matrix</strong></li> <li>$ dF_p(H) $ coincides with the <strong>Fr√©chet / directional derivative</strong></li> </ul> <hr/> <h3 id="32-worked-example--fa--atop-a-">3.2 Worked Example: $ F(A) = A^\top A $</h3> <p>Let \(F : \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}, \quad F(A) = A^\top A.\)</p> <p>Since $ \mathbb{R}^{n \times n} $ is a vector space, \(T_A(\mathbb{R}^{n \times n}) \cong \mathbb{R}^{n \times n}.\)</p> <h4 id="321-direct-computation">3.2.1 Direct computation</h4> <p>There are several ways to compute the differential. The most straight-forward method is to do the following (as you might imagine):</p> <p>For \(H \in T_A(\mathbb{R}^{n \times n})\), \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} (A+tH)^\top(A+tH).\)</p> <p>Expanding: \((A+tH)^\top(A+tH) = A^\top A + t(H^\top A + A^\top H) + t^2 H^\top H.\)</p> <p>Thus: \(\boxed{ dF_A(H) = H^\top A + A^\top H. }\)</p> <h4 id="322-curve-based-computation">3.2.2 Curve-based computation</h4> <p>Let \(A(t)\) be a smooth curve with: \(A(0)=A, \quad A'(0)=H.\)</p> <p>Then: \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} A(t)^\top A(t) = H^\top A + A^\top H.\)</p> <p>This makes it clear that the definition of \(dF_A(H)\) is <strong>coordinate-free</strong>.</p> <p>Sidenote: if we restrict \(A\) to symmetric or SPD matrices, what will we see? Or what if we connect this to Riemannian geometry on \(GL(n)\) or \(SPD(n)\)? We‚Äôll come back to this later when we discuss Lie group and Lie algebra.</p> <hr/> <h2 id="4-back-to-classical-regular-surfaces-parametrizations-immersions">4] Back to classical regular surfaces (parametrizations, immersions)</h2> <h3 id="the-question-in-do-carmo-diffgeom">The Question in do Carmo DiffGeom</h3> <p>In do Carmo‚Äôs definition of a <strong>regular surface</strong> in \(\mathbb{R}^3\), a coordinate map \(X : U \subset \mathbb{R}^2 \to \mathbb{R}^3\) is required to satisfy two conditions:</p> <ol> <li>\(X\) is a <strong>differentiable homeomorphism</strong> onto its image.</li> <li>The differential \(dX_p\) is <strong>injective</strong> at every point $p \in U$.</li> </ol> <p>Since \(X\) maps from \(\mathbb{R}^2\) to \(\mathbb{R}^3\), its differential can never be surjective, so injectivity (rank 2) is the meaningful requirement.</p> <p>A natural question arises:</p> <blockquote> <p>If \(X\) is already a differentiable homeomorphism, isn‚Äôt its differential automatically injective?</p> </blockquote> <p>The answer is <strong>no</strong>.</p> <p>Well, to make it explicit, let‚Äôs figure out first what a differentiable homeomorphism actually gives us: If \(X : U \to \mathbb{R}^3\) is a differentiable homeomorphism onto its image, then:</p> <ul> <li>\(X\) is <strong>continuous and injective</strong></li> <li>\(X^{-1}\) is <strong>continuous</strong> (but <em>not</em> necessarily differentiable)</li> <li>Topologically, \(X(U)\) looks like a 2‚Äëdimensional surface</li> </ul> <p>This is a <strong>topological</strong> statement plus differentiability of $X$. It controls <em>points</em>, but says nothing about what happens to <em>directions</em>. Crucially, differentiability of the inverse is <em>not</em> assumed.</p> <p>Fine, but then why injectivity of the differential is not automatically assured? Notice that the differential</p> \[dX_p : \mathbb{R}^2 \to \mathbb{R}^3\] <p>being injective means it has <strong>rank 2</strong> meaning no tangent direction is collapsed. A map can be:</p> <ul> <li>injective,</li> <li>continuous with continuous inverse,</li> <li>differentiable,</li> </ul> <p>and <em>still</em> have rank drop somewhere. Let me give a concrete example:</p> <p>Consider \(X(u,v) = (u^3, v, 0).\)</p> <p>Properties of this map:</p> <ul> <li>It is <strong>injective</strong></li> <li>It is a <strong>homeomorphism onto its image</strong></li> <li>It is differentiable everywhere</li> </ul> <p>However, its differential is \(dX_{(u,v)} = \begin{pmatrix} 3u^2 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix}\)</p> <p>At \($u = 0\), this matrix has <strong>rank 1</strong>, not 2. One tangent direction is squashed. Therefore, this map is <strong>not an immersion</strong>, and it does <strong>not</strong> define a regular surface in do Carmo‚Äôs sense.</p> <hr/> <p>However, you might observe here a seemingly apparent paradox: ‚ÄúBut the image is <strong>Just</strong> a plane!‚Äù</p> <p>Well, the image of \(X(u,v) = (u^3, v, 0)\) is \({(x,y,0) : x,y \in \mathbb{R}},\) which is the entire (xy)-plane indeed. As a <strong>subset</strong> of \(\mathbb{R}^3\), this plane is perfectly flat, so one might expect its tangent plane at every point to be the whole plane. So then why does the tangent collapse under this map?</p> <hr/> <p>The key point here is that here are <strong>two distinct notions</strong> at play:</p> <ol> <li>The tangent plane of a <strong>subset</strong> of \(\mathbb{R}^3\)</li> <li>The tangent plane <strong>defined by a parametrization</strong></li> </ol> <p>In do Carmo‚Äôs approach, tangent planes are defined <em>via parametrizations</em>. The tangent plane at a point is \(T_pS = \operatorname{span}{X_u(p), X_v(p)}.\)</p> <p>For the map above: \(X_u = (3u^2, 0, 0), \quad X_v = (0,1,0).\)</p> <p>At (u=0): \(X_u(0,v) = (0,0,0), \quad X_v(0,v) = (0,1,0),\) so the span is <strong>1‚Äëdimensional</strong>.</p> <p>This means:</p> <blockquote> <p>The parametrization fails to distinguish two independent directions in the parameter domain.</p> </blockquote> <p>Geometrically, the $u$-direction has been crushed.</p> <hr/> <p>But does the plane still have a 2D tangent plane?</p> <p>Yes ‚Äî but <strong>not via this parametrization</strong>.</p> <p>If instead we parametrize the same plane by \(Y(s,t) = (s,t,0),\) then \(dY = \begin{pmatrix} 1 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix},\) which has rank 2 everywhere.</p> <p>So the <em>same subset</em> becomes a <strong>regular surface</strong> under a different map.</p> <hr/> <p>Finally, what does this meana conceptually? The key lesson is:</p> <blockquote> <p><strong>Regularity is not a property of the subset alone ‚Äî it is a property of the subset together with its smooth structure.</strong></p> </blockquote> <p>Different parametrizations can induce:</p> <ul> <li>a <strong>good</strong> smooth structure (immersion)</li> <li>or a <strong>bad</strong> one (rank collapse)</li> </ul> <p>This is why do Carmo requires that <strong>there exists</strong> a local parametrization with injective differential.</p> <hr/> <p>In other words, if you ask ‚ÄúDoes that mean there might exist other maps which make the plane a regular surface?‚Äù, then the answer is Yes ‚Äî absolutely. The plane <em>is</em> a regular surface because such maps exist.</p> <p>Behind that is another question: ‚ÄúDoes immersion entirely depend on which maps we pick?‚Äù Yes!</p> <ul> <li><strong>Immersion is a property of the map</strong>, not of the set.</li> <li>A <strong>regular surface</strong> is a set for which <em>good immersions exist everywhere</em>.</li> </ul> <p>In this seense, do Carmo does separate the conditions, being deliberately modular:</p> <ol> <li><strong>Homeomorphism</strong> ‚Üí good topology (no self‚Äëintersections)</li> <li><strong>Injective differential</strong> ‚Üí good differential geometry</li> </ol> <p>whether neither condition implies the other.</p> <p>Finally, some of key points:</p> <ul> <li>The same subset of \(\mathbb{R}^3\) can support <strong>many different smooth structures</strong></li> <li>Differential geometry only works once a smooth structure is fixed</li> <li>Parametrizations are how do Carmo <em>builds</em> that structure</li> </ul> <p>This is why modern texts often say:</p> <blockquote> <p>‚ÄúA surface is a 2‚Äëdimensional smooth manifold embedded in $$\mathbb{R}^3$.‚Äù</p> </blockquote> <p>Do Carmo reaches this notion <strong>from parametrizations upward</strong>, rather than assuming it at the start.</p> <hr/> <blockquote> <p><strong>Topology sees points.</strong> <strong>Differential geometry sees directions.</strong></p> </blockquote> <p>A homeomorphism controls points. Injectivity of the differential controls directions.</p> <p>You need <strong>both</strong> to get a regular surface.</p> <h2 id="5-association-with-inverse-function-theorem">5] Association with inverse function theorem</h2> <h2 id="6-immersion-submersion">6] Immersion, submersion</h2> <p>Immersion basics</p> <p>Here I want to emphasize one key fact that:</p> <blockquote> <p>immersion imply a nonzero determinant in coordinates</p> </blockquote> <p>Why? Again, let‚Äôs recall that immersion means the differential is injective Let<br/> \(\phi:M^m\to N^n,\qquad m\le n\) and let $p\in M$.<br/> Saying <strong>(\phi) is an immersion at (p)</strong> means the differential \(d\phi_p:T_pM\to T_{\phi(p)}N\) is <strong>injective</strong>.</p> <p>Equivalently (in linear algebra language):<br/> \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Now, if write it in local coordinates ‚Üí Jacobian matrix has rank \(m\) Pick coordinate charts:</p> <ul> <li>on \(M\): \(x=(x^1,\dots,x^m)\) around \(p\)</li> <li>on \(N\): \(y=(y^1,\dots,y^n)\) around \(\phi(p)\)</li> </ul> <p>Then locally \(\phi\) looks like a smooth map between Euclidean spaces: \(y^a = \phi^a(x^1,\dots,x^m),\qquad a=1,\dots,n.\)</p> <p>Its differential in these coordinates is represented by the <strong>Jacobian matrix</strong> \(J(p)=\left(\frac{\partial \phi^a}{\partial x^i}(p)\right)\) which is an (n\times m) matrix.</p> <p>The immersion condition says: \(\operatorname{rank}(J(p))=m.\)</p> <p>So the columns of \(J(p)\) are linearly independent.</p> <p>Now we use a standard linear algebra fact:</p> <blockquote> <p>An \(n\times m\) matrix has rank \(m\) <strong>iff</strong> there exists an \(m\times m\) submatrix (choose \(m\) rows) whose determinant is nonzero.</p> </blockquote> <p>Why?</p> <ul> <li>If <strong>every</strong> \(m\times m\) minor determinant were zero, then <strong>every</strong> set of $m$ rows would be linearly dependent, so the rank would be \(&lt;m\).</li> <li>Since the rank is \(m\), at least one choice of $m$ rows gives an invertible \(m\times m\) matrix.</li> </ul> <p>Concretely: there exist indices \(1\le a_1&lt;\cdots&lt;a_m\le n\) such that the matrix \(\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)_{\alpha,i}\) has \(\det\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)\neq 0.\)</p> <p>If we now <strong>rename/reorder the target coordinates</strong> so that those special indices become \(1,\dots,m\), then we can assume:</p> \[\det\left(\frac{\partial (\phi^1,\dots,\phi^m)}{\partial (x^1,\dots,x^m)}(p)\right)\neq 0.\] <p>That‚Äôs exactly the statement: in coordinates (after renumbering if needed), an immersion gives a nonzero determinant of an \(m\times m\) Jacobian block.</p> <p>In one line summary: an immersion means \(\phi\) ‚Äúdoesn‚Äôt collapse any tangent directions,‚Äù so locally you can find $m$ coordinate functions of \(\phi\) that vary independently ‚Äî and ‚Äúvary independently‚Äù is exactly ‚ÄúJacobian block has nonzero determinant.‚Äù</p> <hr/> <h4 id="side-note-connection-to-the-constant-rank-theorem--local-normal-form-of-an-immersion">Side note: Connection to the Constant Rank Theorem / local normal form of an immersion?</h4> <p>Remember that Constant Rank Theorem (specialized to immersions) says: Let \(\phi:M^m\to N^n\) be smooth, and suppose \(\phi\) is an <strong>immersion at \(p\)</strong>.<br/> That means \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Then the constant rank theorem says:</p> <blockquote> <p>There exist coordinate charts<br/> \((U,x)\ \text{around }p,\qquad (V,y)\ \text{around }\phi(p)\) such that in these coordinates the map becomes \(y\circ \phi\circ x^{-1}(u^1,\dots,u^m) \;=\; (u^1,\dots,u^m,0,\dots,0).\)</p> </blockquote> <p>So locally, \(\phi\) looks like the <strong>standard inclusion</strong> \(\mathbb{R}^m \hookrightarrow \mathbb{R}^n,\qquad u\mapsto (u,0).\)</p> <p>That is the precise geometric meaning of ‚Äúimmersion.‚Äù</p> <p>In these special coordinates, \(\phi^1(u)=u^1,\;\dots,\;\phi^m(u)=u^m,\qquad \phi^{m+1}(u)=0,\dots,\phi^n(u)=0.\)</p> <p>So the Jacobian matrix is literally \(J= \begin{pmatrix} I_m\\ 0 \end{pmatrix}\) (an \(n\times m\) matrix).</p> <p>Now look at the top \(m\times m\) block: \(\frac{\partial(\phi^1,\dots,\phi^m)}{\partial(u^1,\dots,u^m)} = I_m,\) so \(\det(I_m)=1\neq 0.\)</p> <p>That‚Äôs exactly the coordinate statement.</p> <p>How this matches the ‚Äúminor is nonzero‚Äù argument? Before using the constant rank theorem, we only know:</p> <ul> <li>\(J(p)\) has rank \(m\)</li> <li>therefore some \(m\times m\) minor determinant is nonzero</li> </ul> <p>Then the constant rank theorem tells us that we can actually <strong>choose coordinates</strong> so that the ‚Äúgood minor‚Äù becomes the <em>first</em> \(m\) coordinates, and the map becomes \($(u,0)\).</p> <p>So:</p> <ul> <li><strong>Linear algebra fact:</strong> full rank \(\Rightarrow\) some minor \(\neq 0\)</li> <li><strong>Constant rank theorem:</strong> we can change coordinates to make that minor the obvious identity matrix.</li> </ul> <p>The geometric picture (why \((u,0)\) is the right normal form) is that an immersion means \(\phi\) ‚Äúinjects tangent vectors,‚Äù so locally \(\phi(U)\subset N\) is an $m-dimensional ‚Äúsheet‚Äù sitting inside an n-dimensional space. In good coordinates on N, that sheet looks like:</p> \[\{(y^1,\dots,y^n): y^{m+1}=\cdots=y^n=0\},\] <p>i.e. an embedded copy of \(\mathbb{R}^m\\).</p> <p>So locally, \(\phi\) is just a parametrization of that sheet.</p> <h2 id="7-embedding">7] Embedding</h2> <h3 id="72-a-key-difference-between-embedding-and-immersion">7.2 A key difference between embedding and immersion</h3> <p><strong>‚Äúno self-intersections‚Äù is one of the key geometric consequences of being <em>embedded</em></strong> (as opposed to merely <em>immersed</em>). As stated above, if a manifold \(M\) is <strong>embedded</strong> in \(\mathbb{R}^k\), it sits inside \(\mathbb{R}^k\) as a ‚Äúnice subset,‚Äù like a surface you could physically draw without crossing itself. More formally, an <strong>embedding</strong> \(F: M \to \mathbb{R}^k\) means:</p> <ol> <li>\(F\) is a <strong>smooth immersion</strong> (its differential is injective everywhere), and</li> <li>\(F\) is a <strong>homeomorphism onto its image</strong> \(F(M)\) (with the subspace topology).</li> </ol> <p>That second condition is exactly what rules out the classic ‚Äúself-crossing‚Äù pathology. If the image ‚Äúintersects itself‚Äù in the sense that two different points \(p\neq q\in M\) map to the same point in \(\mathbb{R}^k\), i.e. \(F(p)=F(q),\) then \(F\) is <strong>not injective</strong>, so it can‚Äôt be an embedding.</p> <p>So: <strong>an embedded submanifold cannot cross itself as a set in \(\mathbb{R}^k\)</strong>.</p> <p>On the other hand, an <strong>immersion</strong> can look like a manifold with self-crossings in \(\mathbb{R}^k\). An example will be the ‚Äúfigure-eight curve‚Äù in \(\mathbb{R}^2\) can be parametrized smoothly with nonzero derivative everywhere, so it‚Äôs an immersion, but it‚Äôs <strong>not embedded</strong> because it fails injectivity / fails to be a homeomorphism onto its image.</p> <p>In shoft,</p> <ul> <li><strong>Embedded \(\Rightarrow\)</strong> injective + ‚Äútopologically correct‚Äù inclusion<br/> \(\Rightarrow\) <strong>no self-intersections</strong>.</li> <li><strong>Immersed \(\Rightarrow\)</strong> locally nice but can globally overlap<br/> \(\Rightarrow\) <strong>self-intersections possible</strong>.</li> </ul> <h2 id="8-submanifolds">8] Submanifolds</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Explore relations of maps between manifolds]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)"/><published>2025-09-17T23:30:02+00:00</published><updated>2025-09-17T23:30:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(2)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/"><![CDATA[<p>This is episode 2 on the smooth manifold series. Today we will be diving into concepts that appear initially very intuitive at first glance, but then the extended version of which is indeed quite abstract.</p> <h2 id="0-familiar-examples">0] Familiar examples</h2> <p>Let me start with two simple concrete examples to illustrate what tangent vectors and tangent space are. Indeed they match up to our intuition!</p> <p>Let‚Äôs first say that we have a unit circle in \(\mathbb{R}^2\), or basically let‚Äôs denote it as \(S^1\) (this is a standard notation as</p> <p>\(S^{n} = \{ x \in \mathbb{R}^{n+1} \mid |x| = 1 \}\),</p> <p>representing the surface of an \((n+1)\)-dimensional ball). Pick \(p = (1, 0)\). If I ask you what the tangent vector is starting at \(p\) and tangnet to \(S^1\)? Your answer is probably a vector pointing upward or downward with its tail at \(p\). Indeed,</p> <h2 id="1-three-equivalent-definitions-of-tangent-space">1] Three equivalent definitions of Tangent Space</h2> <p>We‚Äôll cover three equivalent definitions tangent space.</p> <h3 id="1-tangent-vectors-are-equivalence-classes-of-curves">[1] Tangent vectors are equivalence classes of curves</h3> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>tangent vectors are equivalence classes of smooth curves through \(p\):</p> \[T_pM = \{\frac{d}{dt}\Big|_{t = 0} \gamma(t) \Big| \gamma: (\epsilon, \epsilon) \rightarrow M, \; \gamma(0) = p \}\] <p>where \(\gamma_1 \sim \gamma_2\) if in some (equivalently, any) coordinate chart \(\phi: U \subset M \rightarrow \mathbb{R}^n\):</p> \[\frac{d}{dt}\Big|_{t = 0} \phi(\gamma_1(t)) = \frac{d}{dt}\Big|_{t = 0} \phi(\gamma_2(t))\] <p>Consequently, each tangent vector is represented by the velocity of a curve through \(p\).</p> <h3 id="2-tangent-vectors-are-derivations-at-p">[2] Tangent vectors are derivations at \(p\)</h3> <p>Let \(M\) be a manifold, the tangent space at \(p \in M\), denoted as \(T_pM\), is the the vector space of all derivations at \(p\). Notice that a derivation at \(p\) is a <strong>linear</strong> operator (at \(p\)):</p> \[D: C^{\infty}_{p}(M) \rightarrow \mathbb{R}\] <p>satisfying the Leibniz rule:</p> \[D(fg) = f(p)D(g) + g(p)D(f), \; \forall f, g \in C^{\infty}(M)\] <p>where \(C^{\infty}_{p}\) is the equivalence class of \((f, U)\), where \(f \in C^{\infty}\) and \(U\) is a neighbourhood of \(p\). Two functions \((f, U)\) and \((g, V)\) are equivalent iff \(\exist \; W \subset U \cap V\) a neighbourhood of \(p\) such that \(f(x) = g(x), \forall x \in W\), where \(g \in C^{\infty}\) and \(V\) is a neighbourhood of \(p\).</p> <p>Intuitively, \(D\) is like a directional derivative operator acting on smooth functions near \(p\). Consequently,</p> \[T_p{M} = \{ D \mid D \; \mathrm{is\ a\ derivation\ at} p \}\] <p>and a tangent vector \(v \in T_p{M}\) is a derivation \(D\). In fact, Tu (Theorem 2.2, An introduction to manifolds) showed that there‚Äôs a bijection between derivations at \(p\) and directional derivaties.</p> <h3 id="3-tangent-vectors-as-equivalences-on-function-germs">[3] Tangent vectors as equivalences on function germs</h3> <h3 id="local-coordinate-description">Local coordinate description</h3> <p>Specifically if we adopt interpretation [2], then with \((U, \phi)\) a chart with coordinates \((x^1, \dots, x^n)\) near \(p\)</p> \[\{ \frac{\partial}{\partial x^1}\Big|_p, \dots, \frac{\partial}{\partial x^n}\Big|_p \}\] <p>form a basis of \(T_pM\).</p> <p>Thus any tangent vector \(v\) can be written uniquely as</p> \[v = \sum_{i = 1}^{n} v^i \frac{\partial}{\partial x^i}\Big|_p\] <p>where \((v^1, \dots, v^n)\) are the components of the vector in this coordinate system.</p> <h2 id="2-equivalence-between-derivations-and-curves">2] Equivalence between derivations and curves</h2> <p>Perhaps not so surprisingly, the above two definitions are compatible and even equivalent to one another.</p> <p>From curves to derivations:</p> <h2 id="3-a-concrete-computational-example">3] A concrete computational example</h2> <p>A coherent walkthrough using the sphere \(S^2\)</p> <p>Let‚Äôs re-emphasize again that a vector field on a Manifold is defined as the following:</p> <p>Let \(M\) be a smooth manifold. A <strong>vector field</strong> on \(M\) is a smooth section/assignment \(p \mapsto X_p \in T_p M\) where \(T_p M\) is the tangent space at \(p\).</p> <p><strong>Fundamental viewpoint</strong>:</p> <blockquote> <p>A tangent vector is a <strong>derivation</strong>: a linear map \(X_p : C^\infty(M) \to \mathbb{R}\) satisfying the Leibniz rule.</p> </blockquote> <p>We‚Äôll also give the local coordinates and coordinate vector fields as the following:</p> <p>Let \((U, \varphi), \quad \varphi : U \subset M \to V \subset \mathbb{R}^n\) be a coordinate chart, with coordinates \((x^1, \dots, x^n).\)</p> <p>Each coordinate \(x^i\) is itself a <strong>function on the manifold</strong>: \(x^i : U \to \mathbb{R}.\)</p> <p>Definition of the ‚ÄúCoordinate Vector Field‚Äù:</p> <p>The coordinate vector field \(\left.\frac{\partial}{\partial x^i}\right|_p\) is defined <strong>intrinsically</strong> by its action on smooth functions: \(\boxed{ \left.\frac{\partial}{\partial x^i}\right|_p(f) := \frac{\partial}{\partial x^i} \big(f \circ \varphi^{-1}\big) \Big(\varphi(p)\Big) }\) where \(x^i\) is the \(i\)-th coordinate on \(\mathbb{R}^n\).</p> <p>Notice that there‚Äôs an abuse of notation (the left ‚Äúpartial‚Äù is what we define, whereas the right partial is the usual partial differentiation). Hopefully, it‚Äôs obvious that the above definition stems from the following basic definition of graph differential (which I will talk more into next time):</p> \[\boxed{ (dF_p(v))(f) := v_p(f \circ F) }\] <p>More importantly, this definition:</p> <ul> <li>uses <strong>only</strong> the chart,</li> <li>does <strong>not</strong> require an embedding and thus does $NOT$ give us components in $\mathbb{R}^n$</li> <li>explains what ‚Äú\(\partial f / \partial x^i\)‚Äù actually means: It means the ordinary partial derivative of the coordinate expression of $f$ after pulling $f$ back to a coordinate chart.Note that nothing is being differentiated on $\mathbb{R}^n$ unless we explicitly choose an embedding (see below).</li> </ul> <hr/> <p>Now, let‚Äôs take a look at the simple example: Sphere \(S^2\) and spherical coordinates. Given the manifold \(S^2 = \{(x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}\)</p> <p>Coordinate chart (away from poles)</p> <p>\((\theta, \varphi)\) with embedding map \(F(\theta,\varphi) = (\sin\varphi\cos\theta,\; \sin\varphi\sin\theta,\; \cos\varphi).\)</p> <p>Here:</p> <ul> <li>\(\theta, \varphi\) are <strong>functions on \(S^2\)</strong>,</li> <li>not abstract variables.</li> </ul> <p>According to the above discussions, the <strong>intrinsic</strong> meaning of \(\partial / \partial \theta\) and \(\partial / \partial \varphi\) are the following:</p> <p>For any \(f : S^2 \to \mathbb{R}\), \(\frac{\partial}{\partial \theta}(f) = \frac{\partial}{\partial \theta} \big(f \circ \varphi^{-1}\big)(\theta,\varphi), \quad \frac{\partial}{\partial \varphi}(f) = \frac{\partial}{\partial \varphi} \big(f \circ \varphi^{-1}\big).\)</p> <p>Again, this is the <strong>definition</strong>, not an interpretation. This offers us one way to calculate the vector field/tangent vectors at $\forall p \in M$.</p> <p>If we set</p> <p>\(f(x, y, z) = z\), then we could pull it back and obtain \((f \circ \phi^{-1})(\theta, \phi) = \cos(\phi)\). Consequently,</p> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X_p(f)= \frac{\partial \cos(\phi)}{\partial \theta} = 0\] <p>at $p = (\theta, \phi)$.</p> <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y_p(f)= \frac{\partial \cos(\phi)}{\partial \phi} = -\sin(\phi)\] <p>at $p = (\theta, \phi)$.</p> <hr/> <p>However, because \(S^2 \subset \mathbb{R}^3\), we may compute concrete representatives by viewing it as an embedded in $\mathbb{R}^3$ and to extrinsically compute the vector fields by pushforward of coordinate basis vectors via the embedding map of the manifold. Consequently, such computation lives in the embedded picture, not the intrinsic definition.</p> \[\frac{\partial}{\partial \theta} = \frac{\partial F}{\partial \theta} = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0)\] \[\frac{\partial}{\partial \varphi} = \frac{\partial F}{\partial \varphi}= (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi)\] <p>These are <strong>actual vectors in \(\mathbb{R}^3\)</strong> tangent to \(S^2\).</p> <blockquote> <p>Important:<br/> This is <strong>not the definition</strong> of coordinate vector fields ‚Äî<br/> it is a <strong>representation</strong> using the embedding.</p> </blockquote> <p>Consequently, we could act on functions without pulling back to coordinate space:</p> <p>Let \(f(p) = z(p)\) be a function on \(S^2\).</p> <p>Choose an extension \(\tilde f(x,y,z) = z \quad\Rightarrow\quad \nabla \tilde f = (0,0,1).\)</p> <p>Here‚Äôs the key observation:</p> <blockquote> <p>Once a tangent vector is represented in \(\mathbb{R}^3\), its action on $f$ is given by the directional derivative of an extension of $f$.</p> </blockquote> <p>Formally, \(\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\) where</p> <ul> <li>$\tilde{f}$ is any smooth extension of $f$ to $\mathbb{R}^3$</li> <li>$X_p \in T_pS^2 \subset \mathbb{R}^3$.</li> </ul> <p>This works because tangent vectors annihilate normal components.</p> <hr/> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X(f)= (0,0,1) \cdot (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0) = 0\] <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y(f)= (0,0,1) \cdot (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi) = -\sin\varphi\] <p>Same results as the intrinsic definition.</p> <p>Note that to be very concrete, in the above examples, at $p = (\theta, \phi)$, the tangent vector is:</p> <p>$X_p = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\;0)$ (we could interpret this as horizontal circles of latitude) and $Y_p = (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\ -\sin\varphi)$. This should make it very clear that we are treating the tangent vectors as actually ‚Äúvectors‚Äù living in $\mathbb{R}^3$ like how we usually refer them to be. With this extrinsic embedding, the general form of a vector field coincides with the intrinsic notion, but more tangible:</p> <p>$X = a(\theta, \phi)\frac{\partial}{\partial \theta} + b(\theta, \phi)\frac{\partial}{\partial \phi}$,</p> <p>then as a vector in $\mathbb{R}^3$: $X_p = a \frac{\partial F}{\partial \theta} + b\frac{\partial F}{\partial \phi}$, and</p> <p>$X_p(f) = \nabla \tilde{f}(p) \cdot X_p$</p> <p>Notice that for this computation no coordinate pullback is required. But to be crystal clear, we do <strong>not</strong> compute the vector field ‚Äúwithout coordinates‚Äù. We simply You replaced intrinsic coordinates on $\mathbb{S}^2$ by ambient Cartesian coordinates in $\mathbb{R}^3$. So coordinates are still there ‚Äî just in a different space.</p> <hr/> <p>Before we continue, let‚Äôs dwell on this formula for a while:</p> \[\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\] <p>This identity guarantees consistency between 1] the derivation definition and 2] the embedded vector representation (Question: is this just chain rule?s). It explains why the extrinsic calculation agrees with the intrinsic definition. If we have a nice embedding, two views are nice. However, if we go to the general abstract case without the ambient space, we could only assort to the intrinsic definition.</p> <hr/> <p>To be rigorous, you may be concerned about the specific use of the function extension here. However, we could show that extensions do not matter, because <strong>‚Äútangent vectors annihilate normal components‚Äù</strong>. Let me explain below:</p> <p>If \(g : \mathbb{R}^3 \to \mathbb{R}\) satisfies \(g|_{S^2} = 0,\) then for all \(v \in T_p S^2\), \(v(g) = 0.\) Equivalently, \(\nabla g(p) \perp T_p\mathbb{S}^2\)</p> <hr/> <h3 id="proof-via-curves">Proof (via curves):</h3> <p>Let \(\gamma(t) \subset S^2\) with \(\gamma(0)=p, \quad \gamma'(0)=v.\)</p> <p>Since \(g(\gamma(t)) = 0\) for all \(t\), \(v(g) = \frac{d}{dt} g(\gamma(t))\big|_{t=0} = 0.\)</p> <hr/> <h3 id="concrete-example">Concrete example</h3> <p>Let \(g(x,y,z) = x^2 + y^2 + z^2 - 1.\)</p> <p>Then: \(\nabla g = (2x,2y,2z)\) which is normal to \(S^2\).</p> <p>For any tangent vector \(v\), \(v(g) = \nabla g \cdot v = 0.\)</p> <hr/> <h3 id="consequence-crucial">Consequence (crucial)</h3> <p>If \(\tilde f_1\) and \(\tilde f_2\) are two extensions of \(f\), then \(g = \tilde{f_1} - \tilde{f_2}\) vanishes on $\mathbb{S}^2$. For any tangent vector $v$:</p> \[v(\tilde f_1) - v(\tilde f_2) = v(\tilde g) = 0\] <p>thus:</p> \[v(\tilde f_1) = v(\tilde f_2) \quad\forall v \in T_p S^2.\] <p>Hence directional derivatives along tangent vectors are <strong>well-defined</strong>: this is why directional derivatives along tangent vectors do not depend on how f extends off the manifold.</p> <p>View from another perspective, this indicates that a function that is identically zero on the manifold cannot change when we move tangentially: so tangent vectors ‚Äúdon‚Äôt see‚Äù normal variations. What I mean by the former statement ‚ÄúTangent vectors annihilate normal components‚Äù is that derivatives along tangent directions ignore any part of a function that only varies off the manifold, because those variations are invisible to curves that stay on the manifold.</p> <hr/> <h3 id="intrinsic-vs-extrinsic-viewpoints-summary">Intrinsic vs Extrinsic Viewpoints (Summary)</h3> <table> <thead> <tr> <th>Aspect</th> <th>Intrinsic</th> <th>Extrinsic</th> </tr> </thead> <tbody> <tr> <td>Tangent vector</td> <td>Derivation</td> <td>Vector in \(\mathbb{R}^3\)</td> </tr> <tr> <td>Definition</td> <td>Via charts</td> <td>Via embedding</td> </tr> <tr> <td>Computation</td> <td>Pullback</td> <td>Directional derivative</td> </tr> <tr> <td>Dependence</td> <td>Coordinate-dependent</td> <td>Embedding-dependent</td> </tr> </tbody> </table> <p>Both viewpoints are <strong>equivalent</strong> when an embedding exists.</p> <hr/> <p>Something to note from the above example:</p> <blockquote> <p>Coordinate vector fields are <strong>defined intrinsically as derivations</strong>,<br/> may be <strong>represented extrinsically</strong> using embeddings,<br/> and act on functions in a way that is <strong>independent of extensions</strong> because tangent vectors annihilate normal components.</p> </blockquote> <p>Through this simple example we already develop a taste of the difference between intrinsic and extrinsic geometry, the abstract definitions and concrete calculations. Such theme will be recurring for our journey in studying differential geometry.</p> <p>One of the advantages of the intrinsic view is that it is independent of coordinate charts. It paves the ground for explaining why</p> <blockquote> <p><strong>Gradients, geodesics, and Lie brackets make sense without embedding (or ever mentioning) the abstract manifold into $\mathbb{R}^n$</strong>.</p> </blockquote> <h2 id="discussions">Discussions</h2> <p>Firstly, note that tangent/cotangent space is an intrinsic? property. A usual image of interpreting tangent space is \(S^2\) embedded in \(\mathbb{R}^3\). Our definition above does not require so, and in fact the shift of perspective from extrinsic into intrinsic properties of geometric objects is a grand evolution starting from Gauss and Rieman.</p> <p>[TODO: An illustrative figure of a sphere embedded in Euclidean space]</p> <p>From derivations to curves:</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Intuition and definition of tangent/cotangent space]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)"/><published>2025-09-17T17:49:33+00:00</published><updated>2025-09-17T17:49:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Today I will cover a beautiful subject in differenetial geometry: Ricci flows and Ricci curvature.</p> <p>Generally, in Riemannian geometry, curvature measures how space bends. For instance, on a sphere, geodesics (shortest paths) come closer together compared to flat space; on a hyperbolic surface, they diverge.</p> <p>Ricci curvature is a particular way of summarizing curvature: Instead of describing how all directions bend (that‚Äôs what the full <strong>Riemann curvature tensor</strong> does), Ricci curvature focuses on <strong>volume distortion</strong>. More concretely, it tells us how the volume of a small geodesic ball deviates from the volume we‚Äôd expect in flat Euclidean space.</p> <p>Intuitively, we could summarize into the following:</p> <blockquote> <p>Positive Ricci curvature (like on a sphere) means geodesics tend to converge, and small balls have less volume than in flat space.</p> <p>Zero Ricci curvature (like in Euclidean space) means geodesics neither converge nor diverge, so volumes match Euclidean.</p> <p>Negative Ricci curvature (like on a hyperbolic space) means geodesics diverge, so small balls have more volume than Euclidean.</p> </blockquote> <p>Mathematically, Ricci curvature is obtained by ‚Äútracing‚Äù the <strong>Riemann curvature tensor</strong>. It compresses information about how different directions curve into a symmetric 2-tensor <code class="language-plaintext highlighter-rouge">Ric</code>.</p> <h3 id="riemannian-geometry-and-tensor">Riemannian Geometry and Tensor</h3> <p>The Riemann tensor is written in the following way:</p> \[R(X, Y)Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X, Y]} Z\] <p>This tensor captures all information about curvature. Succinctly, this is a 4-tensor: \(R_{ijkl}\).</p> <p>The above is an unfair treatment of Riemannian geometry. I‚Äôll have a separate blog on that subject soon.</p> <p>How to understand: Riemannian metric tensor informs the manifold where to expand, shrink, and curve. How does Riemannian metric tensor relate with curvature?</p> <h3 id="ricci-curvature">Ricci Curvature</h3> <p>Based on the Riemann tensor, what is the curvature?</p> <p>To get Ricci curvature, we take a <strong>trace</strong> of the Riemann tensor:</p> \[Ric_{ij} = R^{k}_{ikj} = g^{kl}R_{kilj}\] <p>This reduces the information down to a 2-tensor (like the metric itself). Geometrically, this represents the volume distortion of geodesic balls.</p> <h3 id="ricci-flow">Ricci Flow</h3> <p>Introduced by <code class="language-plaintext highlighter-rouge">Richard Hamilton (1982)</code>, Ricci flow is a process that evolves a Riemannian metric \(g(t)\) over time:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>The factor -2 is just convention (to simplify later computations). We could think of this as a heat equation for geometry: Just as heat diffuses to smooth out temperature differences, Ricci flow smooths out <strong>irregularities</strong> in curvature. As illustrated in the above section of Ricci curvature, we could naturally arrive at the result that positive curvature regions tend to shrink and negative curvature regions expand. Over time, the underlying geometry becomes more ‚Äúregular‚Äù, like ironing out wrinkles.</p> <p>The effect of Ricci flow on curvature could be expressed in the following way. The derivative of scalar curvature \(R\) under this flow is:</p> \[\frac{\partial R}{\partial t} = \Delta R + 2 |Ric|^2\] <p>This resembles a heat equation $(\Delta R)$ plus a positive correction. Consequently, curvature tends to diffuse out but also grows in positive-curvature regions.</p> <h4 id="examples-of-ricci-flow">Examples of Ricci Flow</h4> <p>Perhaps the simplest example is to imagine how a sphere evolves under Ricci flow. We all know that a round sphere has positive Ricci curvature. The Ricci flow equation says:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>Since Ricci is positive, the metric shrinks, and thus makes the sphere to contract uniformly, eventually collapsing to a point. This closely mirrors the idea that positive curvature makes geodesics converge. Under the Ricci flow, it tightens further.</p> <p>Another simple example is the flat torus. Since the Ricci curvature is 0 everywhere,</p> \[\frac{\partial g_{ij}}{\partial t} = 0\] <p>the torus will stay unchanged after the flow forever. This is analogous to heat diffusion on a perfectly uniform temperature field where nothing would effectively changes.</p> <p>Likewise, a hyperbolic surface has negative Ricci curvature. Consequently, under the Ricci flow, the metric would expand and the hyperbolic surface would grow larger and more uniform in curvature.</p> <p>In a nutshell, irregular geometries with bumps or folds (different curvature in different regions) get ‚Äúsmoothed‚Äù over time. All the high-curvature ‚Äúwrinkles‚Äù would get flatten out, like how heat equalizes temperature.</p> <h3 id="application-of-ricci-flow">Application of Ricci Flow</h3> <p>Poincare conjecture, Ricci flow, surgery theory, what Terrence Tao called ‚Äúone of the most impressive recent achievements of modern mathematics‚Äù</p> <p>Poincare conjecture:</p> <blockquote> <p>Any closed 3-manifold that is simply-connected, compact, and boundless is homeomorphic to a 3-sphere.</p> </blockquote> <p>Specifically, Poincare conjecture in higher dimensions has been solved around 1961, and dimension 4 case has been proved by Michael Freedman who by which won Fields medal in 1986. The \(n = 3\) case seemed really difficult to crack and it was only at 2002 that Grisha Perelman proved it using Ricci flow.</p> <p>Very briefly, since a sphere has positive curvature, by applying Ricci flow through time such sphere will contract and eventually vanish. Perelman proved the opposite also holds: if metric goes to 0, it must have been a sphere. To prove Poincare‚Äôs conjecture using Ricci flow,</p> <p>One of the most triumphant use of Ricci flow happens when Grigori Perelman (2002‚Äì2003) to prove <strong>the Poincar√© conjecture</strong> and the more general <strong>Thurston geometrization conjecture</strong>. He showed how Ricci flow with ‚Äúsurgery‚Äù (cutting and patching when singularities form) classifies 3-manifolds.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Introduction of Ricci flows/curvatures]]></summary></entry><entry><title type="html">The Dance of Space (2): Differential Forms (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/" rel="alternate" type="text/html" title="The Dance of Space (2): Differential Forms (in progress)"/><published>2025-09-16T16:34:22+00:00</published><updated>2025-09-16T16:34:22+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Differential_Forms</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/"><![CDATA[<p>The second episode to appreciate the inner workings of space is through the differential forms. Differential forms is not an ancient subject, ‚Ä¶</p> <h3 id="vector-outer-product">Vector Outer Product</h3> <h3 id="wedge-productexterior-derivative">Wedge Product/Exterior Derivative</h3> <h3 id="three-in-one">Three in One</h3> <p>Green‚Äôs theorem, Gauss‚Äô theorem, Stoke‚Äôs theorem.</p> <p>Green‚Äôs theorem:</p> \[\int_{L} Pdx + Qdy = \iint_{D}(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y})dxdy\] <p>Generalized Stoke‚Äôs theorem.</p> <p>A k-form is supposed to be integrated over an oriented k-dimensional manifold</p> <h3 id="fundamental-theorem-of-calculus-ftoc">Fundamental Theorem of Calculus (FTOC)</h3> <p>High dimensional Stoke‚Äôs theorem is exactly the fundamental theorem of calculus (FTOC) in high-dimensional space.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Intuite and define differential forms]]></summary></entry><entry><title type="html">Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Equivalence/" rel="alternate" type="text/html" title="Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)"/><published>2025-09-07T00:23:16+00:00</published><updated>2025-09-07T00:23:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Equivalence</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Equivalence/"><![CDATA[<p>Explain and compare multiple equivalences from differential geometry and topology, including homeomorphism, diffeomorphism, homotopy equivalence, homomorphism, isomorphism, etc.</p> <p>Invariant and equivariant functions (CNN is equivariant).</p> <p>Also discussions on cardinality among sets (including finite and infinite (countably infinite \(N0\)? and uncountably infinite), Hillbert Hotel problem, equipotent sets)</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Topology"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Compare various equivalences in geometry/topology/group theory]]></summary></entry></feed>