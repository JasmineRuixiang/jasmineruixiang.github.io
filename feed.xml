<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jasmineruixiang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasmineruixiang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-12T02:22:29+00:00</updated><id>https://jasmineruixiang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">High Dimensional Nitty-gritty (2): Z-scoring before PCA?</title><link href="https://jasmineruixiang.github.io/blog/2026/covariance/" rel="alternate" type="text/html" title="High Dimensional Nitty-gritty (2): Z-scoring before PCA?"/><published>2026-02-09T01:22:40+00:00</published><updated>2026-02-09T01:22:40+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/covariance</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/covariance/"><![CDATA[<p>This blog originates from a daily discussion of neural signal (pre)processing with my mentors and peers. People utilize z-scoring and PCA all the time, and it’s a little shameful to admit by hindsight that I haven’t dwelled on the following question deep enough. Again, we reencounter the conundrum in high dimensional observations haunted by irreducible noise, under which lies our ambitious intent to extract robust and effective information.</p> <hr/> <h3 id="0-problem-setup">0] Problem Setup</h3> <p>Let’s say we have a collection of neural data in the format \(X \in \mathbb{R}^{n \times d}\), where \(n\) is the number of samples, and \(d\) the number of features. These are raw features each coming from a single electrode. For simplicity let’s assume that there is only 1 kind of feature, like threshold crossing. Let’s further say that we want to visualize the low dimensional structure of this collection of data, preserving its global geometry as much as possible (we’ll clarify this later).</p> <p>Now we’d like to apply PCA on it for the first try, meaning to transform from \(\mathbb{R}^{n \times d}\) into \(\mathbb{R}^{n \times d'}\), where \(d'\) might be just \(3\), for example. Many methods would start by z-scoring \(X\) for each feature (so for each column of \(X\), after z-scoring would be mean 0 and standard deviation 1; for more discussions please refer to the <a href="https://jasmineruixiang.github.io/blog/2026/zscore/">previous blog</a> of this series) before applying PCA.</p> <p>I’m a little unsure about the motivation behind it. There’re of course many but I cannot pinpoint a conclusive answer.</p> <p>More importantly, I’m naively concerned that if we apply this feature-wise normalization, whether that would still preserve the covariance matrix between the original features (the diagonal will be 1, but I’m wondering how the off-diagonal terms would possibly change, or … would they).</p> <p>Let’s begin.</p> <hr/> <h3 id="1-what-pca-actually-does">1] What PCA actually does</h3> <p>The very first step: a quick review of what PCA is doing:</p> <blockquote> <p>Standard PCA:</p> <ol> <li>Center the data (subtracts column-wise/global means across samples; For the following, \(X\) will denote the centered data, if without specifications)</li> <li>Compute the <strong>covariance matrix</strong>: \(\Sigma = \frac{1}{n}X^TX\)</li> <li>Find eigenvectors of \(\Sigma\)</li> </ol> </blockquote> <p>PCA finds directions of maximal variance in the original coordinate system. We could also interpret PCA as finding minimization of reconstruction error, but that’s not explicitly helpful for deepening our interpretation here. However, I’ll provide another useful perspective in section 3] from the angle of constrained optimization, but this covariance interpretation is what we will grapple with now for this section —</p> <p>Because it already makes it obvious that</p> <blockquote> <p>PCA is sensitive to feature scale.</p> </blockquote> <p>If one electrode has variance 100 and another has variance 1, the first will dominate the principal components — even if its structure might not be more meaningful. And that’s exactly where z-scoring makes a huge distinction.</p> <hr/> <h3 id="2-what-does-z-scoring-do">2] What does z-scoring do?</h3> <p>Column-wise z-scoring transforms</p> \[\tilde{X}_{ij} = \frac{X_{ij} - \mu_j}{\sigma_j}\] <p>(Here’s a bit of abuse of notation, as \(\tilde{X}_{ij}, X_{ij}\) are scalars while \(\mu_j, \sigma_j \in \mathbb{R}^{1\times d}\))</p> <p>If we use matrix notation, \(D = diag{(\sigma_1, \cdots, \sigma_n)}\), then the above could be simplified into (again, assume it’s already centered):</p> \[\tilde{X} = XD^{-1}\] <p>Now if we look at the new covariance matrix:</p> \[\tilde{\Sigma} = \frac{1}{n}\tilde{X}^T\tilde{X} = \frac{1}{n}D^{-1}X^TXD^{-1} = D^{-1}\Sigma D^{-1}\] <p>This is obviously <strong>not the same</strong> covariance matrix.</p> <p>In fact, it’s not hard to see that, from simple linear algebra:</p> \[\tilde{\Sigma}_{ij} = \frac{\Sigma_{ij}}{\sigma_i \sigma_j}\] <p>And this turns out to be exactly the <strong>correlation matrix</strong>.</p> <p>So:</p> <blockquote> <p>PCA on z-scored data = PCA on the <code class="language-plaintext highlighter-rouge">correlation matrix</code></p> </blockquote> <blockquote> <p>PCA on raw centered data = PCA on the <code class="language-plaintext highlighter-rouge">covariance matrix</code></p> </blockquote> <p>So back to one of our original questions: does normalization preserve <code class="language-plaintext highlighter-rouge">covariance</code> between original features?</p> <p>No. The off-diagonal terms change as:</p> \[\frac{\Sigma_{ij}}{\sigma_i \sigma_j} \leftarrow \Sigma_{ij}\] <p>So they become <code class="language-plaintext highlighter-rouge">correlations</code> (or we could say that the correlations are preserved). The important distinction is that <code class="language-plaintext highlighter-rouge">covariance</code> measures co-variation in physical units, while <code class="language-plaintext highlighter-rouge">correlation</code> measures co-variation relative to each variable’s scale. So z-scoring does not preserve the original covariance geometry: It preserves the <code class="language-plaintext highlighter-rouge">correlation</code> <em>structure</em> instead.</p> <hr/> <h3 id="3-a-geometric-way-to-think-about-this">3] A geometric way to think about this</h3> <p>The above covariance calculation is clear, yet we could reinterpret PCA in a different way by variational characterization of eigenvectors, i.e. the Rayleigh quotient formulation of PCA (depending on your views, these two migth be considered the exact same thing; but even as an explanation for why we care about eigenvectors of the covariance, let me elaborate below).</p> <h4 id="31-pca-as-a-variational-problem">3.1] PCA as a Variational Problem</h4> <p>Let \(X \in \mathbb{R}^{n \times d}\) be the centered data and the sample covariance matrix</p> \[\Sigma = \frac{1}{n} X^\top X\] <p>If we project the data onto a direction \(v \in \mathbb{R}^d\), the projected variance is:</p> \[\mathrm{Var}(Xv) = \frac{1}{n} \|Xv\|^2 = v^\top \Sigma v\] <p>Therefore, the first principal component solves:</p> \[\max_{\|v\| = 1} v^\top \Sigma v\] <p>This is the Rayleigh quotient, and the solution is the top eigenvector of \(\Sigma\) (not proved here; many other sources exist online).</p> <h4 id="32-pca-after-z-scoring">3.2] PCA After Z-Scoring</h4> <p>Suppose we z-score each feature (column-wise normalization). Again, let me reiterate from the above that</p> \[D = \mathrm{diag}(\sigma_1, \dots, \sigma_d)\] <p>and the transformed data is:</p> \[\tilde{X} = X D^{-1}\] <p>The new covariance matrix becomes:</p> \[\tilde{\Sigma} = \frac{1}{n} \tilde{X}^\top \tilde{X} = D^{-1} \Sigma D^{-1}\] <p>PCA on z-scored data solves:</p> \[\max_{\|v\| = 1} v^\top D^{-1} \Sigma D^{-1} v\] <hr/> <h4 id="33-equivalent-reformulation-changing-the-constraint">3.3] Equivalent Reformulation (Changing the Constraint)</h4> <p>Now let’s do a simple trick. Let:</p> \[w = D^{-1} v \quad \text{so that} \quad v = D w\] <p>Substitute into the objective:</p> \[v^\top D^{-1} \Sigma D^{-1} v = (Dw)^\top D^{-1} \Sigma D^{-1} (Dw) = w^\top \Sigma w\] <p>Now examine the constraint:</p> \[\|v\|^2 = 1\] \[v^\top v = (Dw)^\top (Dw) = w^\top D^2 w\] <p>So the optimization becomes:</p> \[\max_{w^\top D^2 w = 1} w^\top \Sigma w\] <hr/> <h4 id="34-geometric-interpretation">3.4] Geometric Interpretation</h4> <p>The raw PCA solves:</p> \[\max_{v^\top v = 1} v^\top \Sigma v\] <p>Now, the z-scored PCA solves:</p> \[\max_{w^\top D^2 w = 1} w^\top \Sigma w\] <p>So at a glance from 2], z-scoring rescales the covariance matrix into the correlation matrix.</p> <p>But viewed from this different perspective, it <strong>changes the metric constraint</strong>. Instead of using the standard Euclidean norm:</p> \[v^\top v\] <p>we now use a weighted norm:</p> \[w^\top D^2 w\] <p>This means:</p> <ul> <li>Raw PCA assumes the <strong>standard Euclidean</strong> inner product.</li> <li>Z-scored PCA uses a different inner product <strong>induced by \(D^2\)</strong>.</li> </ul> <hr/> <h4 id="35-multi-dimensional-pca-k-components">3.5] Multi-Dimensional PCA (k Components)</h4> <p>Up to this point, you might object that the above is just to find one single direction/one principal component. Usually we do multiple components. Well, there isn’t too much effort for an extension.</p> <p>Raw PCA solves:</p> \[\max_{V^\top V = I} \mathrm{Tr}(V^\top \Sigma V)\] <p>where \(V \in \mathbb{R}^{d \times k}\). The solution is the top-\(k\) eigenvectors of \(\Sigma\) (proof omitted, similar as 3.1]). As in 3.2], After z-scoring, we solve:</p> \[\max_{V^\top V = I} \mathrm{Tr}(V^\top D^{-1} \Sigma D^{-1} V)\] <p>Using the substitution \(V = D W\), this naturally becomes (similar to 3.3]):</p> \[\max_{W^\top D^2 W = I} \mathrm{Tr}(W^\top \Sigma W)\] <hr/> <h4 id="36-generalized-eigenvalue-interpretation">3.6] Generalized Eigenvalue Interpretation</h4> <p>What does this mean geometrically about the solution?</p> <ul> <li>For the raw PCA: <ul> <li>Orthonormal basis in standard <strong>Euclidean</strong> metric</li> <li>Maximizes variance</li> </ul> </li> <li>For Z-scored PCA: <ul> <li>Orthonormal basis under <strong>weighted</strong> metric \(D^2\)</li> <li>This is equivalent to solving a <strong>generalized eigenvalue</strong> problem:</li> </ul> </li> </ul> \[\Sigma w = \lambda D^2 w\] <p>I’ll also skip the details of why the solution is equivalent to finding the generalized eigenvalues/eigenvectors. However, this fact informs us of the fundamental framework of PCA:</p> <p>the big geometric insight is that PCA always solves:</p> \[\max_{W^\top G W = I} \mathrm{Tr}(W^\top \Sigma W)\] <p>where \(G\) defines the metric.</p> <ul> <li>Raw PCA: \(G = I\)</li> <li>Z-scored PCA: \(G = D^2\)</li> </ul> <p>So z-scoring means that we are not trusting that Euclidean length in raw coordinates is meaningful. We redefine what unit length means.</p> <p>Which is consistent with the previous observation that</p> <ul> <li>Raw PCA preserves covariance geometry.</li> <li>Z-scored PCA preserves correlation geometry.</li> </ul> <hr/> <h3 id="4-what-geometry-are-we-preserving">4] What geometry are we preserving?</h3> <p>To some extent, this is the real conceptual issue.</p> <p>If we do PCA without z-scoring, it preserves the Euclidean geometry in the original feature space. Variance magnitude is meaningful because we indeed keep such information. We’d hold the underlying premise that</p> <blockquote> <p>Electrodes with larger variance are considered more important.</p> </blockquote> <p>This suggests that this methood is good if variance magnitude reflects real neural signal strength or the feature scale is physically meaningful.</p> <p>On the other hand, if we do PCA after z-scoring, then it would preserve geometry under a <strong>reweighted metric</strong> and all dimensions are treated equally. Each electrode is thus given equal prior importance.</p> <p>This should work if variance differences are arbitrary (e.g., electrode gain differences) and we care about patterns of co-variation, not absolute magnitude.</p> <p>Since neural data often has:</p> <ul> <li>Different firing rates across electrodes</li> <li>Different noise levels</li> <li>Different dynamic ranges</li> </ul> <p>If we don’t z-score:</p> <blockquote> <p>High firing-rate neurons dominate PCA.</p> </blockquote> <p>whereas if we do z-score:</p> <blockquote> <p>Each neuron contributes equally in variance units.</p> </blockquote> <hr/> <h3 id="5-which-one-preserves-global-geometry">5] Which one preserves global geometry?</h3> <p>This depends on what geometry we think is meaningful.</p> <p>If our raw space is: \(\mathbb{R}^d\) with standard Euclidean metric, then PCA without z-scoring preserves global geometry better. If we believe that true geometry should not depend on firing rate scale, then z-scoring defines a more appropriate metric (as elaborated in section 3]):</p> \[&lt;x, y&gt;_D = x^T(D^{-1})^{2}y\] <p>which means we could alternatively interpret this as keeping the underlying space unchanged but essentially altering the metric before doing PCA.</p> <p>Usually in systems neuroscience people often z-score across time and then do PCA. The reason behind is that neural manifold studies often care about relative population patterns, not which neuron fires more. If we want true population variance magnitude, then we should not z-score. If we intend to obtain population structure independent of scale, then z-score.</p> <hr/> <h3 id="6-summary">6] Summary</h3> <p>In conclusion, if we do z-scoring before PCA, then</p> <ul> <li>Z-scoring does NOT preserve the <code class="language-plaintext highlighter-rouge">covariance</code> matrix.</li> <li>It converts <code class="language-plaintext highlighter-rouge">covariance</code> to <code class="language-plaintext highlighter-rouge">correlation</code>.</li> <li>Off-diagonal terms are divided by product of standard deviations.</li> <li>Equivalently, we are changing the <strong>metric</strong> of the space.</li> <li>PCA result can change dramatically depending on scaling.</li> </ul> <p>Finally, in practice we often have more than one kind of features. For example, we could obtain both threshold crossings and spike band power from each electrode at the same time. However, these two measures have drastically different scales. In this scenario, of course we could look into them separately, but if combined, the spike power would dominate. Consequently, z-scoring also helps to re-weight the feature importance apriori.</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Brain Computer Interface"/><summary type="html"><![CDATA[Zscoring, covariance, PCA]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(4): from Geodesics Strip to Gauss Theorem Egregium</title><link href="https://jasmineruixiang.github.io/blog/2026/geodesics/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(4): from Geodesics Strip to Gauss Theorem Egregium"/><published>2026-02-08T14:58:02+00:00</published><updated>2026-02-08T14:58:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/geodesics</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/geodesics/"><![CDATA[<p>This weekend I was (re)thinking about the geometric connotations of geodesics, and that reminds me of a brilliant illustration with an intuitive method the eloquent mathematician Tristan Needham noted down in his remarkable and mind-numbing book <em>Visual Differential Geometry and Forms</em>. I thought just a little more, and only to recover a long-lasting misconception about Gauss Theorem Egregium which I was both shameful to admit for possessing so long and excited to have it cleared out of my mind.</p> <p>However, as I dive more into Needham’s method, I discovered a potential and simple failure mode, which naturally echoes another important statement: The Gauss-Bonnet Theorem. This deepens my grasp of the tension between <strong>local isometry</strong> (the peeling method, see below) and <strong>global topology</strong> (the area enclosed).</p> <p>Again, since this is the mumble series, I’ll not give all definitions and assume you have already known some flavor of the basics. Let’s begin.</p> <hr/> <h2 id="0-the-problem-confusion-setup">0] The Problem (Confusion) Setup</h2> <p>On Page 12-13, Tristan introduced a method to easily and intuitively construct geodesics on a curved surface (see figure [1.11] below). He claimed that “If a narrow strip surrounding a segment G of a geodesic is cut out of a surface and laid flat in the plane, then G becomes a segment of a straight line.” Well, he presented an intuitive proof (figure [1.12] below), but I’m immediately reminded of <strong>Gauss’ Theorem Egregium</strong> and there seemed to be something inconsistent (I’ll omit other information and count on you to look up the basics).</p> <p>I was thinking: the peeling is an (local) isometry without doubt, so geodesics have to be preserved. That’s why geodesics “peeled” off from the surface (the fruit/vegetable used in the illustration) has to be geodesics in Eucliean 2D space, which is straight according to the normal definitions. However, as I’m thinking a bit more deeply, the Gaussian curvature does change (before it’s nonzero, on 2D it’s zero, which contradicts Theorem Egregium), which leads to me to think backwards towards using this Theorem Egregium again. Immediately I realize that Gaussian theorem egregium is applied only to 2D surfaces (not to 1D curve since there’s no “Gaussian curvature” for a curve). Ah, shame to have blundered upon conceptual confusion. But treating this as an opportunity for an upgrade overhaul of my conceptual framework, let’s sort this out step by step and see what we might also dabble into.</p> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/VDGF/VDGF_1.11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/VDGF/VDGF_1.12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig [1.11] and [1.12] in <a class="citation" href="#VDGF">(Needham, 2021)</a>. </div> <h2 id="1-the-peeling-intuition">1] The “Peeling” Intuition</h2> <p>Let’s make it clear: in Figure [1.11] of Needham’s text, he introduces the following powerful intuitive tool:</p> <blockquote> <p>“If a narrow strip surrounding a segment $G$ of a geodesic is cut out of a surface and laid flat in the plane, then $G$ becomes a segment of a straight line.”</p> </blockquote> <p>At first glance, this indeed seems to challenge Gauss’s <strong>Theorema Egregium</strong>. Since Gaussian curvature (\(K\)) is an intrinsic invariant, how can a patch of a curved surface (\(K \neq 0\)) be laid flat (\(K = 0\)) without stretching or tearing?</p> <p>Well, the resolution lies in the <strong>limit</strong>. Gaussian curvature is a property of a 2D area. By taking a “narrow strip,” we are effectively reducing the 2D surface to a 1D curve and its immediate neighborhood. In the limit, as the width of the strip goes to zero, the “area” of the surface being considered goes to zero. In other words, the Gaussian curvature of the surface does not change because we aren’t flattening the whole surface; we are only flattening an “infinitesimally” narrow strip.</p> <hr/> <h2 id="2-geodesic-curvature-vs-gaussian-curvature">2] Geodesic Curvature vs. Gaussian Curvature</h2> <p>The Theorema Egregium applies to <strong>isometries</strong>. While we cannot map a patch of a sphere to a plane isometrically, we can map a curve and its first-order neighborhood to a plane isometrically (often called “developing” the strip; or just developable).</p> <p>We could think of it this way:</p> <blockquote> <p>The Surface: Has intrinsic curvature \(K\). The Strip: Because it is “infinitesimally narrow,” the Gaussian curvature doesn’t “<strong>trap</strong>” the strip. We aren’t forcing the 2D relationships across the strip to remain the same; we are only preserving the lengths along the geodesic \(G\).</p> </blockquote> <p>Consequently, the core of the questions (why such an intuition makes sense, beyond the simple proof shown in Fig.[1.12]) does not lie in Gauss’s Theorem, but in another concept: the geodesic curvature: The Theorema Egregium is about the surface (\(K\)), but the property of being a “straight line” is about Geodesic Curvature (\(\kappa_g\)):</p> <blockquote> <p>Gaussian Curvature (\(K\)) reflects the property of the surface. Geodesic Curvature (\(\kappa_g\)): A property of a curve relative to the surface. It measures how much the curve bends within the surface.</p> </blockquote> <p>The “straightness” of the peeled strip is governed by <strong>Geodesic Curvature ($\kappa_g$)</strong>, not Gaussian Curvature (\(K\)).</p> <ul> <li><strong>Gaussian Curvature (\(K\)):</strong> A property of the surface itself (\(K = \kappa_1 \kappa_2\)). It dictates whether a 2D patch can be flattened.</li> <li><strong>Geodesic Curvature (\(\kappa_g\)):</strong> A property of a curve <em>relative</em> to the surface. It measures how much the curve “veers” to the left or right within the surface.</li> </ul> <p>Recall that a geodesic is defined as a curve where \(\kappa_g = 0\) everywhere (Will do a another blog on the computation of geodesics from the perspective of modern (Riemannian) manifold). Because \(\kappa_g\) is an intrinsic property (it can be measured by flatlanders like intelligent ants living on the surface), it must be preserved under isometry.</p> <p>With this, we could demystify the underpinning logic of the peel:</p> <ol> <li>The act of peeling the strip is a local isometry along the curve.</li> <li>The \(\kappa_g\) of the curve on the surface was 0 (by definition of a geodesic).</li> <li>Therefore, the \(\kappa_g\) of the curve on the flat plane must also be 0 .</li> <li>In the Euclidean plane, a curve with zero curvature is a straight line.</li> </ol> <p>Notice that the 3rd step above relies on the fact that isometry preserves intrinsic properties and here the geodesic curvature, but again this is NOT Gauss Theorem Egregium (same fundamental fact but applied to different objects).</p> <p>Gaussian curvature is not defined for a 1D curve. A curve only has curvature (how much it bends in space) and geodesic curvature (how much it bends relative to the surface it’s on). By narrowing the strip until the 2D “surface” nature of the paper effectively vanishes, we are bypassing the restriction of the Theorema Egregium regarding the 2D area, while retaining the intrinsic measurement of the curve’s straightness. We are essentially “cheating” the theorem by reducing the 2D surface to a 1D line where the concept of Gaussian Curvature has no grip.</p> <p>The Theorem Egregium is a negative constraint (it tells us what we can’t do with a 2D patch), but the Geodesic Curvature (\(\kappa_g\)) is the positive proof. It yet emphasizes again that it’s important to separate the topological “budget” of the surface (the Gaussian Curvature) from the local behavior of the curve (the Geodesic Curvature).</p> <p>There’s only one last piece left to make the story rigorous.</p> <hr/> <h2 id="3-physical-approximation">3] Physical Approximation</h2> <p>Let me paraphrase again in another way. This narrow strip, in practice, since it cannot be infinitesimally narrow, by Theorem Egregium, <strong>CANNOT</strong> lie flat on the plane, but the more narrow we could get, the better aligned it is with respect to the plane. If it goes to limit, then Gaussian Theorem Egregium does not apply because this limit 1D curve is no longer constrained by this theorem.</p> <p>On the other hand, even in practice we assume that after peeling it “is” flat, this appeal to Theorem Egregium still does not “prove” the fact that once laid down the strip would become straight; we still need to assort to the preserved geodesic curvature, which is covered above.</p> <p>Now, back to the reality. In practice, a strip of finite width \(w\) cannot lie perfectly flat if \(K \neq 0\). The “error” or distortion required to flatten it is proportional to the area of the strip (\(Area \approx Length \times w\)). As \(w \to 0\):</p> <ul> <li>The area vanishes.</li> <li>The constraint of the Theorema Egregium vanishes.</li> <li>The 1D “straightness” remains perfectly preserved.</li> </ul> <p>Another way to view Needham’s trick is that the strip isn’t just being “flattened”; it is being identified with the unrolled <strong>tangent developable</strong> of the geodesic. If we imagine a sequence of tangent planes along the geodesic, they form a “ribbon” that has zero Gaussian curvature (because it’s a developable surface, like a cylinder or a cone). Because this ribbon has \(K=0\), it can be laid perfectly flat in a plane without any distortion at all. Needham’s “narrow strip” is essentially a physical approximation of this developable ribbon.</p> <hr/> <h2 id="4-extension-of-the-original-trick-connection-to-gauss-bonnet">4] Extension of the original trick: Connection to Gauss-Bonnet</h2> <h3 id="41-failure-mode-of-a-strip-when-in-closed-loop">4.1] Failure mode of a strip when in closed loop</h3> <p>Now with the above puzzle cleared out, let’s think a little deeper into the next single, perhaps most natural topic: How about peeling off not just a single strip segment, but instead a strip which loops back into itself?</p> <p>Think about the physical reality of Needham’s experiment. If we cut a strip along a great circle (a closed geodesic) of a sphere:</p> <ul> <li>1] The Segment: If we cut just a small arc (say 30°), the strip is essentially a tiny rectangle. Because it’s so narrow, the “surface tension” of the sphere’s curvature isn’t strong enough to prevent us from pressing it flat.</li> <li>2] The Loop: If we try to cut the entire great circle, we get a “ring” or a “hoop.” On the sphere, this hoop has a specific circumference (\(C = 2\pi R\)).</li> <li>3] The Failure: If we try to lay that hoop perfectly flat on a table without stretching it, we’ll find it impossible. To lie flat in Euclidean space as a circle, the relationship between its radius and circumference must be \(C = 2\pi r\). But on the sphere, the “radius” (the distance from the pole to the equator) is an arc length. <strong>The geometry of the 2D area inside the hoop “locks” the hoop’s shape</strong>.</li> </ul> <p>Or maybe another perhaps more intuitive example —</p> <blockquote> <p>The “Paper Cone” Analogy:</p> </blockquote> <p>Think of a paper cone (like a party hat).</p> <ul> <li> <p>1] Local Flattening: You can cut a narrow strip from the cone running from the tip to the base. You can lay this strip flat on the table perfectly. In fact, you can lay any part of the cone flat.</p> </li> <li> <p>2] Global Failure: But if you try to flatten the <em>entire</em> cone at once, you can’t. You have to make a cut. When you flatten it, the cut edges don’t meet; there is a angular gap.</p> </li> </ul> <p>Yet perhaps another thought experiment:</p> <blockquote> <p>The “Train Track” Experiment</p> </blockquote> <p>Imagine the “narrow strip” as a set of flexible but straight train tracks.</p> <p>On the Sphere: You lay the tracks along the equator. They go all the way around and connect perfectly at the start.</p> <p>The “Peeling” (Transfer to Plane): Now you transfer these tracks to a flat Euclidean floor. Because the tracks are geodesics (straight), you must lay them down as a straight line on the floor. You keep laying them down, inch by inch.</p> <p>Now the Problem: On the floor, a straight line goes on forever. It never comes back to start.</p> <p>The Contradiction: To make the tracks close a loop on the floor, you would have to bend them (add Geodesic Curvature). But we know geodesics are straight!</p> <p>So, the “narrow strip” of a closed geodesic loop on a sphere becomes an infinite straight line on the plane. It loses its “loop-ness” entirely.</p> <p>Later we will make it clear that this “angular gap” is precisely what the Gauss-Bonnet Theorem calculates (\(\iint K dA\)). The curvature \(K\) inside the loop on the sphere is responsible for “turning” the geometry so that it closes. The flat plane (\(K=0\)) lacks this “turning power,” so the strip simply runs away in a straight line.</p> <p>But anyway, for now, in short: We can flatten a line because a line has no “inside.” We cannot flatten a closed loop without accounting for the gap (which is what we will show later as the holonomy, or “equivalently” integration of the Gaussian curvature of the area it encloses.)</p> <hr/> <h3 id="42-the-formula-and-geodesic-loop">4.2] The formula and geodesic-loop</h3> <p>We resort to The Gauss-Bonnet Theorem, which almost fits in immediately, since it bridges the gap between the local straightness of the geodesic and the global curvature of the surface.</p> <p>Well, the Gauss-Bonnet Theorem is essentially a “budgeting” equation. It tells us exactly how much “straightness” we have to give up to account for the curvature of the surface. Let’s see if we could intuitively see its effect from the closed loop peeling failure.</p> <p>Let me state the theorem:</p> <p>For a simply connected region \(R\) bounded by a curve \(C\), the theorem states:</p> \[\iint_R K \, dA + \oint_C \kappa_g \, ds + \sum \alpha_i = 2\pi\] <p>\(K\) is the Gaussian curvature, \(\kappa_g\) is the geodesic curvature, \(\alpha_i\) are the exterior angles at any corners.</p> <p>Moreover, if we create a closed loop (like a triangle) using only geodesic segments, then \(\kappa_g = 0\) along the edges by definition. The middle term of the equation vanishes, leaving a direct relationship between the “area-integral of curvature” and how much the geodesics had to “turn” at the corners to close the loop:</p> \[\iint_R K \, dA = 2\pi - \sum \alpha_i\] <p>Furthermore, if we use a smooth closed geodesic, there is no geodesic curvature (\(\kappa_g = 0\)) and also no corners (\(\sum \alpha_i = 0\)). The equation thus becomes:</p> \[\iint_R K \, dA = 2\pi\] <p>This tells us that for a smooth closed geodesic to exist, the total Gaussian curvature of the area it encloses must equal \(2\pi\) (the “angle” of a full circle). This is why we can have a closed geodesic on a sphere (\(K &gt; 0\)), but we can never have a simple closed geodesic on a flat plane (\(K=0\)) or a saddle (\(K&lt;0\))—the “curvature budget” doesn’t add up to \(2\pi\).</p> <p>Let us dwindle here for a while, as I feel like building up intuition, a feeling of this curvature budge is of significant importance. So let’s just try again to reinterpret this equation by the following simple example.</p> <p>To walk in a simple closed loop (a circle, a square, a blob) and end up facing the same way we started, we must physically turn a total of 360° (\(2\pi\) radians).</p> <p>However, here the Gauss-Bonnet theorem says there are two ways to pay for this 360° budget:</p> <blockquote> <ol> <li>Steering (\(\kappa_g\)): we physically turn our body (like turning a steering wheel).</li> <li>Surface Curvature (\(K\)): The ground itself curves underneath us, effectively “turning” us without we realizing it.</li> </ol> </blockquote> <p>The equation is thus:</p> <p>\(\text{Steering} + \text{Surface Curvature} = 360^\circ\) \(\oint \kappa_g ds + \iint K dA = 2\pi\)</p> <p>Note that a geodesic is defined as a path where we do not steer. Our steering wheel is locked in the straight-ahead position.</p> <p>Therefore: Steering = 0.</p> <p>Now look at our budget equation again:</p> <p>\(0 + \text{Surface Curvature} = 360^\circ\) \(\iint K dA = 2\pi\).</p> <p>This means the only way to close a loop without steering is if the surface itself does 100% of the turning for us.</p> <p>Consequently, this would explain why it fails on the plane (\(K = 0\)). The surface or the plane is flat, which contributes 0° of turning. The geodesic contributes 0° of steering. Hence the result: \(0 + 0 = 0\). We have turned 0° and we are walking in a straight line forever. We will never close the loop. Thus our conclusion: simple closed geodesics cannot exist on a plane.</p> <p>Similarly, simple closed geodesics cannot exist on a saddle, because a saddle has negative curvature. It curves “away” from itself and contributes negative turning. Along a geodesic there’s no contribution of steering, and the net result is still negative, not \(2\pi\). We are actually diverging away from a closed loop. The surface is actively pushing us path apart.</p> <p>In contrast, a sphere has positive curvature. It curves “inward” toward itself and contributes positive turning. If we enclose enough area (specifically, a hemisphere), the total positive curvature adds up to exactly \(2\pi\). The surface has bent us around exactly enough to meet our own tail without us ever turning the wheel.</p> <p>In some sense, the “Curvature Budget” is like a tax. To close a loop, we must pay \(2\pi\). On a Geodesic, we refuse to pay (we won’t steer). Therefore, the Landlord (Surface) must pay the entire tax for us. Only a positively curved landlord (Sphere) has the cash (\(K&gt;0\)) to pay it. The plane is broke (\(K=0\)), and the saddle is in debt ($K&lt;0$).</p> <p>Well, you may wonder in the above interpretation where do the exterior angle sum go. Recall that we’re talking about smooth geodesic loops. Because the loop is smooth, there are no sharp corners (“kinks”), so there are no exterior angles to sum up. The term \(\sum \alpha_i\) becomes exactly 0. You may also wonder, then how would geodesic “turn” be different from the exterior angle kink turn? This is a little more subtle, but also intuitive.</p> <blockquote> <p>The “Conservation of Turning”</p> <ul> <li>Usually, when we smooth out a shape (like turning a square into a circle), we don’t lose the turning angles; we just spread them out.</li> <li>Square (Polygon): We walk straight (\(\kappa_g=0\)) and make four sharp \(90^{\circ}\) turns.</li> </ul> </blockquote> <ul> <li>\(\text{Turning} = \sum \alpha_i = 360^{\circ}\). I’d more think about these sharp turns/kinks as some colossal mechanical arm sticking from space and grabs our car to turn it around certain angles at the current position.</li> <li> <p>Circle (Smooth Curve): We never make a sharp turn (\(\alpha_i=0\)), but we are constantly steering a tiny bit to the side (\(\kappa_g = \text{constant}\)).</p> </li> <li>\(\text{Turning} = \oint \kappa_g \, ds = 360^{\circ}\).</li> </ul> <p>In both cases, we provided the turning. However, for the geodesic “miracle”, a smooth closed geodesic is a bizarre object because it refuses to turn in either way: there’s no sharp corners (smooth, \(\sum \alpha_i = 0\)) and no steering (geodesic, \(\kappa_g = 0\)). So where does the mandatory \(360^{\circ}\) (\(2\pi\)) turning come from to close the loop? It must come entirely from the Gaussian Curvature (\(K\)) of the area we enclosed. The surface itself has to rotate the universe under our feet by exactly \(360^{\circ}\) while we walk in a “straight” line.</p> <hr/> <h3 id="43-how-does-the-strip-fit-in">4.3] How does the “strip” fit in?</h3> <p>Now, imagine applying Needham’s “peeling” trick to each side of a geodesic triangle.</p> <blockquote> <ol> <li>On the surface: We would have three “straight” paths (geodesics) that enclose a region of curvature \(K\). Because of that \(K\), the interior angles sum to more than \(\pi\) (on a sphere).</li> <li><strong>The “Peeling” Conflict</strong>: If we tried to peel a “narrow strip” that followed the entire boundary of the triangle and lay it flat in one go, we would encounter a physical gap or an overlap where the ends meet.</li> </ol> </blockquote> <p>The Gauss-Bonnet Theorem effectively measures this “gap.” The amount of Gaussian curvature “trapped” inside the triangle is exactly equal to the “holonomy”—the amount a vector rotates when transported around that loop:</p> <hr/> <h3 id="44-holonomy-parallel-transport-and-the-gap">4.4] Holonomy, Parallel Transport, and the “Gap”</h3> <p>There’s a simple and intuitive relationship between holonomy, parallel transport, and exterior angles.</p> <p>Let’s start with a simple thought experiment about parallel transport (you might have seen this everywhere):</p> <p>Imagine walking along a geodesic triangle on a sphere, carrying a spear (a vector) pointing straight ahead.</p> <ul> <li>Along the edge: Because we are on a geodesic, we never turn our “steering wheel.” The spear stays parallel to our path.</li> <li>At the corner: we stop and turn our body by an exterior angle (\(\alpha_i\)). We do not turn the spear; it still points where it was pointing.</li> <li>Back at the start: When we complete the loop, we compare the spear’s current direction to its starting direction. They won’t match. This net rotation of the vector after the full trip, the “error” in direction, is called the <strong>Holonomy (\(\Delta \theta\))</strong>.</li> </ul> <p>The above way of sliding a vector along a geodesic while keeping it “parallel” is called <strong>Parallel Transport</strong>.</p> <p>At the same time, just by some simple calculation we would know that for the total turn on a flat plane, our total change in heading (sum of exterior angles \(\sum \alpha_i\)) must be \(2\pi\) <em>to close a loop</em>. In other words, the holonomy \(\Delta \theta = 0\). However, on a curved surface, the amount we actually turned, the holonomy, is \(2\pi - \sum \alpha_i \neq 0\).</p> <p>Also notice that if we do parallel transport along the “peeled” flat strip, the vector remains parallel in the Euclidean sense because the strip is a straight line.</p> <p>However, when we close the loop,</p> <ul> <li>On the flat plane, the vector would return to its start pointing in the original direction.</li> <li>On the curved surface, the vector returns rotated by an angle \(\Delta \theta\).</li> </ul> <p>The theorem tells us that this holonomy \(\Delta \theta\) is precisely the integral of the Gaussian curvature over the area we bypassed:</p> \[\Delta \theta = 2\pi - \sum\alpha_i = \iint_R K \, dA\] <p>Intuition Check: If you are on a flat plane, \(K=0\). Therefore, \(\iint K dA = 0\). This means \(0 = 2\pi - \sum \alpha_i\), or \(\sum \alpha_i = 2\pi\). This is just some high-school geometry rule that the exterior angles of any polygon sum to 360°. On a sphere, the curvature \(K\) “helps” us turn, so we don’t need as much “exterior angle” to close the loop.</p> <p>Consequently, if we apply Needham’s trick, i.e., peel off the strip of geodesic loop, we would find that</p> <blockquote> <p>While we can peel a <strong>single</strong> geodesic segment and lay it <strong>flat</strong> perfectly, we <strong>cannot</strong> peel a <strong>closed loop</strong> of geodesics and lay the resulting “frame” flat in the plane without a gap or overlap.</p> </blockquote> <p>The “angle” of that gap is the <strong>holonomy</strong>. The Gauss-Bonnet theorem tells us that this gap is exactly equal to the total Gaussian curvature “trapped” inside the area we just cut out.</p> <ul> <li><strong>On a Sphere (\(K&gt;0\)):</strong> The geodesics turn “toward” each other, and the interior angles sum to \(&gt;\pi\).</li> <li><strong>On a Saddle (\(K&lt;0\)):</strong> The geodesics flare “away” from each other, and the interior angles sum to \(&lt;\pi\).</li> </ul> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Needham’s trick works because a 1D line has no “area” to trap curvature. The “straightness” we see on the paper is the physical manifestation of zero geodesic curvature, an intrinsic property that survives the transition from the fruit’s skin to the flat desk.</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Manifold"/><category term="Differential Geometry"/><summary type="html"><![CDATA[Geodescis and its construction, Egregium, Gauss-Bonnet and Chern]]></summary></entry><entry><title type="html">High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring</title><link href="https://jasmineruixiang.github.io/blog/2026/zscore/" rel="alternate" type="text/html" title="High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring"/><published>2026-02-06T11:16:09+00:00</published><updated>2026-02-06T11:16:09+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/zscore</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/zscore/"><![CDATA[<p>This short blog provides a detailed, self-contained computational analysis of whether two z-scoring procedures applied to block-structured neural data are equivalent, and if indeed different how.</p> <hr/> <h2 id="1-problem-setup">1] Problem setup</h2> <p>Let’s say that we have neural data organized into blocks:</p> <ul> <li>Number of blocks: \(B\)</li> <li>Each block has data matrix in shape: \((x_{i, j}^b) \in \mathbb{R}^{t \times n}, \quad b = 1, \dots, B\)</li> <li>\(t\) = number of time points (samples)</li> <li>\(n\) = number of neural features</li> </ul> <p>Z-scoring is performed <strong>feature-wise</strong>, so all derivations below consider <strong>one fixed feature</strong> (column) at a time. The argument applies independently to every feature.</p> <hr/> <h2 id="2-notation-for-a-single-feature">2] Notation for a single feature</h2> <p>For a fixed feature \(j\):</p> <ul> <li> <p>Let \(x_{i,j}^{b} \in \mathbb{R}^{1}\) denote the data at time \(i\) for the feature \(j\) in block \(b\). For the following paragraphs, I will simplify \(x_{:,j}^{b}\) into \(x_{j}^{b} \in \mathbb{R}^{t\times 1}\), and \(x_{i,:}^{b}\) into \(x_{i}^{b} \in \mathbb{R}^{1\times n}\). Naturally, \(x^b = (x_{ij}^b) \in \mathbb{R}^{t\times n}\) with the same shape for all blocks. Basically, \(i\) corresponds to the index of time \(t\), and \(j\) the index of the number of neurons \(n\).</p> </li> <li> <p>Block-wise mean: \(\mu_{j}^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i, j} \in \mathbb{R}^{1}\),</p> <p>\(\mu ^{b} = [\mu_{1}^{b}, \cdots, \mu_{n}^{b}] \in \mathbb{R}^{1 \times n}\).</p> <p>Or, we could simply write \(\mu ^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i} \in \mathbb{R}^{1 \times n}\)</p> </li> <li> <p>Block-wise variance: \((\sigma_{j}^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i, j} - \mu_{j}^b)^2 \in \mathbb{R}^{1}\),</p> <p>\((\sigma ^{b})^2 = [(\sigma_{1}^{b})^2, \cdots, (\sigma_{n}^{b})^2] \in \mathbb{R}^{1 \times n}\),</p> <p>Or, we could simplify the above as \((\sigma ^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i} - \mu^b) \odot (x^{b}_{i} - \mu^b) \in \mathbb{R}^{1\times n}\)</p> <p>where \(\odot\) is the Hadamard product between two vectors (or just elementwise multiplication), defined as \(x \odot y = diag(x)y = (x_i y_i)_i \in \mathbb{R}^{1\times n},\) where \(x,y \in \mathbb{R}^{1\times n}\)</p> </li> </ul> <hr/> <h2 id="3-method-a-block-wise-z-scoring-concatenate-and-then-global-z-scoring">3] Method A: Block-wise z-scoring, concatenate, and then global z-scoring</h2> <h3 id="step-3a-z-score-within-each-block">Step 3a]: Z-score within each block</h3> <p>Each block is normalized independently:</p> \[z^{b}_{i} = \frac{x^{b}_i - \mu^b}{\sigma^b} \in \mathbb{R}^{1\times n}\] <p>Notice that this is element-wise division (to not over-complicate the symbols, I’ll use this abuse of notation for the following).</p> <p>By construction, for every block \(b\):</p> \[\frac{1}{t}\sum_{i=1}^t z^{b}_{i} = \vec{0} \in \mathbb{R}^{1\times n}, \qquad \frac{1}{t}\sum_{i=1}^t (z^{b}_{i} - \vec{0}) \odot (z^{b}_{i} - \vec{0}) = \vec{1} \in \mathbb{R}^{1\times n},\] <p>Consequently, each feature in each block has mean 0 and standard deviation 1. \(z^b\) observes the same notation rule as I described above for \(x^b\).</p> <hr/> <h3 id="step-3b-concatenate-all-normalized-blocks">Step 3b]: Concatenate all normalized blocks</h3> <p>Concatenate all \(z^{b}\) into a single vector of length \(Bt\).</p> <h4 id="global-mean">Global mean</h4> \[\frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B \vec{0} = \vec{0} \in \mathbb{R}^{1\times n}\] <h4 id="global-variance">Global variance</h4> \[\frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) \\ = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) = \frac{1}{B}\sum_{b=1}^B\vec{1} = \vec{1} \in \mathbb{R}^{1\times n}\] <hr/> <h3 id="step-3c-second-z-scoring">Step 3c]: Second z-scoring?</h3> <p>Since the concatenated data already has zero mean and unit variance, adding any other layers of z-scoring has no effect.</p> <p><strong>Final output of Method A:</strong></p> \[\boxed{z^{b} \in \mathbb{R}^{t\times n}}\] <p>for each block. So method A simply returns the block-wise standardized data.</p> <hr/> <h2 id="4-method-b-concatenate-first-then-global-z-scoring">4] Method B: Concatenate first, then global z-scoring</h2> <h3 id="step-4a-concatenate-raw-data">Step 4a]: Concatenate raw data</h3> <p>Concatenate all blocks \(x^{b}\) into a single matrix \(X \in \mathbb{R}^{Bt \times n}\).</p> <hr/> <h3 id="step-4b-compute-global-mean-and-standard-deviation">Step 4b]: Compute global mean and standard deviation</h3> \[\mu = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t x^{b}_i = \frac{1}{B}\sum_{b=1}^B \frac{1}{t}\sum_{i=1}^{t}x_i^b \\ = \frac{1}{B}\sum_{b = 1}^{B} \mu^b \in \mathbb{R}^{1\times n}\] <p>The global mean is the average of block-wise means (it’s not hard to show that if each block has different samples, this average will become <em>weighted average</em> by the ratio of the amount of each block’s data to total data amount).</p> <hr/> <h3 id="step-4c-compute-global-variance">Step 4c]: Compute global variance</h3> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (x^{b}_i - \mu) \odot (x^{b}_i - \mu) \in \mathbb{R}^{1 \times n}\] <p>Expand the product term:</p> \[(x^{b}_i - \mu) \odot (x^{b}_i - \mu) \\ = (x^{b}_i - \mu^b + \mu^b - \mu) \odot (x^{b}_i - \mu^b + \mu^b - \mu) \\ = (x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + 2(x^{b}_i - \mu^b) \odot (\mu^b - \mu) \\ + (\mu^b - \mu) \odot (\mu^b - \mu) \\\] <p>Let’s look at each of the terms more closely. Notice that when summing over \(i\) in \(\sigma^2\), the cross term vanishes:</p> \[\frac{1}{t}\sum_{i=1}^t ((x^{b}_i - \mu^b) \odot (\mu^b - \mu))\\ = (\frac{1}{t}\sum_{i=1}^t (x^{b}_i - \mu^b)) \odot (\mu^b - \mu)\\ = (\mu^b - \mu^b) \odot (\mu^b - \mu) \\ = \vec{0}\] <p>Thus,</p> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t((x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + (\mu^b - \mu) \odot (\mu^b - \mu)) \\ = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (\mu^b - \mu) \odot (\mu^b - \mu) \\ = \frac{1}{B}\sum_{1}^{B}\frac{1}{t}\sum_{i=1}^{t}(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\\ = \frac{1}{B}\sum_{1}^{B}(\sigma^b)^2 + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\] <p>This is a <strong>variance decomposition</strong> into:</p> <ul> <li>average within-block variance</li> <li>variance of block means (between-block variance)</li> </ul> <p>which meets our intuitive expectation (what else could it be anyway…).</p> <hr/> <h3 id="step-4d-global-z-scoring">Step 4d]: Global z-scoring</h3> <p>Each sample is normalized as:</p> \[y^{b}_i = \frac{x^{b}_i - \mu}{\sigma} \in \mathbb{R}^{1\times n}\] <p>By the nature of z-scoring, if we now calculate the global mean and standard deviation on concatenated \(y^b\), as in 3b] for each feature, we will obtain 0 and 1 respectively.</p> <p>Now if we rewrite the above using block-wise z-scores, since</p> <p>\(z_i^b = \frac{x_i^b - \mu^b}{\sigma^b} \rightarrow x_i^b = \sigma^bz_i^b + \mu^b\), then:</p> \[\boxed{ y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} }\] <p>which reinforces the idea that these all are just linear transformations: Transformations being linear, linear into one another.</p> <hr/> <h2 id="comparison-of-method-a-and-method-b">Comparison of Method A and Method B</h2> <p>Method A output:</p> \[z^{b}_i \in \mathbb{R}^{1\times n},\; \forall i, b\] <p>Method B output:</p> \[y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} \in \mathbb{R}^{1\times n}, \; \forall i, b\] <p>For the two methods to be identical \(\forall i,b\), we must have:</p> \[\sigma_b = \sigma \quad \text{and} \quad \mu_b = \mu \quad \forall b\] <p>And again, if we concatenate all \(y^b\) and \(z^b\) together separately into \(Y, Z\), they <strong>BOTH</strong> have feature-wise mean 0 and std 1.</p> <hr/> <h2 id="final-result">Final result</h2> \[\boxed{ \begin{array}{l} \text{The two procedures are NOT equivalent in general, even though} \\ \text{both yield global mean } 0 \text{ and std } 1 \text{ after transformations}. \end{array} }\] <p>They are equivalent <strong>if and only if</strong> every block already has identical feature-wise means and variances.</p> <ul> <li><strong>Method A</strong> removes all block-level mean and variance differences before concatenation.</li> <li><strong>Method B</strong> preserves block-level differences and normalizes relative to the pooled distribution.</li> </ul> <p>Block-wise z-scoring and global z-scoring <strong>do not commute</strong>. These choices encode different assumptions about whether block identity (e.g., session, subject, condition) should be preserved or discarded. Our choice should be driven by whether block-to-block variability is meaningful signal or nuisance variability in our analysis.</p> <p>Fun quesitons:</p> <ul> <li>1] What if block size \(t\) is not the same across all blocks?</li> <li>2] What are other (useful/effective) ways of normalization which also return the same mean/std (0/1, e.g.)?</li> </ul> <p>Practical question: In practice, how much do the statistics from these two methods actually differ? How should we interpret such differences?</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Brain Computer Interface"/><summary type="html"><![CDATA[Zscoring, block vs session level comparisons]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(3): Subspace geometry</title><link href="https://jasmineruixiang.github.io/blog/2026/subspace/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(3): Subspace geometry"/><published>2026-02-03T21:49:38+00:00</published><updated>2026-02-03T21:49:38+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/subspace</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/subspace/"><![CDATA[<h2 id="setup">Setup</h2> <p>Suppose we compute PCA on two datasets (e.g., train vs.\ test), and we keep the top-$r$ principal components. Let</p> \[U_{\text{train}} \in \mathbb{R}^{d \times r}, \qquad U_{\text{test}} \in \mathbb{R}^{d \times r},\] <p>where each matrix has <strong>orthonormal columns</strong> (so each is a basis for an $r$-dimensional subspace of $\mathbb{R}^d$).</p> <p>Our first obvious intuition might be to define the <strong>subspace overlap matrix</strong> as the following:</p> \[S = U_{\text{train}}^{\top} U_{\text{test}} \in \mathbb{R}^{r \times r}.\] <p>So, each entry is a dot product between basis vectors:</p> \[S_{ij} = u_i^{(\text{train})} \cdot u_j^{(\text{test})}.\] <p>At first glance, this looks like a direct “basis alignment” comparison, which is exactly what we aim for. How else could we characterize subspace change other than looking at pairs-wise relationships among two sets of basis vectors? Well, there’re a few caveats…</p> <hr/> <h2 id="1-why-basis-by-basis-alignment-is-unstable">1] Why basis-by-basis alignment is unstable</h2> <p>Comparing PCA vectors one-by-one (e.g., “PC1 vs.\ PC1”) is unstable because PCA eigenvectors are <strong>not uniquely defined</strong> in two common cases. The following problems might pop up —</p> <h3 id="i-sign-flips">(i) Sign flips</h3> <p>If \(u\) is an eigenvector, then \(-u\) is also an eigenvector.</p> <p>So dot products can flip sign even when the <em>subspace is identical</em>.</p> <h3 id="ii-degenerate--near-degenerate-eigenvalues-rotation-inside-the-subspace">(ii) Degenerate / near-degenerate eigenvalues (rotation inside the subspace)</h3> <p>If</p> \[\lambda_i \approx \lambda_{i+1},\] <p>then the corresponding principal directions inside the 2D span can rotate dramatically under tiny perturbations (noise, finite-sample effects, etc.).</p> <p>This means that even if the <em>span</em> is essentially the same, the individual vectors $u_i$ can change a lot.<br/> So comparing “PC1 to PC1” is not meaningful.</p> <hr/> <h2 id="2-what-singular-values-do-that-dot-products-dont">2] What singular values do that dot products don’t</h2> <p>The matrix</p> \[S = U_{\text{train}}^{\top} U_{\text{test}}\] <p>depends on the <em>chosen bases</em> inside each subspace. If we change bases within either subspace via orthogonal transformations:</p> \[U_{\text{train}} \to U_{\text{train}} R_1, \qquad U_{\text{test}} \to U_{\text{test}} R_2,\] <p>where $R_1, R_2 \in \mathbb{R}^{r \times r}$ are orthogonal (including sign flips as a special case), then</p> \[S \to (U_{\text{train}}R_1)^{\top}(U_{\text{test}}R_2) = R_1^{\top} S R_2.\] <p>So the <em>entries</em> of $S$ can change wildly.</p> <h3 id="key-fact-invariance">Key fact (invariance)</h3> <blockquote> <p>The <strong>singular values</strong> of \(S\) are invariant under left/right orthogonal rotations:</p> </blockquote> <ul> <li>Left-multiplying by an orthogonal matrix does not change singular values.</li> <li>Right-multiplying by an orthogonal matrix does not change singular values.</li> </ul> <p>Therefore, even if PCA “relabels,” flips signs, or rotates the basis vectors within the subspace, the <strong>singular values remain unchanged</strong>.</p> <p>This means singular values capture a property of the <strong>subspaces</strong>, not of the particular eigenvectors chosen.</p> <hr/> <h2 id="3-geometric-meaning-principal-angles">3] Geometric meaning: principal angles</h2> <p>Take the SVD:</p> \[S = Q \Sigma R^{\top},\] <p>where</p> \[\Sigma = \mathrm{diag}(\sigma_1,\dots,\sigma_r).\] <p>A fundamental result is:</p> \[\sigma_i = \cos(\theta_i),\] <p>where $\theta_i$ are the <strong>principal angles</strong> between the two $r$-dimensional subspaces.</p> <p>Interpretation:</p> <ul> <li>$\theta_i = 0 \implies$ perfectly aligned direction exists (since $\cos(\theta_i)=1$)</li> <li>$\theta_i = 90^\circ \implies$ orthogonal direction (since $\cos(\theta_i)=0$)</li> </ul> <p>So the singular values summarize <em>how much overlap</em> the two subspaces have along their best-aligned directions.</p> <hr/> <h2 id="4-why-this-is-the-stable-comparison">4] Why this is the “stable” comparison</h2> <p>Think of $U_{\text{train}}$ and $U_{\text{test}}$ as <strong>arbitrary coordinate systems</strong> inside their respective subspaces.</p> <p>A meaningful comparison should ignore that arbitrariness.</p> <p>Principal angles / singular values do exactly this: they compute the <strong>best possible matching</strong> between directions in the two subspaces.</p> <p>Instead of comparing “PC1 $\leftrightarrow$ PC1,” we solve an optimal alignment problem:</p> \[\max_{\|a\|=\|b\|=1} a^{\top}\bigl(U_{\text{train}}^{\top}U_{\text{test}}\bigr)b,\] <p>and the sequence of best matches yields</p> \[\sigma_1,\sigma_2,\dots\] <p>as the strengths of alignment along the best-aligned directions.</p> <hr/> <h2 id="5-tiny-example-intuition-2d-case">5] Tiny example intuition (2D case)</h2> <p>Suppose both subspaces are actually the same 2D plane in $\mathbb{R}^d$.</p> <p>You could pick:</p> <ul> <li>$U_{\text{train}}$ = standard basis in that plane</li> <li>$U_{\text{test}}$ = same plane but rotated by $45^\circ$ inside it</li> </ul> <p>Then $S$ might look like a rotation matrix:</p> \[S= \begin{pmatrix} \cos 45^\circ &amp; -\sin 45^\circ \\ \sin 45^\circ &amp; \cos 45^\circ \end{pmatrix}.\] <p>The entries are not the identity, so basis-by-basis dot products look “not aligned.”</p> <p>But the singular values of a rotation matrix are both $1$.</p> <p>So singular values correctly say: <strong>the subspaces are identical</strong>.</p> <p>That’s the whole point.</p> <hr/> <h2 id="bottom-line">Bottom line</h2> <p>We use singular values of</p> \[U_{\text{train}}^{\top}U_{\text{test}}\] <p>because:</p> <ul> <li>✅ they are invariant to sign flips / rotations / re-ordering of PCA vectors inside the subspace</li> <li>✅ they define principal angles, which are a true subspace-to-subspace comparison</li> <li>✅ they give a stable measure of drift even when eigenvectors are not uniquely defined</li> </ul> <hr/> <h2 id="a-sidenote-connection-to-projection-distance">A sidenote: Connection to projection distance</h2> <p>Let $P_{\text{train}}$ and $P_{\text{test}}$ be the orthogonal projection matrices onto the two subspaces:</p> \[P_{\text{train}} = U_{\text{train}}U_{\text{train}}^{\top}, \qquad P_{\text{test}} = U_{\text{test}}U_{\text{test}}^{\top}.\] <p>Then one can show the Frobenius-distance relationship:</p> \[\|P_{\text{train}} - P_{\text{test}}\|_F^2 = 2r - 2\|U_{\text{train}}^{\top}U_{\text{test}}\|_F^2 = 2\sum_{i=1}^r \sin^2(\theta_i).\]]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Subspace Geometry and Computation]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (4): Connections (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (4): Connections (in progress)"/><published>2026-01-15T15:48:02+00:00</published><updated>2026-01-15T15:48:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(4)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/"><![CDATA[<p>This is episode 4 on the smooth manifold series. Today we will be exploring more on tangent vectors, and another key concept related to tangent spaces for different tangent planes: connections.</p> <h2 id="intuition-directional-derivatives">Intuition: Directional Derivatives</h2> <p>🌐 The Connection: Bridging Derivatives from $\mathbb{R}^3$ to Curved Manifolds The concept of a connection is the necessary tool that allows us to perform differential calculus on curved spaces (manifolds), such as the surface of a sphere. It generalizes the familiar idea of the directional derivative from flat Euclidean space ($\mathbb{R}^3$).</p> <ol> <li>Directional Derivatives in Euclidean Space ($\mathbb{R}^3$) In $\mathbb{R}^3$ with Cartesian coordinates $(x, y, z)$, the directional derivative provides a simple way to measure change. The basis vectors $\left{ \mathbf{i}, \mathbf{j}, \mathbf{k} \right}$ (or the equivalent operators $\left{ \frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z} \right}$) are constant, allowing us to define derivatives simply as component-wise partial derivatives. Let $p=(1, 2, 0)$ be a point, $X=(y, -x, 3x)$ be the direction vector field, and $V=(xz, y^2, -2x)$ be a vector field. A. Derivative of a Scalar Function ($D_X f$) This is the directional derivative of a smooth scalar function $f$ in the direction $X$.</li> </ol> <p>Perspective Formula Example Result for f(x,y,z)=xy2+z at p Vector-Based (Calculus) $D_X f = \nabla f \cdot X$ $D_X f(p) = \langle 4, 4, 1 \rangle \cdot \langle 2, -1, 3 \rangle = \mathbf{7}$ Point Derivation (Geometry) $X[f] = X^i \frac{\partial f}{\partial x^i}$ $Xf = (y^3 - 2x^2y + 3x)\big</p> <p>This confirms that in differential geometry, a tangent vector $X$ is rigorously defined as a point derivation—an operator that mimics the directional derivative by satisfying the Leibniz rule. B. Derivative of a Vector Field ($D_X V$) This derivative measures how the vector field $V$ changes as we move in the direction $X$. In $\mathbb{R}^3$, this is calculated by taking the directional derivative of each component of $V$. Using the vector fields $X$ and $V$: The $k$-th component of the resulting vector $D_X V$ is $(D_X V)^k = \sum_{i} X^i \frac{\partial V^k}{\partial x^i}$. Component 1 (i.e., $k=1$): $(D_X V)^1 = yz + 3x^2$ Component 2 (i.e., $k=2$): $(D_X V)^2 = -2xy$ Component 3 (i.e., $k=3$): $(D_X V)^3 = -2y$ Evaluating at $p=(1, 2, 0)$ gives:</p> <p>\(D_X V(p) = \langle 3, -4, -4 \rangle\)</p> <ol> <li>The General Connection: The Covariant Derivative ($\nabla_X V$) The formula for $D_X V$ fails on a curved manifold $M$ because the tangent spaces $T_p M$ and $T_q M$ at nearby points $p$ and $q$ are distinct. We cannot simply subtract the vector $V(p)$ from $V(q)$. A Connection ($\nabla$) is the rule that provides the necessary “correction” to define the derivative intrinsically on $M$. The resulting derivative is called the covariant derivative $\nabla_X V$. A. Definition and Axioms The connection is an operator $\nabla: C^{\infty}(M) \times C^{\infty}(M) \to C^{\infty}(M)$ that maps two vector fields, $X$ and $V$, to a new vector field $\nabla_X V$, satisfying: Linearity over Functions in $X$: $\nabla_{fX} V = f \nabla_X V$ Linearity in $V$: $\nabla_X (aV + bW) = a \nabla_X V + b \nabla_X W$ Leibniz Rule: $\nabla_X (fV) = (Xf) V + f \nabla_X V$ (where $Xf$ is the directional derivative of $f$) B. The Coordinate Form and Christoffel Symbols In local coordinates, the covariant derivative $\nabla_X V$ is defined using the Christoffel symbols ($\Gamma^k_{ij}$), which represent the rate of change of the coordinate basis vectors $\left{ \frac{\partial}{\partial x^i} \right}$:</li> </ol> <p>\((\nabla_X V)^k = \underbrace{X^i \frac{\partial V^k}{\partial x^i}}_{\text{I. Flat-Space Derivative Term}} + \underbrace{X^i \Gamma^k_{ij} V^j}_{\text{II. Curvature Correction Term}}\) The Christoffel symbols $\Gamma^k_{ij}$ are defined by the action of the connection on the basis vectors:</p> <p>\(\nabla_{\frac{\partial}{\partial x^i}} \frac{\partial}{\partial x^j} = \sum_{k} \Gamma^k_{ij} \frac{\partial}{\partial x^k}\) Difference from Euclidean Case: In $\mathbb{R}^3$ with Cartesian coordinates, $\Gamma^k_{ij} = 0$, and the second term vanishes, resulting in $\nabla_X V = D_X V$. On a curved manifold, $\Gamma^k_{ij} \neq 0$, and the correction term is essential.</p> <ol> <li>The Levi-Civita Connection In Riemannian Geometry, a Riemannian metric $g$ is introduced to measure lengths and angles. The Levi-Civita Connection is the unique connection that respects this metric structure. It is defined by two crucial properties: Metric Compatibility: The connection must preserve the metric $g$ under parallel transport.</li> </ol> <p>\(X(g(V, W)) = g(\nabla_X V, W) + g(V, \nabla_X W)\) Zero Torsion: The connection must satisfy:</p> \[\nabla_X Y - \nabla_Y X = [X, Y]\] <p>where $[X, Y]$ is the Lie bracket. The Christoffel symbols of the Levi-Civita Connection are thus entirely determined by the components of the metric $g_{ij}$ and their first derivatives:</p> \[\Gamma^k_{ij} = \frac{1}{2} g^{k\ell} \left( \frac{\partial g_{j\ell}}{\partial x^i} + \frac{\partial g_{i\ell}}{\partial x^j} - \frac{\partial g_{ij}}{\partial x^\ell} \right)\] <h2 id="connections">Connections</h2> <h2 id="christoffel-symbols">Christoffel Symbols</h2> <h2 id="levi-civita-connections">Levi-Civita Connections</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Exploration of Connection]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)"/><published>2026-01-14T20:22:25+00:00</published><updated>2026-01-14T20:22:25+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(3)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/"><![CDATA[<p>This is episode 3 on the smooth manifold series. Today we will be diving into the properties of maps between manifolds. I will first summarize how to understand and compute the differential of a smooth map between manifolds, both abstractly and concretely, culminating in the matrix-valued example<br/> \(F(A) = A^\top A.\)</p> <hr/> <h2 id="1-differential-canonical-definition">1] Differential: canonical definition</h2> <h3 id="11-differential-of-a-smooth-map-intrinsic-definition">1.1 Differential of a Smooth Map (Intrinsic Definition)</h3> <p>Let \(F : M \to N\) be a smooth map between smooth manifolds.</p> <p>For any point $ p \in M $, the <strong>differential</strong> is a linear map which is a <strong>pushforward of derivations</strong>. \(dF_p : T_p M \longrightarrow T_{F(p)} N.\)</p> <h3 id="derivation-based-definition">Derivation-based definition</h3> <p>If $ v \in T_p M $ is a tangent vector viewed as a derivation, then \((dF_p v)(g) := v(g \circ F), \qquad g \in C^\infty(N).\)</p> <p>This definition is <strong>coordinate-free</strong>.</p> <p>It might appear at first both unnecessarily abstract and underestimated as to its computation. We might claim that it is essentially just a Jacobian matrix in local coordinates. However, the essence of this concept resides on its definition to be conceptually a coordinate-independent linear map between tangent spaces.</p> <hr/> <h3 id="12-coordinate-representation-and-the-jacobian">1.2 Coordinate Representation and the Jacobian</h3> <p>To compute $ dF_p $ in practice:</p> <ol> <li>Choose a chart $ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>Choose a chart $ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ol> <p>Define the coordinate expression: \(\tilde F = \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Then: \(\boxed{ dF_p \;\text{is represented by}\; D\tilde F(\varphi(p)) }\)</p> <p>That is, <strong>the Jacobian matrix is the coordinate representation of the differential</strong>.</p> <blockquote> <p>The Jacobian depends on coordinates; the linear map $ dF_p $ does not.</p> </blockquote> <hr/> <h3 id="13-why-this-is-not-just-the-jacobian">1.3 Why this is not “just” the Jacobian</h3> <p>The Jacobian depends on coordinates;<br/> $ dF_p $ does not.</p> <p>More precisely:</p> <ul> <li>$ dF_p $ is a <strong>geometric linear map</strong></li> <li>The Jacobian is a <strong>matrix representation</strong> of that map in chosen bases: \(\frac{\partial \bigl(\psi^1 \circ F,\;\dots,\;\psi^n \circ F\bigr)} {\partial \bigl(x^1,\;\dots,\;x^m\bigr)}\)</li> </ul> <p>If you change charts, the matrix changes by:</p> \[\boxed{ J_{\text{new}} = D\psi\,\cdot\, J_{\text{old}} \,\cdot\, (D\varphi^{-1}) }\] <p>but the underlying linear map $ dF_p $ stays the same.</p> <hr/> <h2 id="2-differential-alternative-interpretation">2] Differential: alternative interpretation</h2> <h3 id="21-curve-based-definition">2.1 Curve-based definition</h3> <p>This is also coordinate-free:</p> <p>If<br/> \(\gamma : (-\varepsilon,\varepsilon) \to M\) is a smooth curve with \(\gamma(0) = p \quad \text{and} \quad \gamma'(0) = v \in T_p M,\) then \(\boxed{ dF_p(v) = (F \circ \gamma)'(0) \in T_{F(p)} N. }\)</p> <p>No coordinates anywhere. This viewpoint is often the most intuitive and is fully equivalent to the derivation definition.</p> <hr/> <h3 id="22-curves-in-local-coordinates">2.2 Curves in local coordinates</h3> <p>Choose charts:</p> <ul> <li>$ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>$ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ul> <p>Define the coordinate representation: \(\tilde F := \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Now define the coordinate curve: \(\tilde\gamma := \varphi \circ \gamma : (-\varepsilon,\varepsilon) \to \mathbb{R}^m.\)</p> <p>Consequently, in coordinates the statement is the following:</p> \[\boxed{ D\tilde F(\varphi(p)) \cdot \tilde\gamma'(0) = D(\psi \circ F \circ \varphi^{-1})(\varphi(p)) \cdot (\varphi \circ \gamma)'(0). }\] <p>In the coordinate formula, \(\gamma'(0) \quad \text{really means} \quad \tilde\gamma'(0) = (\varphi \circ \gamma)'(0).\)</p> <p>Here’s a diagram that makes everything explicit</p> \[\begin{array}{ccc} T_p M &amp; \xrightarrow{dF_p} &amp; T_{F(p)} N \\ \downarrow d\varphi_p &amp; &amp; \uparrow d\psi^{-1}_{\psi(F(p))} \\ \mathbb{R}^m &amp; \xrightarrow{D\tilde F(\varphi(p))} &amp; \mathbb{R}^n \end{array}\] <p>Thus:</p> <ul> <li>$ \gamma’(0) $ lives in $ T_p M $</li> <li>$ (\varphi \circ \gamma)’(0) = d\varphi_p(\gamma’(0)) \in \mathbb{R}^m $</li> <li>$ D\tilde F(\varphi(p)) $ acts on that coordinate vector</li> </ul> <hr/> <h3 id="23-sidenote-graph-differential">2.3 Sidenote: Graph Differential</h3> <p>The <strong>graph</strong> of $ F $ is \(\Gamma_F = \{ (p, F(p)) \mid p \in M \} \subset M \times N.\)</p> <p>Define the graph map: \(\Phi : M \to M \times N, \quad \Phi(p) = (p, F(p)).\)</p> <p>Its differential is: \(\boxed{ d\Phi_p(v) = (v, dF_p(v)). }\)</p> <p>This is what is often called the <strong>graph differential</strong>.</p> <hr/> <h2 id="3-special-case-maps-between-vector-spaces">3] Special Case: Maps Between Vector Spaces</h2> <h3 id="31-a-great-simplification">3.1 A great simplification</h3> <p>If $ M = \mathbb{R}^m $, $ N = \mathbb{R}^n $, then: \(T_p M \cong \mathbb{R}^m, \quad T_{F(p)} N \cong \mathbb{R}^n.\)</p> <p>In this case:</p> <ul> <li>$ dF_p $ is a linear map $ \mathbb{R}^m \to \mathbb{R}^n $</li> <li>Its matrix is exactly the <strong>Jacobian matrix</strong></li> <li>$ dF_p(H) $ coincides with the <strong>Fréchet / directional derivative</strong></li> </ul> <hr/> <h3 id="32-worked-example--fa--atop-a-">3.2 Worked Example: $ F(A) = A^\top A $</h3> <p>Let \(F : \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}, \quad F(A) = A^\top A.\)</p> <p>Since $ \mathbb{R}^{n \times n} $ is a vector space, \(T_A(\mathbb{R}^{n \times n}) \cong \mathbb{R}^{n \times n}.\)</p> <h4 id="321-direct-computation">3.2.1 Direct computation</h4> <p>There are several ways to compute the differential. The most straight-forward method is to do the following (as you might imagine):</p> <p>For \(H \in T_A(\mathbb{R}^{n \times n})\), \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} (A+tH)^\top(A+tH).\)</p> <p>Expanding: \((A+tH)^\top(A+tH) = A^\top A + t(H^\top A + A^\top H) + t^2 H^\top H.\)</p> <p>Thus: \(\boxed{ dF_A(H) = H^\top A + A^\top H. }\)</p> <h4 id="322-curve-based-computation">3.2.2 Curve-based computation</h4> <p>Let \(A(t)\) be a smooth curve with: \(A(0)=A, \quad A'(0)=H.\)</p> <p>Then: \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} A(t)^\top A(t) = H^\top A + A^\top H.\)</p> <p>This makes it clear that the definition of \(dF_A(H)\) is <strong>coordinate-free</strong>.</p> <p>Sidenote: if we restrict \(A\) to symmetric or SPD matrices, what will we see? Or what if we connect this to Riemannian geometry on \(GL(n)\) or \(SPD(n)\)? We’ll come back to this later when we discuss Lie group and Lie algebra.</p> <hr/> <h2 id="4-back-to-classical-regular-surfaces-parametrizations-immersions">4] Back to classical regular surfaces (parametrizations, immersions)</h2> <h3 id="the-question-in-do-carmo-diffgeom">The Question in do Carmo DiffGeom</h3> <p>In do Carmo’s definition of a <strong>regular surface</strong> in \(\mathbb{R}^3\), a coordinate map \(X : U \subset \mathbb{R}^2 \to \mathbb{R}^3\) is required to satisfy two conditions:</p> <ol> <li>\(X\) is a <strong>differentiable homeomorphism</strong> onto its image.</li> <li>The differential \(dX_p\) is <strong>injective</strong> at every point $p \in U$.</li> </ol> <p>Since \(X\) maps from \(\mathbb{R}^2\) to \(\mathbb{R}^3\), its differential can never be surjective, so injectivity (rank 2) is the meaningful requirement.</p> <p>A natural question arises:</p> <blockquote> <p>If \(X\) is already a differentiable homeomorphism, isn’t its differential automatically injective?</p> </blockquote> <p>The answer is <strong>no</strong>.</p> <p>Well, to make it explicit, let’s figure out first what a differentiable homeomorphism actually gives us: If \(X : U \to \mathbb{R}^3\) is a differentiable homeomorphism onto its image, then:</p> <ul> <li>\(X\) is <strong>continuous and injective</strong></li> <li>\(X^{-1}\) is <strong>continuous</strong> (but <em>not</em> necessarily differentiable)</li> <li>Topologically, \(X(U)\) looks like a 2‑dimensional surface</li> </ul> <p>This is a <strong>topological</strong> statement plus differentiability of $X$. It controls <em>points</em>, but says nothing about what happens to <em>directions</em>. Crucially, differentiability of the inverse is <em>not</em> assumed.</p> <p>Fine, but then why injectivity of the differential is not automatically assured? Notice that the differential</p> \[dX_p : \mathbb{R}^2 \to \mathbb{R}^3\] <p>being injective means it has <strong>rank 2</strong> meaning no tangent direction is collapsed. A map can be:</p> <ul> <li>injective,</li> <li>continuous with continuous inverse,</li> <li>differentiable,</li> </ul> <p>and <em>still</em> have rank drop somewhere. Let me give a concrete example:</p> <p>Consider \(X(u,v) = (u^3, v, 0).\)</p> <p>Properties of this map:</p> <ul> <li>It is <strong>injective</strong></li> <li>It is a <strong>homeomorphism onto its image</strong></li> <li>It is differentiable everywhere</li> </ul> <p>However, its differential is \(dX_{(u,v)} = \begin{pmatrix} 3u^2 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix}\)</p> <p>At \($u = 0\), this matrix has <strong>rank 1</strong>, not 2. One tangent direction is squashed. Therefore, this map is <strong>not an immersion</strong>, and it does <strong>not</strong> define a regular surface in do Carmo’s sense.</p> <hr/> <p>However, you might observe here a seemingly apparent paradox: “But the image is <strong>Just</strong> a plane!”</p> <p>Well, the image of \(X(u,v) = (u^3, v, 0)\) is \({(x,y,0) : x,y \in \mathbb{R}},\) which is the entire (xy)-plane indeed. As a <strong>subset</strong> of \(\mathbb{R}^3\), this plane is perfectly flat, so one might expect its tangent plane at every point to be the whole plane. So then why does the tangent collapse under this map?</p> <hr/> <p>The key point here is that here are <strong>two distinct notions</strong> at play:</p> <ol> <li>The tangent plane of a <strong>subset</strong> of \(\mathbb{R}^3\)</li> <li>The tangent plane <strong>defined by a parametrization</strong></li> </ol> <p>In do Carmo’s approach, tangent planes are defined <em>via parametrizations</em>. The tangent plane at a point is \(T_pS = \operatorname{span}{X_u(p), X_v(p)}.\)</p> <p>For the map above: \(X_u = (3u^2, 0, 0), \quad X_v = (0,1,0).\)</p> <p>At (u=0): \(X_u(0,v) = (0,0,0), \quad X_v(0,v) = (0,1,0),\) so the span is <strong>1‑dimensional</strong>.</p> <p>This means:</p> <blockquote> <p>The parametrization fails to distinguish two independent directions in the parameter domain.</p> </blockquote> <p>Geometrically, the $u$-direction has been crushed.</p> <hr/> <p>But does the plane still have a 2D tangent plane?</p> <p>Yes — but <strong>not via this parametrization</strong>.</p> <p>If instead we parametrize the same plane by \(Y(s,t) = (s,t,0),\) then \(dY = \begin{pmatrix} 1 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix},\) which has rank 2 everywhere.</p> <p>So the <em>same subset</em> becomes a <strong>regular surface</strong> under a different map.</p> <hr/> <p>Finally, what does this meana conceptually? The key lesson is:</p> <blockquote> <p><strong>Regularity is not a property of the subset alone — it is a property of the subset together with its smooth structure.</strong></p> </blockquote> <p>Different parametrizations can induce:</p> <ul> <li>a <strong>good</strong> smooth structure (immersion)</li> <li>or a <strong>bad</strong> one (rank collapse)</li> </ul> <p>This is why do Carmo requires that <strong>there exists</strong> a local parametrization with injective differential.</p> <hr/> <p>In other words, if you ask “Does that mean there might exist other maps which make the plane a regular surface?”, then the answer is Yes — absolutely. The plane <em>is</em> a regular surface because such maps exist.</p> <p>Behind that is another question: “Does immersion entirely depend on which maps we pick?” Yes!</p> <ul> <li><strong>Immersion is a property of the map</strong>, not of the set.</li> <li>A <strong>regular surface</strong> is a set for which <em>good immersions exist everywhere</em>.</li> </ul> <p>In this seense, do Carmo does separate the conditions, being deliberately modular:</p> <ol> <li><strong>Homeomorphism</strong> → good topology (no self‑intersections)</li> <li><strong>Injective differential</strong> → good differential geometry</li> </ol> <p>whether neither condition implies the other.</p> <p>Finally, some of key points:</p> <ul> <li>The same subset of \(\mathbb{R}^3\) can support <strong>many different smooth structures</strong></li> <li>Differential geometry only works once a smooth structure is fixed</li> <li>Parametrizations are how do Carmo <em>builds</em> that structure</li> </ul> <p>This is why modern texts often say:</p> <blockquote> <p>“A surface is a 2‑dimensional smooth manifold embedded in $$\mathbb{R}^3$.”</p> </blockquote> <p>Do Carmo reaches this notion <strong>from parametrizations upward</strong>, rather than assuming it at the start.</p> <hr/> <blockquote> <p><strong>Topology sees points.</strong> <strong>Differential geometry sees directions.</strong></p> </blockquote> <p>A homeomorphism controls points. Injectivity of the differential controls directions.</p> <p>You need <strong>both</strong> to get a regular surface.</p> <h2 id="5-association-with-inverse-function-theorem">5] Association with inverse function theorem</h2> <h2 id="6-immersion-submersion">6] Immersion, submersion</h2> <p>Immersion basics</p> <p>Here I want to emphasize one key fact that:</p> <blockquote> <p>immersion imply a nonzero determinant in coordinates</p> </blockquote> <p>Why? Again, let’s recall that immersion means the differential is injective Let<br/> \(\phi:M^m\to N^n,\qquad m\le n\) and let $p\in M$.<br/> Saying <strong>(\phi) is an immersion at (p)</strong> means the differential \(d\phi_p:T_pM\to T_{\phi(p)}N\) is <strong>injective</strong>.</p> <p>Equivalently (in linear algebra language):<br/> \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Now, if write it in local coordinates → Jacobian matrix has rank \(m\) Pick coordinate charts:</p> <ul> <li>on \(M\): \(x=(x^1,\dots,x^m)\) around \(p\)</li> <li>on \(N\): \(y=(y^1,\dots,y^n)\) around \(\phi(p)\)</li> </ul> <p>Then locally \(\phi\) looks like a smooth map between Euclidean spaces: \(y^a = \phi^a(x^1,\dots,x^m),\qquad a=1,\dots,n.\)</p> <p>Its differential in these coordinates is represented by the <strong>Jacobian matrix</strong> \(J(p)=\left(\frac{\partial \phi^a}{\partial x^i}(p)\right)\) which is an (n\times m) matrix.</p> <p>The immersion condition says: \(\operatorname{rank}(J(p))=m.\)</p> <p>So the columns of \(J(p)\) are linearly independent.</p> <p>Now we use a standard linear algebra fact:</p> <blockquote> <p>An \(n\times m\) matrix has rank \(m\) <strong>iff</strong> there exists an \(m\times m\) submatrix (choose \(m\) rows) whose determinant is nonzero.</p> </blockquote> <p>Why?</p> <ul> <li>If <strong>every</strong> \(m\times m\) minor determinant were zero, then <strong>every</strong> set of $m$ rows would be linearly dependent, so the rank would be \(&lt;m\).</li> <li>Since the rank is \(m\), at least one choice of $m$ rows gives an invertible \(m\times m\) matrix.</li> </ul> <p>Concretely: there exist indices \(1\le a_1&lt;\cdots&lt;a_m\le n\) such that the matrix \(\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)_{\alpha,i}\) has \(\det\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)\neq 0.\)</p> <p>If we now <strong>rename/reorder the target coordinates</strong> so that those special indices become \(1,\dots,m\), then we can assume:</p> \[\det\left(\frac{\partial (\phi^1,\dots,\phi^m)}{\partial (x^1,\dots,x^m)}(p)\right)\neq 0.\] <p>That’s exactly the statement: in coordinates (after renumbering if needed), an immersion gives a nonzero determinant of an \(m\times m\) Jacobian block.</p> <p>In one line summary: an immersion means \(\phi\) “doesn’t collapse any tangent directions,” so locally you can find $m$ coordinate functions of \(\phi\) that vary independently — and “vary independently” is exactly “Jacobian block has nonzero determinant.”</p> <hr/> <h4 id="side-note-connection-to-the-constant-rank-theorem--local-normal-form-of-an-immersion">Side note: Connection to the Constant Rank Theorem / local normal form of an immersion?</h4> <p>Remember that Constant Rank Theorem (specialized to immersions) says: Let \(\phi:M^m\to N^n\) be smooth, and suppose \(\phi\) is an <strong>immersion at \(p\)</strong>.<br/> That means \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Then the constant rank theorem says:</p> <blockquote> <p>There exist coordinate charts<br/> \((U,x)\ \text{around }p,\qquad (V,y)\ \text{around }\phi(p)\) such that in these coordinates the map becomes \(y\circ \phi\circ x^{-1}(u^1,\dots,u^m) \;=\; (u^1,\dots,u^m,0,\dots,0).\)</p> </blockquote> <p>So locally, \(\phi\) looks like the <strong>standard inclusion</strong> \(\mathbb{R}^m \hookrightarrow \mathbb{R}^n,\qquad u\mapsto (u,0).\)</p> <p>That is the precise geometric meaning of “immersion.”</p> <p>In these special coordinates, \(\phi^1(u)=u^1,\;\dots,\;\phi^m(u)=u^m,\qquad \phi^{m+1}(u)=0,\dots,\phi^n(u)=0.\)</p> <p>So the Jacobian matrix is literally \(J= \begin{pmatrix} I_m\\ 0 \end{pmatrix}\) (an \(n\times m\) matrix).</p> <p>Now look at the top \(m\times m\) block: \(\frac{\partial(\phi^1,\dots,\phi^m)}{\partial(u^1,\dots,u^m)} = I_m,\) so \(\det(I_m)=1\neq 0.\)</p> <p>That’s exactly the coordinate statement.</p> <p>How this matches the “minor is nonzero” argument? Before using the constant rank theorem, we only know:</p> <ul> <li>\(J(p)\) has rank \(m\)</li> <li>therefore some \(m\times m\) minor determinant is nonzero</li> </ul> <p>Then the constant rank theorem tells us that we can actually <strong>choose coordinates</strong> so that the “good minor” becomes the <em>first</em> \(m\) coordinates, and the map becomes \($(u,0)\).</p> <p>So:</p> <ul> <li><strong>Linear algebra fact:</strong> full rank \(\Rightarrow\) some minor \(\neq 0\)</li> <li><strong>Constant rank theorem:</strong> we can change coordinates to make that minor the obvious identity matrix.</li> </ul> <p>The geometric picture (why \((u,0)\) is the right normal form) is that an immersion means \(\phi\) “injects tangent vectors,” so locally \(\phi(U)\subset N\) is an $m-dimensional “sheet” sitting inside an n-dimensional space. In good coordinates on N, that sheet looks like:</p> \[\{(y^1,\dots,y^n): y^{m+1}=\cdots=y^n=0\},\] <p>i.e. an embedded copy of \(\mathbb{R}^m\\).</p> <p>So locally, \(\phi\) is just a parametrization of that sheet.</p> <h2 id="7-embedding">7] Embedding</h2> <h3 id="72-a-key-difference-between-embedding-and-immersion">7.2 A key difference between embedding and immersion</h3> <p><strong>“no self-intersections” is one of the key geometric consequences of being <em>embedded</em></strong> (as opposed to merely <em>immersed</em>). As stated above, if a manifold \(M\) is <strong>embedded</strong> in \(\mathbb{R}^k\), it sits inside \(\mathbb{R}^k\) as a “nice subset,” like a surface you could physically draw without crossing itself. More formally, an <strong>embedding</strong> \(F: M \to \mathbb{R}^k\) means:</p> <ol> <li>\(F\) is a <strong>smooth immersion</strong> (its differential is injective everywhere), and</li> <li>\(F\) is a <strong>homeomorphism onto its image</strong> \(F(M)\) (with the subspace topology).</li> </ol> <p>That second condition is exactly what rules out the classic “self-crossing” pathology. If the image “intersects itself” in the sense that two different points \(p\neq q\in M\) map to the same point in \(\mathbb{R}^k\), i.e. \(F(p)=F(q),\) then \(F\) is <strong>not injective</strong>, so it can’t be an embedding.</p> <p>So: <strong>an embedded submanifold cannot cross itself as a set in \(\mathbb{R}^k\)</strong>.</p> <p>On the other hand, an <strong>immersion</strong> can look like a manifold with self-crossings in \(\mathbb{R}^k\). An example will be the “figure-eight curve” in \(\mathbb{R}^2\) can be parametrized smoothly with nonzero derivative everywhere, so it’s an immersion, but it’s <strong>not embedded</strong> because it fails injectivity / fails to be a homeomorphism onto its image.</p> <p>In shoft,</p> <ul> <li><strong>Embedded \(\Rightarrow\)</strong> injective + “topologically correct” inclusion<br/> \(\Rightarrow\) <strong>no self-intersections</strong>.</li> <li><strong>Immersed \(\Rightarrow\)</strong> locally nice but can globally overlap<br/> \(\Rightarrow\) <strong>self-intersections possible</strong>.</li> </ul> <h2 id="8-submanifolds">8] Submanifolds</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Explore relations of maps between manifolds]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)"/><published>2025-09-17T23:30:02+00:00</published><updated>2025-09-17T23:30:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(2)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/"><![CDATA[<p>This is episode 2 on the smooth manifold series. Today we will be diving into concepts that appear initially very intuitive at first glance, but then the extended version of which is indeed quite abstract.</p> <h2 id="0-familiar-examples">0] Familiar examples</h2> <p>Let me start with two simple concrete examples to illustrate what tangent vectors and tangent space are. Indeed they match up to our intuition!</p> <p>Let’s first say that we have a unit circle in \(\mathbb{R}^2\), or basically let’s denote it as \(S^1\) (this is a standard notation as</p> <p>\(S^{n} = \{ x \in \mathbb{R}^{n+1} \mid |x| = 1 \}\),</p> <p>representing the surface of an \((n+1)\)-dimensional ball). Pick \(p = (1, 0)\). If I ask you what the tangent vector is starting at \(p\) and tangnet to \(S^1\)? Your answer is probably a vector pointing upward or downward with its tail at \(p\). Indeed,</p> <h2 id="1-three-equivalent-definitions-of-tangent-space">1] Three equivalent definitions of Tangent Space</h2> <p>We’ll cover three equivalent definitions tangent space.</p> <h3 id="1-tangent-vectors-are-equivalence-classes-of-curves">[1] Tangent vectors are equivalence classes of curves</h3> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>tangent vectors are equivalence classes of smooth curves through \(p\):</p> \[T_pM = \{\frac{d}{dt}\Big|_{t = 0} \gamma(t) \Big| \gamma: (\epsilon, \epsilon) \rightarrow M, \; \gamma(0) = p \}\] <p>where \(\gamma_1 \sim \gamma_2\) if in some (equivalently, any) coordinate chart \(\phi: U \subset M \rightarrow \mathbb{R}^n\):</p> \[\frac{d}{dt}\Big|_{t = 0} \phi(\gamma_1(t)) = \frac{d}{dt}\Big|_{t = 0} \phi(\gamma_2(t))\] <p>Consequently, each tangent vector is represented by the velocity of a curve through \(p\).</p> <h3 id="2-tangent-vectors-are-derivations-at-p">[2] Tangent vectors are derivations at \(p\)</h3> <p>Let \(M\) be a manifold, the tangent space at \(p \in M\), denoted as \(T_pM\), is the the vector space of all derivations at \(p\). Notice that a derivation at \(p\) is a <strong>linear</strong> operator (at \(p\)):</p> \[D: C^{\infty}_{p}(M) \rightarrow \mathbb{R}\] <p>satisfying the Leibniz rule:</p> \[D(fg) = f(p)D(g) + g(p)D(f), \; \forall f, g \in C^{\infty}(M)\] <p>where \(C^{\infty}_{p}\) is the equivalence class of \((f, U)\), where \(f \in C^{\infty}\) and \(U\) is a neighbourhood of \(p\). Two functions \((f, U)\) and \((g, V)\) are equivalent iff \(\exist \; W \subset U \cap V\) a neighbourhood of \(p\) such that \(f(x) = g(x), \forall x \in W\), where \(g \in C^{\infty}\) and \(V\) is a neighbourhood of \(p\).</p> <p>Intuitively, \(D\) is like a directional derivative operator acting on smooth functions near \(p\). Consequently,</p> \[T_p{M} = \{ D \mid D \; \mathrm{is\ a\ derivation\ at} p \}\] <p>and a tangent vector \(v \in T_p{M}\) is a derivation \(D\). In fact, Tu (Theorem 2.2, An introduction to manifolds) showed that there’s a bijection between derivations at \(p\) and directional derivaties.</p> <h3 id="3-tangent-vectors-as-equivalences-on-function-germs">[3] Tangent vectors as equivalences on function germs</h3> <h3 id="local-coordinate-description">Local coordinate description</h3> <p>Specifically if we adopt interpretation [2], then with \((U, \phi)\) a chart with coordinates \((x^1, \dots, x^n)\) near \(p\)</p> \[\{ \frac{\partial}{\partial x^1}\Big|_p, \dots, \frac{\partial}{\partial x^n}\Big|_p \}\] <p>form a basis of \(T_pM\).</p> <p>Thus any tangent vector \(v\) can be written uniquely as</p> \[v = \sum_{i = 1}^{n} v^i \frac{\partial}{\partial x^i}\Big|_p\] <p>where \((v^1, \dots, v^n)\) are the components of the vector in this coordinate system.</p> <h2 id="2-equivalence-between-derivations-and-curves">2] Equivalence between derivations and curves</h2> <p>Perhaps not so surprisingly, the above two definitions are compatible and even equivalent to one another.</p> <p>From curves to derivations:</p> <h2 id="3-a-concrete-computational-example">3] A concrete computational example</h2> <p>A coherent walkthrough using the sphere \(S^2\)</p> <p>Let’s re-emphasize again that a vector field on a Manifold is defined as the following:</p> <p>Let \(M\) be a smooth manifold. A <strong>vector field</strong> on \(M\) is a smooth section/assignment \(p \mapsto X_p \in T_p M\) where \(T_p M\) is the tangent space at \(p\).</p> <p><strong>Fundamental viewpoint</strong>:</p> <blockquote> <p>A tangent vector is a <strong>derivation</strong>: a linear map \(X_p : C^\infty(M) \to \mathbb{R}\) satisfying the Leibniz rule.</p> </blockquote> <p>We’ll also give the local coordinates and coordinate vector fields as the following:</p> <p>Let \((U, \varphi), \quad \varphi : U \subset M \to V \subset \mathbb{R}^n\) be a coordinate chart, with coordinates \((x^1, \dots, x^n).\)</p> <p>Each coordinate \(x^i\) is itself a <strong>function on the manifold</strong>: \(x^i : U \to \mathbb{R}.\)</p> <p>Definition of the “Coordinate Vector Field”:</p> <p>The coordinate vector field \(\left.\frac{\partial}{\partial x^i}\right|_p\) is defined <strong>intrinsically</strong> by its action on smooth functions: \(\boxed{ \left.\frac{\partial}{\partial x^i}\right|_p(f) := \frac{\partial}{\partial x^i} \big(f \circ \varphi^{-1}\big) \Big(\varphi(p)\Big) }\) where \(x^i\) is the \(i\)-th coordinate on \(\mathbb{R}^n\).</p> <p>Notice that there’s an abuse of notation (the left “partial” is what we define, whereas the right partial is the usual partial differentiation). Hopefully, it’s obvious that the above definition stems from the following basic definition of graph differential (which I will talk more into next time):</p> \[\boxed{ (dF_p(v))(f) := v_p(f \circ F) }\] <p>More importantly, this definition:</p> <ul> <li>uses <strong>only</strong> the chart,</li> <li>does <strong>not</strong> require an embedding and thus does $NOT$ give us components in $\mathbb{R}^n$</li> <li>explains what “\(\partial f / \partial x^i\)” actually means: It means the ordinary partial derivative of the coordinate expression of $f$ after pulling $f$ back to a coordinate chart.Note that nothing is being differentiated on $\mathbb{R}^n$ unless we explicitly choose an embedding (see below).</li> </ul> <hr/> <p>Now, let’s take a look at the simple example: Sphere \(S^2\) and spherical coordinates. Given the manifold \(S^2 = \{(x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}\)</p> <p>Coordinate chart (away from poles)</p> <p>\((\theta, \varphi)\) with embedding map \(F(\theta,\varphi) = (\sin\varphi\cos\theta,\; \sin\varphi\sin\theta,\; \cos\varphi).\)</p> <p>Here:</p> <ul> <li>\(\theta, \varphi\) are <strong>functions on \(S^2\)</strong>,</li> <li>not abstract variables.</li> </ul> <p>According to the above discussions, the <strong>intrinsic</strong> meaning of \(\partial / \partial \theta\) and \(\partial / \partial \varphi\) are the following:</p> <p>For any \(f : S^2 \to \mathbb{R}\), \(\frac{\partial}{\partial \theta}(f) = \frac{\partial}{\partial \theta} \big(f \circ \varphi^{-1}\big)(\theta,\varphi), \quad \frac{\partial}{\partial \varphi}(f) = \frac{\partial}{\partial \varphi} \big(f \circ \varphi^{-1}\big).\)</p> <p>Again, this is the <strong>definition</strong>, not an interpretation. This offers us one way to calculate the vector field/tangent vectors at $\forall p \in M$.</p> <p>If we set</p> <p>\(f(x, y, z) = z\), then we could pull it back and obtain \((f \circ \phi^{-1})(\theta, \phi) = \cos(\phi)\). Consequently,</p> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X_p(f)= \frac{\partial \cos(\phi)}{\partial \theta} = 0\] <p>at $p = (\theta, \phi)$.</p> <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y_p(f)= \frac{\partial \cos(\phi)}{\partial \phi} = -\sin(\phi)\] <p>at $p = (\theta, \phi)$.</p> <hr/> <p>However, because \(S^2 \subset \mathbb{R}^3\), we may compute concrete representatives by viewing it as an embedded in $\mathbb{R}^3$ and to extrinsically compute the vector fields by pushforward of coordinate basis vectors via the embedding map of the manifold. Consequently, such computation lives in the embedded picture, not the intrinsic definition.</p> \[\frac{\partial}{\partial \theta} = \frac{\partial F}{\partial \theta} = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0)\] \[\frac{\partial}{\partial \varphi} = \frac{\partial F}{\partial \varphi}= (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi)\] <p>These are <strong>actual vectors in \(\mathbb{R}^3\)</strong> tangent to \(S^2\).</p> <blockquote> <p>Important:<br/> This is <strong>not the definition</strong> of coordinate vector fields —<br/> it is a <strong>representation</strong> using the embedding.</p> </blockquote> <p>Consequently, we could act on functions without pulling back to coordinate space:</p> <p>Let \(f(p) = z(p)\) be a function on \(S^2\).</p> <p>Choose an extension \(\tilde f(x,y,z) = z \quad\Rightarrow\quad \nabla \tilde f = (0,0,1).\)</p> <p>Here’s the key observation:</p> <blockquote> <p>Once a tangent vector is represented in \(\mathbb{R}^3\), its action on $f$ is given by the directional derivative of an extension of $f$.</p> </blockquote> <p>Formally, \(\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\) where</p> <ul> <li>$\tilde{f}$ is any smooth extension of $f$ to $\mathbb{R}^3$</li> <li>$X_p \in T_pS^2 \subset \mathbb{R}^3$.</li> </ul> <p>This works because tangent vectors annihilate normal components.</p> <hr/> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X(f)= (0,0,1) \cdot (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0) = 0\] <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y(f)= (0,0,1) \cdot (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi) = -\sin\varphi\] <p>Same results as the intrinsic definition.</p> <p>Note that to be very concrete, in the above examples, at $p = (\theta, \phi)$, the tangent vector is:</p> <p>$X_p = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\;0)$ (we could interpret this as horizontal circles of latitude) and $Y_p = (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\ -\sin\varphi)$. This should make it very clear that we are treating the tangent vectors as actually “vectors” living in $\mathbb{R}^3$ like how we usually refer them to be. With this extrinsic embedding, the general form of a vector field coincides with the intrinsic notion, but more tangible:</p> <p>$X = a(\theta, \phi)\frac{\partial}{\partial \theta} + b(\theta, \phi)\frac{\partial}{\partial \phi}$,</p> <p>then as a vector in $\mathbb{R}^3$: $X_p = a \frac{\partial F}{\partial \theta} + b\frac{\partial F}{\partial \phi}$, and</p> <p>$X_p(f) = \nabla \tilde{f}(p) \cdot X_p$</p> <p>Notice that for this computation no coordinate pullback is required. But to be crystal clear, we do <strong>not</strong> compute the vector field “without coordinates”. We simply You replaced intrinsic coordinates on $\mathbb{S}^2$ by ambient Cartesian coordinates in $\mathbb{R}^3$. So coordinates are still there — just in a different space.</p> <hr/> <p>Before we continue, let’s dwell on this formula for a while:</p> \[\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\] <p>This identity guarantees consistency between 1] the derivation definition and 2] the embedded vector representation (Question: is this just chain rule?s). It explains why the extrinsic calculation agrees with the intrinsic definition. If we have a nice embedding, two views are nice. However, if we go to the general abstract case without the ambient space, we could only assort to the intrinsic definition.</p> <hr/> <p>To be rigorous, you may be concerned about the specific use of the function extension here. However, we could show that extensions do not matter, because <strong>“tangent vectors annihilate normal components”</strong>. Let me explain below:</p> <p>If \(g : \mathbb{R}^3 \to \mathbb{R}\) satisfies \(g|_{S^2} = 0,\) then for all \(v \in T_p S^2\), \(v(g) = 0.\) Equivalently, \(\nabla g(p) \perp T_p\mathbb{S}^2\)</p> <hr/> <h3 id="proof-via-curves">Proof (via curves):</h3> <p>Let \(\gamma(t) \subset S^2\) with \(\gamma(0)=p, \quad \gamma'(0)=v.\)</p> <p>Since \(g(\gamma(t)) = 0\) for all \(t\), \(v(g) = \frac{d}{dt} g(\gamma(t))\big|_{t=0} = 0.\)</p> <hr/> <h3 id="concrete-example">Concrete example</h3> <p>Let \(g(x,y,z) = x^2 + y^2 + z^2 - 1.\)</p> <p>Then: \(\nabla g = (2x,2y,2z)\) which is normal to \(S^2\).</p> <p>For any tangent vector \(v\), \(v(g) = \nabla g \cdot v = 0.\)</p> <hr/> <h3 id="consequence-crucial">Consequence (crucial)</h3> <p>If \(\tilde f_1\) and \(\tilde f_2\) are two extensions of \(f\), then \(g = \tilde{f_1} - \tilde{f_2}\) vanishes on $\mathbb{S}^2$. For any tangent vector $v$:</p> \[v(\tilde f_1) - v(\tilde f_2) = v(\tilde g) = 0\] <p>thus:</p> \[v(\tilde f_1) = v(\tilde f_2) \quad\forall v \in T_p S^2.\] <p>Hence directional derivatives along tangent vectors are <strong>well-defined</strong>: this is why directional derivatives along tangent vectors do not depend on how f extends off the manifold.</p> <p>View from another perspective, this indicates that a function that is identically zero on the manifold cannot change when we move tangentially: so tangent vectors “don’t see” normal variations. What I mean by the former statement “Tangent vectors annihilate normal components” is that derivatives along tangent directions ignore any part of a function that only varies off the manifold, because those variations are invisible to curves that stay on the manifold.</p> <hr/> <h3 id="intrinsic-vs-extrinsic-viewpoints-summary">Intrinsic vs Extrinsic Viewpoints (Summary)</h3> <table> <thead> <tr> <th>Aspect</th> <th>Intrinsic</th> <th>Extrinsic</th> </tr> </thead> <tbody> <tr> <td>Tangent vector</td> <td>Derivation</td> <td>Vector in \(\mathbb{R}^3\)</td> </tr> <tr> <td>Definition</td> <td>Via charts</td> <td>Via embedding</td> </tr> <tr> <td>Computation</td> <td>Pullback</td> <td>Directional derivative</td> </tr> <tr> <td>Dependence</td> <td>Coordinate-dependent</td> <td>Embedding-dependent</td> </tr> </tbody> </table> <p>Both viewpoints are <strong>equivalent</strong> when an embedding exists.</p> <hr/> <p>Something to note from the above example:</p> <blockquote> <p>Coordinate vector fields are <strong>defined intrinsically as derivations</strong>,<br/> may be <strong>represented extrinsically</strong> using embeddings,<br/> and act on functions in a way that is <strong>independent of extensions</strong> because tangent vectors annihilate normal components.</p> </blockquote> <p>Through this simple example we already develop a taste of the difference between intrinsic and extrinsic geometry, the abstract definitions and concrete calculations. Such theme will be recurring for our journey in studying differential geometry.</p> <p>One of the advantages of the intrinsic view is that it is independent of coordinate charts. It paves the ground for explaining why</p> <blockquote> <p><strong>Gradients, geodesics, and Lie brackets make sense without embedding (or ever mentioning) the abstract manifold into $\mathbb{R}^n$</strong>.</p> </blockquote> <h2 id="discussions">Discussions</h2> <p>Firstly, note that tangent/cotangent space is an intrinsic? property. A usual image of interpreting tangent space is \(S^2\) embedded in \(\mathbb{R}^3\). Our definition above does not require so, and in fact the shift of perspective from extrinsic into intrinsic properties of geometric objects is a grand evolution starting from Gauss and Rieman.</p> <p>[TODO: An illustrative figure of a sphere embedded in Euclidean space]</p> <p>From derivations to curves:</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Intuition and definition of tangent/cotangent space]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)"/><published>2025-09-17T17:49:33+00:00</published><updated>2025-09-17T17:49:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Today I will cover a beautiful subject in differenetial geometry: Ricci flows and Ricci curvature.</p> <p>Generally, in Riemannian geometry, curvature measures how space bends. For instance, on a sphere, geodesics (shortest paths) come closer together compared to flat space; on a hyperbolic surface, they diverge.</p> <p>Ricci curvature is a particular way of summarizing curvature: Instead of describing how all directions bend (that’s what the full <strong>Riemann curvature tensor</strong> does), Ricci curvature focuses on <strong>volume distortion</strong>. More concretely, it tells us how the volume of a small geodesic ball deviates from the volume we’d expect in flat Euclidean space.</p> <p>Intuitively, we could summarize into the following:</p> <blockquote> <p>Positive Ricci curvature (like on a sphere) means geodesics tend to converge, and small balls have less volume than in flat space.</p> <p>Zero Ricci curvature (like in Euclidean space) means geodesics neither converge nor diverge, so volumes match Euclidean.</p> <p>Negative Ricci curvature (like on a hyperbolic space) means geodesics diverge, so small balls have more volume than Euclidean.</p> </blockquote> <p>Mathematically, Ricci curvature is obtained by “tracing” the <strong>Riemann curvature tensor</strong>. It compresses information about how different directions curve into a symmetric 2-tensor <code class="language-plaintext highlighter-rouge">Ric</code>.</p> <h3 id="riemannian-geometry-and-tensor">Riemannian Geometry and Tensor</h3> <p>The Riemann tensor is written in the following way:</p> \[R(X, Y)Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X, Y]} Z\] <p>This tensor captures all information about curvature. Succinctly, this is a 4-tensor: \(R_{ijkl}\).</p> <p>The above is an unfair treatment of Riemannian geometry. I’ll have a separate blog on that subject soon.</p> <p>How to understand: Riemannian metric tensor informs the manifold where to expand, shrink, and curve. How does Riemannian metric tensor relate with curvature?</p> <h3 id="ricci-curvature">Ricci Curvature</h3> <p>Based on the Riemann tensor, what is the curvature?</p> <p>To get Ricci curvature, we take a <strong>trace</strong> of the Riemann tensor:</p> \[Ric_{ij} = R^{k}_{ikj} = g^{kl}R_{kilj}\] <p>This reduces the information down to a 2-tensor (like the metric itself). Geometrically, this represents the volume distortion of geodesic balls.</p> <h3 id="ricci-flow">Ricci Flow</h3> <p>Introduced by <code class="language-plaintext highlighter-rouge">Richard Hamilton (1982)</code>, Ricci flow is a process that evolves a Riemannian metric \(g(t)\) over time:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>The factor -2 is just convention (to simplify later computations). We could think of this as a heat equation for geometry: Just as heat diffuses to smooth out temperature differences, Ricci flow smooths out <strong>irregularities</strong> in curvature. As illustrated in the above section of Ricci curvature, we could naturally arrive at the result that positive curvature regions tend to shrink and negative curvature regions expand. Over time, the underlying geometry becomes more “regular”, like ironing out wrinkles.</p> <p>The effect of Ricci flow on curvature could be expressed in the following way. The derivative of scalar curvature \(R\) under this flow is:</p> \[\frac{\partial R}{\partial t} = \Delta R + 2 |Ric|^2\] <p>This resembles a heat equation $(\Delta R)$ plus a positive correction. Consequently, curvature tends to diffuse out but also grows in positive-curvature regions.</p> <h4 id="examples-of-ricci-flow">Examples of Ricci Flow</h4> <p>Perhaps the simplest example is to imagine how a sphere evolves under Ricci flow. We all know that a round sphere has positive Ricci curvature. The Ricci flow equation says:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>Since Ricci is positive, the metric shrinks, and thus makes the sphere to contract uniformly, eventually collapsing to a point. This closely mirrors the idea that positive curvature makes geodesics converge. Under the Ricci flow, it tightens further.</p> <p>Another simple example is the flat torus. Since the Ricci curvature is 0 everywhere,</p> \[\frac{\partial g_{ij}}{\partial t} = 0\] <p>the torus will stay unchanged after the flow forever. This is analogous to heat diffusion on a perfectly uniform temperature field where nothing would effectively changes.</p> <p>Likewise, a hyperbolic surface has negative Ricci curvature. Consequently, under the Ricci flow, the metric would expand and the hyperbolic surface would grow larger and more uniform in curvature.</p> <p>In a nutshell, irregular geometries with bumps or folds (different curvature in different regions) get “smoothed” over time. All the high-curvature “wrinkles” would get flatten out, like how heat equalizes temperature.</p> <h3 id="application-of-ricci-flow">Application of Ricci Flow</h3> <p>Poincare conjecture, Ricci flow, surgery theory, what Terrence Tao called “one of the most impressive recent achievements of modern mathematics”</p> <p>Poincare conjecture:</p> <blockquote> <p>Any closed 3-manifold that is simply-connected, compact, and boundless is homeomorphic to a 3-sphere.</p> </blockquote> <p>Specifically, Poincare conjecture in higher dimensions has been solved around 1961, and dimension 4 case has been proved by Michael Freedman who by which won Fields medal in 1986. The \(n = 3\) case seemed really difficult to crack and it was only at 2002 that Grisha Perelman proved it using Ricci flow.</p> <p>Very briefly, since a sphere has positive curvature, by applying Ricci flow through time such sphere will contract and eventually vanish. Perelman proved the opposite also holds: if metric goes to 0, it must have been a sphere. To prove Poincare’s conjecture using Ricci flow,</p> <p>One of the most triumphant use of Ricci flow happens when Grigori Perelman (2002–2003) to prove <strong>the Poincaré conjecture</strong> and the more general <strong>Thurston geometrization conjecture</strong>. He showed how Ricci flow with “surgery” (cutting and patching when singularities form) classifies 3-manifolds.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Introduction of Ricci flows/curvatures]]></summary></entry><entry><title type="html">The Dance of Space (2): Differential Forms (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/" rel="alternate" type="text/html" title="The Dance of Space (2): Differential Forms (in progress)"/><published>2025-09-16T16:34:22+00:00</published><updated>2025-09-16T16:34:22+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Differential_Forms</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/"><![CDATA[<p>The second episode to appreciate the inner workings of space is through the differential forms. Differential forms is not an ancient subject, …</p> <h3 id="vector-outer-product">Vector Outer Product</h3> <h3 id="wedge-productexterior-derivative">Wedge Product/Exterior Derivative</h3> <h3 id="three-in-one">Three in One</h3> <p>Green’s theorem, Gauss’ theorem, Stoke’s theorem.</p> <p>Green’s theorem:</p> \[\int_{L} Pdx + Qdy = \iint_{D}(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y})dxdy\] <p>Generalized Stoke’s theorem.</p> <p>A k-form is supposed to be integrated over an oriented k-dimensional manifold</p> <h3 id="fundamental-theorem-of-calculus-ftoc">Fundamental Theorem of Calculus (FTOC)</h3> <p>High dimensional Stoke’s theorem is exactly the fundamental theorem of calculus (FTOC) in high-dimensional space.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Intuite and define differential forms]]></summary></entry><entry><title type="html">Equivalence: What does “being equal” represent? (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Equivalence/" rel="alternate" type="text/html" title="Equivalence: What does “being equal” represent? (in progress)"/><published>2025-09-07T00:23:16+00:00</published><updated>2025-09-07T00:23:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Equivalence</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Equivalence/"><![CDATA[<p>Explain and compare multiple equivalences from differential geometry and topology, including homeomorphism, diffeomorphism, homotopy equivalence, homomorphism, isomorphism, etc.</p> <p>Invariant and equivariant functions (CNN is equivariant).</p> <p>Also discussions on cardinality among sets (including finite and infinite (countably infinite \(N0\)? and uncountably infinite), Hillbert Hotel problem, equipotent sets)</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Topology"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Compare various equivalences in geometry/topology/group theory]]></summary></entry></feed>