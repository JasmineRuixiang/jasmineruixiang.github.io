<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jasmineruixiang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasmineruixiang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-26T21:02:13+00:00</updated><id>https://jasmineruixiang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Constraints upon learning from a neural manifold perspective</title><link href="https://jasmineruixiang.github.io/blog/2025/Constraint-Learning/" rel="alternate" type="text/html" title="Constraints upon learning from a neural manifold perspective"/><published>2025-08-25T22:09:34+00:00</published><updated>2025-08-25T22:09:34+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Constraint-Learning</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Constraint-Learning/"><![CDATA[<h1 id="neural-constraints-on-learning">Neural constraints on learning</h1> <h2 id="experiment-setup">Experiment setup</h2> <p>For <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>, two male Rhesus macaques were trained to perform closed-loop BCI cursor task (Radial 8). Around 85-91 neural units (threshold-crossings) were recorded. The experiment pipeline is demonstrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1a.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 1a in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. Note that on the right hand side the authors already presented where to place the two kinds of perturbation. The color scheme (green, yellow, and black) is consistent throughout figures in this paper. </div> <h2 id="decoding-paradigm">Decoding paradigm</h2> <h3 id="dimensionality-reduction-technique">Dimensionality reduction technique</h3> <p>The control space is just 2D because the decoder output is cursor velocities in \(\mathbb{R}^2\), illustrated as black line in Fig.2 black line (Note: it’s actually a 2D plane, but here for simplicity shown as a black line (\(\mathbb{R}^1\))).</p> <p>They used Factor Analysis ({cite (Factor-analysis methods for higher-performance neural prostheses) }{cite (Gaussian Process Factor analysis)}; I’ll write a blog on GPFA later) to extract what they called the “intrinsic manifold”, which captures the co-modulation patterns among the recorded neural population. This is shown as the underlying yellow plane in Fig.2 (might be confusing, but it’s not the 2D control space). Note that at the time of publication, neural manifold was not yet in a popular trend, so the authors briefly characterized the term “intrinsic manifold” with the following illustration:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1b.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 1b in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. . </div> <p>The authors further elaborated on the intrinsic manifold and its associated dimensionality at the end of the paper. For consistency of comparisons, Sadtler et. al. used a <strong>linear</strong> 10-D intrinsic manifold across all days. They then performed some offline analysis to explore if 10 is a legitimate choice. Specifically, they estimated the intrinsic dimensionality (EID) as the peak of maximal cross-validated log-likelihood (LL). In panel a the vertical bars represent the standard error of LL from across 4 cross-validation folds. Panel b. shows EID for all days and both 2 monkeys. The authors also showed the LL difference between 10D manifold vs manifold with EID (panel c., with units being the the number of standard errors of LL for the EID model). From panel c. the authors observed that 89% of days the 10-D manifold only differs within one standard error of LL with the EID manifold. Panel d. indicates the cumulative explained variance by the 10-D manifold. Notice that 10 dimensions already explained almost all neural variance.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_4.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.4 in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <p>The factor analysis method works in the following way (I’ll keep the same notation as the paper). Let’s assume the high dimensional neural signal (here the z-scored spike counts) acquired every 45\(ms\) time bin is denoted as \(u \in \mathbb{R}^{q \times 1}\) (naturally, \(q\) neural units), and \(z \in \mathbb{R}^{10}\) the latent variable. Factor analysis assumes the observed neural activity is related to the unobservable latent variables under a Gaussian distribution:</p> \[u | z \sim N(\Lambda z + \mu, \psi)\] <p>where the latent vector is assumed to come from</p> \[z \sim N(0, I)\] <p>Here the covariance matrix \(\psi\) is diagonal. Consequently, the intrinsic manifold is defined on the span of the columns of \(\Lambda\), and each column of \(\Lambda\) represents a latent dimension where \(z\) encodes the corresponding projections/coordinates. All three parameters \(\Lambda, \mu, \psi\) are estimated from <strong>Expectation-Maximization (EM)</strong> method (I’ll also write a blog about this later, especially how it as a classical inferenc engine is closely related to Evidence Lower Bound (<strong>ELBO</strong>), a populat loss/objective function for modern-day generative models based on DNN like VAE and Diffusion).</p> <h3 id="intuitive-mapping">Intuitive mapping</h3> <p>The intuitive mapping is selected by fitting a modified Kalman Filter ({cite Wu W. Gao Y., Bayesian population decoding of motor cortical activity using a Kalman filter})</p> <h2 id="perturbation-method">Perturbation method</h2> <p>Then the core methodology of this study is to change the BCI mapping so that the altered control space would be lying either within or outside of the insintric manifold. The paper does present some confusion as to how intuitive mapping and control space would be distinguished. My interpretation is that the control space refers to the ideal potential neural subspace for which to control the cursor optimally. Since within a short time neural connectivity is kept unaltered, the true intrinsic manifold is approximately invariant and thus the required potential neural subspace might not be reachable. By default the control space/intuitive mapping lies within the intrinsic manifold (that’s why it’s called “intuitive”, because that’s is what the neural network system has learned to achieve).</p> <p>The neuronal connectivity statistics is referred to as the natural co-modulation patterns.</p> <p>In short, a within-manifold perturbation only reoriented the control space such that it still resides in the intrinsic manifold (shown in Fig.3 red line). This does not require monkeys to readapt neuronal connectivity patterns to achieve such new control space. It only altered the function from the intrinsic manifold to cursor kinematics. On the other hand, an outside-manifold perturbation alters the control space allowing it to live off the intrinsic manifold (Fig.3 blue line). Notice that if such outside-manifold perturbation pushes the control space along the orthogonal subspace that passes through the original control space, then the mapping from the neural comodulation patterns to cursor kinematics is preserved (basically just project the altered control space to the intrinsic manifold, then would recover the original control space). However, the underlying comodulation/covariation patterns among the neural population are altered.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1c.png" class="img-fluid rounded z-depth-1" width="45%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.1c in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <p>After the perturbation, the authors observed if the monkeys could eventually learn to readapt to the new mapping, to achieve great cursor control performance. For within-manifold perturbation, the monkeys only need to learn to associate cursor kinemaitcs to a new set of neural comodulation patterns (still within reach because lying in the same intrinsic manifold). However, for outside-manifold perturbation, they had to generate new co-modulation patterns in order to reach outside of the existing intrinsic manifold. Consequently, the authors predicted that within-manifold perturbation is easier to learn compared to outside-manifold perturbation. The authors did find results to back up this claim and since they are not the main focus of this blog, I’ll refer interested readers to the original paper to take a look (FIgure 2).</p> <p>Other than that, I do want to dive deep into how such within/outside-manifold perturbations were implemented. Specifically,</p> <h2 id="quantifiable-metric">Quantifiable metric</h2> <h3 id="amount-of-the-learning">Amount of the learning</h3> <p>To quantify the potential amount of learning under two perturbation kinds, the authors resorted primarily to two performance metric: (the change of) relative acquisition time and relative success rate across perturbation blocks. Specifically, as shown below, the black dot represents the intuitive mapping, while the red and blue dots indicate the imediate performance just after corresponding perturbations. Red and blue asterisks represent the best performance during the within the perturbation sessions. The dashed line indicates the maximum learning vector \(l_{max}\) (note that it starts on the red dot), and thus the aount of learning (\(L \in \mathbb{R}\)) is quantified as the length of the projection of the raw learning vector onto the maximum learning vector, normalized by the length of the maximum learning vector:</p> \[L = \frac{l_{raw, i} \cdot l_{max}}{||l_{max}||^2}\] <p>where \(i \in \{red, blue\}\). Pictorially, it’s illustrated as below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_2cd.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.2c and Fig.2d in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <p>The amount of learning for all sessions was presented above in the right pannel. Notice that a value of 1 indicates “complete” learning of the new relationship between the required neural co-modulation and cursor kinematics, reverting to the performance level of the intuitive control, while 0 indicates no learning. The authors did observe that there’s significant amount of learning for within-manifold perturbations than outside-manifold perturbation. To see changes in success rates and acquisition time during perturbation blocks, instead of a single metric \(L\) as shown above, the authors also plotted them separately as below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Ext_Fig_2.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.2 in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <h3 id="after-effects">After-effects</h3> <p>A second metric Sadtler et. al. employed is observe how the monkeys performed as they reintroduce the intuitive mapping, or the so-called after-effects after washout of the perturbed mapping. Specifically, the after-effect is measured as the amount of performance imparement (tentative: acquisition time, success rate) at the beginning at the wash-out block (like how impairement was measured at the beginning of a perturbation block). A large wash-out effect indicates that the monkeys have learned and adapted to the perturbed mapping. For within-manifold perturbation, the authors did observe brief impaired performance but not so for outside-manifold perturbation, indicating that learning did occur during the former.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Ext_Fig_3.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.3 in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <h2 id="discussions">Discussions</h2> <p>Since Sadtler et. al. employed closed-looop BCI control, they were able to causally alter the model/map from neural activities to the decoded cursor velocities.</p> <p>This paper highlights a potential methodology of BCI research: since the mapping from neural activities to control correlates is <strong>fully specified</strong>, thus could be causally perturbed to explore the corresponding changes of controlled behavior. This allowed the authors to <strong>design/know apriori</strong> the optimal/required neural activities (specified by the altered mapping) to achieve task success, and thus to observe if animals could generate such neural patterns.</p> <p>An outside-manifold perturbation does not necessarily specify that it lives in orthogonal subspace of the intrinsic manifold. Also, because here the intrinsic manifold is illustrated as a plane, in real world scenario, it is unlikely that it exists as a linear subspace. Consequently, specifying a space that is “orthogonal” to the potential manifold (nonlinear, other than a linear subspace) might be problematic.</p> <p>The amount of learning is entirely dependent upon performance itself, which is difficult to causally link to neural changes.</p> <p>For the amount of learning metric, why would 0 indicate no learning? Orthogonal learning?</p> <p>Notice that the after-effects analysis echoed a lot of research methodology in force-field or curl-field perturbation for cursor control in monkey motor control studies.</p> <p>The authors also showed that learning did not improve through sessions (readers might refer to Extended Figure 4 in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a> for further information).</p> <p>The perspective and consideration that Sadtler et.al took to ensure alternative explanations for the observed distinction of learnability do not hold are informative. I enjoyed its rigorosity, especially when they considered perturbed maps which might be initialy difficult to learn and carefully implement the controls (demonstrate that they have controled). To not diverge from the main focus of this blog, I’ll not cover those dicussions. The way that the authors listed clearly alternative explanations and how they tackled each is a very inviting, powerful, and efficient way of writing.</p> <p>From the methods the authors used, they only estimated a linear manifold. According to {cite}, linear manfolds should require more dimensions than a nonlinear manifold which could explain similar amount of variance.</p> <h2 id="conclusions">Conclusions</h2> <p>The neural manifold reflects the inherent connectivity which constrains (in a short term) the potentially learnable patterns. Consequently, the neural connectivity network structure dictates possible neural patterns and corresponding behavior repertoire the animals are capable of performing.</p> <p>This paper strengthens my belief in the legit usability of the low dimensional structure among neural population, and more crucially the value of perturbation methods to causally verify the neural manifold.</p> <h1 id="batista-2025">Batista 2025</h1> <h2 id="discussions-1">Discussions</h2> <h2 id="conclusions-1">Conclusions</h2> <h1 id="discussions-2">Discussions</h1> <p>These two studies offer powerful information that dimensionality reduction could be not just a visualization tool, but a causal summary of the underlying neural connectivity and anatomical constraints, which correlates to the neural computations that neural population could implement.</p> <h1 id="conclusions-2">Conclusions</h1>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Learning"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Employ perturbations of neural manifold to explore learning constraints]]></summary></entry><entry><title type="html">Blogs syntax summary</title><link href="https://jasmineruixiang.github.io/blog/2025/blog-summary/" rel="alternate" type="text/html" title="Blogs syntax summary"/><published>2025-08-16T18:30:16+00:00</published><updated>2025-08-16T18:30:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/blog-summary</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/blog-summary/"><![CDATA[<hr/> <p>Primary text enters here. For a new paragraph, no need for a specified new-line character.</p> <p>Here’s how to embed a weblink: <a href="https://www.pinterest.com">Pinterest</a>, or Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a>. Both methods have the same effect.</p> <h4 id="hipster-list">Hipster list</h4> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> <h4 id="check-list">Check List</h4> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Brush Teeth</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Put on socks <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Put on left sock</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Put on right sock</li> </ul> </li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Go to school</li> </ul> <p>For indented section:</p> <blockquote> <p>We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin</p> </blockquote> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <p>Images can be made zoomable. Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">c++
</span><span class="n">code</span> <span class="n">code</span> <span class="n">code</span>
<span class="p">```</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">j</span> <span class="o">=</span> <span class="mi">1</span>
 <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
</code></pre></div></div> <ol> <li> <p>We can put fenced code blocks inside nested bullets, too.</p> <ol> <li> <p>Like this:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">printf</span><span class="p">(</span><span class="s">"Hello, World!"</span><span class="p">);</span>
</code></pre></div> </div> </li> <li> <p>The key is to indent your fenced block in the same line as the first character of the line.</p> </li> </ol> </li> </ol> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. Produces something like this:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre> <span class="n">j</span> <span class="o">=</span> <span class="mi">1</span>
 <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p><br/> For displaying math, for example:</p> \[\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2\] <p>or</p> <p>\begin{equation} \label{eq:cauchy-schwarz} \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) \end{equation}</p> <p>and by adding <code class="language-plaintext highlighter-rouge">\label{...}</code> inside the equation environment, we can now refer to the equation using <code class="language-plaintext highlighter-rouge">\eqref</code>.</p> <p><br/> Could also include Twitter:</p> <h1 id="tweet">Tweet</h1> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h1 id="timeline">Timeline</h1> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h1 id="additional-details">Additional Details</h1> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <p><br/> For post-citation:</p> <p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p> <p><br/> For post-bibliography:</p> <p>That means simple citation like <a class="citation" href="#einstein1950meaning">(Einstein &amp; Taub, 1950)</a>, multiple citations like <a class="citation" href="#einstein1950meaning">(Einstein &amp; Taub, 1950; Einstein, 1905)</a>, long references like <span id="einstein1905movement">Einstein, A. (1905). Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat. <i>Ann. Phys.</i>, <i>17</i>, 549–560.</span> or also quotes:</p> <blockquote><p>Lorem ipsum dolor sit amet, consectetur adipisicing elit,<br/>sed do eiusmod tempor.</p><p>Lorem ipsum dolor sit amet, consectetur adipisicing.</p><cite><a class="citation" href="#einstein1905electrodynamics">(Einstein, 1905)</a></cite></blockquote> <p>If you would like something more academic, check the <a href="/blog/2021/distill/">distill style post</a>.</p> <p><br/> Table of contents: To add a table of contents to a post, simply add</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">beginning</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div> <p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post.</p> <p>To add a table of contents to a post as a sidebar, simply add</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div> <p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p> <p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p> <p><br/> Videos:</p> <p>This is an example post with videos. It supports local video files.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all. </div> <p>It does also support embedding videos from different sources. Here are some examples:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/jNQXAC9IVRw" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://player.vimeo.com/video/524933864?h=1ac4fd9fb4&amp;title=0&amp;byline=0&amp;portrait=0" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="latent"/><category term="dynamics,"/><category term="neural"/><category term="population"/><summary type="html"><![CDATA[start to write blog posts]]></summary></entry><entry><title type="html">Rotational dynamics in neural population</title><link href="https://jasmineruixiang.github.io/blog/2025/jPCA/" rel="alternate" type="text/html" title="Rotational dynamics in neural population"/><published>2025-08-16T18:30:16+00:00</published><updated>2025-08-16T18:30:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/jPCA</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/jPCA/"><![CDATA[<h2 id="preface">Preface</h2> <blockquote> <p>All changed, changed utterly:</p> <p>A terrible beauty is born.</p> <p>–––– William Butler Yeats, <em>Easter, 1916</em></p> </blockquote> <hr/> <p>Many years later, when I reflect back on the first quarter of the 21st century studies on computational neuroscience and brain-computer interface (BCI), among papers on fancy Neural Network based decoders and clinical breakthroughs expanding from cursor and motor BCI to speech and vision, I might still recollect a distant afternoon when I first heard the name Mark Churchland, and more specifically, neural population dynamics and jPCA. “Terrible beauty”, that’s the phrase which came to my mind at that time. Since then, such impression has taken its roots only deeper as research of similar flavor supported the dynamical system view from neural population.</p> <p>I perhaps first read the paper <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a> at least a decade after it got published, but the astonishing finding and the elegance of the algorithm, together with its implicit influences which reshaped my views on interpreting neural population and thus the currently suring word “emergence”, made it a classic enduring the test of time. This post is both a flashback and an exploration, where I extract a few key components from the article, specifically its dynamical systems interpretation and the jPCA algorithm itself, and raised a few questions and extended from the paper to some open queries in the end.</p> <h2 id="dynamical-systems-perspective">Dynamical systems perspective</h2> <p>A traditional perspective characeterizes neural activities from the primary motor cortex (M1) as representing the corresponding movement parameters. Equivalently, we could write out a parametric equation:</p> \[r_n(t) = f_n(param_{1}(t), param_{2}(t), param_{3}(t), ...)\] <p>where \(r_n(t)\) is the firing rate for the \(n_{th}\) neuron, tuned by the corresponding function \(f_n\). Alternatively, instead of a representational model, another perspective based on neural population encoding which reflects behavior parameters not on the single neuron level, but on the population level with a <strong>dynamical system</strong>, could be written as follows:</p> \[\dot{r}(t) = f(r(t)) + u(t)\] <p>Here \(f\) might represent a linear/nonlinear dynamical system, and \(u(t)\) is an unknown external input. In this view, the dynamics, i.e., the evolution of population response, encodes the movement parameters. Or put differently, within a dynamical system model, each single-unit response should reflect the “dynamical factors” exhibited from each latent state in the latent space, which we aim to identify from the observed high-dimensional neural pouplation recordings.</p> <h2 id="quasi-rthymic-responses">Quasi-rthymic responses</h2> <p>As made clear in the article, the critical finding of this study is that reaching, a non-oscillatory movement (unlike the swimming leech or a walking monkey), leads to a quasi-oscillatory neural trajectory. More surprisingly, the rotations are distinct not by reaching curvatures but determined from the initial conditions, which are encoded by the <strong>preparatory activities</strong>. We will see that <strong>preparatory activities</strong> feature as an essential ingredient in Churchland and colleagues’ research, resulting in surpising analysis on nullspace/output-potent space later <a class="citation" href="#nullspace">(“Cortical Activity in the Null Space: Permitting Preparation without Movement,” 2014)</a><a class="citation" href="#prep_review">(Mark M Churchland, 2024)</a> in the years to. Specifically, as the authors summarized, the trajectories have the following primary properties, which support the dynamical system perspective:</p> <blockquote> <p>1] Rotation is a ubiquitous phenomenon during behavior;</p> <p>2] Trajectories have the same directions for all rotations;</p> <p>3] Preparatory activities determine the initial conditions which govern trajectories;</p> <p>4] Rotations do not directly correlate with the curvature</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_3.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 3 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. jPCA projections into the first 2 dimensions of different monkeys. Each trajectory plots the first <strong>200ms</strong> activities following the preparatory state (initial conditions). Colors correpond to projection onto jPC1. </div> <p>An interesting reflection is that with the dynamical system perspective hypothesis, rotations of neural states should be similar no matter what reaching conditions: only the initial conditions determine the trajectories, with the underlying \(f\) being identical. I found this line of thought extremely insightful, which demonstrates how important it is to deeply understand theory to deduce results, though they might look unorthodox — for example, the neural states were expected be similar even when the reaches are in opposite directions if they share similar preparatory activities, because they are highly correlated wit initial conditions.</p> <p>The authors also carried some control (shuffling) analyses, and corroborated that such rotational pattern does explain a significant amount of data variance. Interestingly, the authors also discovered that, although rotations are consistent for all conditions in the same jPCA plane (similar orietations and speeds), such rotations actually exist in multiple jPCA planes. As shown below, all top 3 jPCA planes contain rotatinos, but with higher-number jPCA planes carrying less ordered rotations with slower speed. Perhaps not so surprisingly, both PMd and M1 exhibit such rotational structures, with initial states/prepatory activities better distinguished in PMd (again not surpising, because PMd is known for movement planning and with stronger preparatory activities. Eight years later, Russo et al. <a class="citation" href="#russo">(“Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation,” 2020)</a> will exhibit an obvious distinction between how M1 and PMd encode movement sequence/planning by exploring corresponding latent trajectories, providing further analyses between these two brain regions)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_4.png" class="img-fluid rounded z-depth-1" width="55%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 4 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. jPCA projections into different planes exhibit similar rotation structures. </div> <h2 id="jpca">jPCA</h2> <p>Based on the dynamical systems perspective, since we focus on time-dependent variations, a naive PCA is not sufficent to extract such temporal structures from neural activities (PCA is not specifically designed for incapsulating dynamical structures). Here Churchland et. al. developed an algorithm called jPCA to reoslve this issue. Specifically, it finds orthonormal axes (thus basis which define linear subspaces) which capture the strongest rotational components from the subspace identified by PCA (to ensure that the rotational dynamics come from subspaces that efficiently “represent” the high dimensional neural space). Conseuqently, this is equivalent to rotating the PCA projections to help viewers better “see” the rotation most clearly (as shown in Supplementary Movie 2 below). In the paper the authors chose the PCA dimension as 6, and the data projected from the 6-D PCA space to the first 2 jPCA components, thus a plane which captures the strongest rotations, is displayed (Adapted Figuer 3) to reveal the underlying oscillatory structure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 2 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>, showing jPCA projections are essentially a different view of PCA projections. </div> <p>The core of jPCA lies in the following steps:</p> <blockquote> <p>1] Fit a linear dynamical system for \(\dot{X}_{red} = M_{skew}X_{red}\), where \(X_{red}\) is of size \(6 \times ct\), \(c\): conditions, \(t\): time. Notice that this is really just a matrix formulation of \(\dot{x}_{red, i} = M_{skew}x_{red, i}\), where \(\dot{x}_{red, i}, \; x_{red, i}\) are column vectors and \(\dot{X}_{red} = [\dot{x}_{red, 1}, \; \dot{x}_{red, 2}, \; ..., \; \dot{x}_{red, (c(t-1))}], X_{red} = [x_{red, 1}, \; x_{red, 2}, \; ..., \; x_{red, c(t-1)}]\). Since \(\dot{X}_{red}\) is of size \(6 \times c(t-1)\) (use discrete time difference as approximations for the differentice), \(M_{skew} \in \mathbb{R}^{6 \times 6}\). Notice that to be precise, \(X_{red}\) is also truncated to be \(6 \times c(t-1)\). 6 is the dimensionlality of a PCA projection before starting jPCA, as mentioned above;</p> <p>2] Since \(M_{skew}\) is a skew-symmetric matrix, ithas pure imaginary eigenvalues and thus captures rotational dynamics;</p> <p>3] Identify the complex vectors \(V_1\) and \(V_2\) corresponding to the largest two imaginary complex values. From these locate the real planes: \(jPC_1 = V_1 + V_2\), and \(jPC_2 = V_1 - V_2\);</p> <p>4] The first jPCA projection is thus \(X_{jPC} = [jPC_1, jPC_2] \times X_{red}\). Similar projections for other jPCA planes.</p> </blockquote> <p>For a given jPCA plane, the choice of orthogonal basis is arbitruary, so the authors pick \(jPC_1\) and \(jPC_2\) such that the rotation is anti-clockwise and the preparatory activities spread along most clearly on \(jPC_1\).</p> <p>For the sections below, I’ll dive into details of the jPCA algorithm. Readers might want to skip the following if the above information is sufficient. For a more detailed description of jPCA, let’s start by reordering \(X\) as \(X \in \mathbb{R}^{ct \times n}\), and for instance \(X\) might be choosen as \(X_{red} \in \mathbb{R}^{ct \times k}\), for \(k = 6\) after PCA projection. For simpler and more general notation, for the following I will use X for illustration.</p> <h3 id="dynamical-summary-of-data">Dynamical summary of data</h3> <p>For a canonical PCA projection, we would start by finding the covaraince matrix \(\Sigma = X^TX \in \mathbb{R}^{n \times n}\) (notice that here n represents not the number of samples, but the number of features). We assume that \(X\) is already mean-centered. To capture dynamical structure, we will need a different \(n \times n\) matrix to summarize the data: consider a time-invariant linear dynamical system: \(\dot{x}(t) = x(t)M, M \in \mathbb{R}^{n \times n}\). Consequently, this reduces to solving</p> \[M^{*} = \argmin_{M \in \mathbb{R}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>Or simply put, \(M^{*} = X \backslash \dot{X}\). Notice that this optimization has a simple and closed-form solution:</p> \[M^{*} = (X^TX)^{-1}X^T\dot{X}\] <p>As the authors argued, the data covariance matrix \(\Sigma\) captures <strong>an ellipsoid which best fits the data</strong> (deep interpretation of this, what ellipsoid and what it represents when it says “best fit”), but with no temporal information, whereas \(M^*\) characterizes a linear dynamical system which best fits the data \(X\).</p> <p>I find this analysis particularly intriguing. In order to stay focus here, I’ll discuss a few reflections in the last section.</p> <h3 id="skey-symmetric-specification-for-rotation">Skey-symmetric specification for rotation</h3> <p>For a general linear dynamical system, the dynamics include both expandions/contractions and rotations. In this study, the authors are particularly interested in dissecting a potentially pure rotational dynamics. The key observation that Churchland and colleagues offered here is that every linear transformation \(M\) can be decomposed into two components:</p> \[M = M_{symm} + M_{skew}\] <p>where \(M_{symm} = (M + M^T)/2, M_{skew} = (M - M^T)/2\), so that \(M_{symm} = M_{symm}^T, \; M_{skew} = -M_{skew}^T\). This raises an interesting point whether all linear transformations can be decomposed into symmetric and skew-symmetric components: well, not quite, since here the formula only exist for \(M\) being a square matrix. However, even if \(M \in \mathbb{R}^{k \times l}, \; k \neq l\), it is still a linear transformation. How should we reconcile and think deeper here? I will leave this to the last section for further discussions.</p> <p>With such decomposition, the benefit it brings is that since \(M_{symm}\) have purely real eigenvalues and \(M_{skew}\) have purely imaginary eigenvalues (in complex conjugate pairs), thery describe expansions/contractions and rotations, correspondingly (the explanation for this and matrix exponential is further illustrated at the last section).</p> <p>Consequently, if we specify the set of skew-symmetric matrices as \(\not \mathbb{S}^{n\times n}\), and</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <h3 id="solution-of-the-constrained-optimization">Solution of the constrained optimization</h3> <h4 id="matrix-problem-rewritten-into-vector-form">Matrix problem rewritten into vector form</h4> <p>For solving the above optimization constrained to only skew-symmetric matrices, Churchland and colleagues did the following modifications: first notice that when solving \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\), each column of M is independently determined by the corresponding column of \(\dot{X}\). Consequently, the matrix optimization problem could be written in vector format: unroll \(M \in \mathbb{R}^{n \times n}\) into \(m = M(:)\), where \(m \in \mathbb{R}^{n^2}\), and thus the unconstrained least squares problem could be rewritten as:</p> \[m^{*} = \argmin_{m \in \mathbb{R}^{n^2}} ||\dot{x} - \tilde{X}m||_{F}\] <p>where \(\dot{x} = \dot{X}(:)\), and \(\tilde{X}\) is a block diagonal matrix with \(X\) repeated on the \(n\) diagonal blocks. I totally agree that the matrix/vector formats are equivalent, but it does not take \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\) to realize such format conversion. By inspecting upon \(\dot{X} = XM\), we should know that each column of \(\dot{X}\) is independently informed by the corresponding column of \(M\).</p> <h4 id="skew-symmetric-matrix-constraint">Skew-symmetric matrix constraint</h4> <p>To add skew-symmetricity constraint to the optimization problem, notice that for \(\not \mathbb{S}^{n \times n}\), the degrees of freedom is \(n(n-1)/2\) (notice that the diagonals are automatically 0, thus \(n\) less than \(n(n+1)/2\) as in symmetric matrix. Could think more on the underlying manifold, Lie group, and Lie algebra, see the last section).</p> <p>The crucial step is to realize that thus a skew-symmetric matrices is equivalently represented as vectors of the form \(k \in \mathbb{R}^{n(n-1)/2}\). This also reinforces my view that acquiring geometric priors, or basically whatever kinds of priors, encoded in the decoding models is alleviating model’s “cognitive burden”. I’d consider the speech BCI, which entails large language models (LLM) for improving RNN decding as adopting a “similar” strategy/line of thinking.</p> <p>Back to the central topic, with this vector \(k \in \mathbb{R}^{n(n-1)/2}\), we could likewise simply form a linear map \(H\) which transforms \(\mathbb{R}^{n(n-1)/2}\) into \(\mathbb{R}^{n^2}\). In a nutshell, \(k\) encodes the lower/upper triangle of \(M_{skew}\) and \(H\) is hand-coded (not shown here, but sparse and has only \(n(n-1)/2\) positions of 1 and \(n(n-1)/2\) positions of -1).</p> <h4 id="final-closed-form-optimization">Final closed-form optimization</h4> <p>Notice that now we finally see the benefit of the vector format: \(Hk\) returns a vector of shape \(\mathbb{R}^{n^2}\) which equivalently represents \(M_{skew} \in \not \mathbb{S}^{n \times n}\). Consequently, the original constrained optimzation problem</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>is eventually rewritten as</p> \[k^{*} = \argmin_{k \in \mathbb{R}^{n(n-1)/2}} ||\dot{x} - \tilde{X}Hk||_{2}\] <p>Notice that this is similar in form to the OLS problem can thus be written in closed form:</p> \[k^{*} = (\tilde{X}H)\backslash \dot{x}\] <p>The authors also briefly discussed a few implementation techniques, including solving using a gradient descent based method. However, since this optimization problem has a unique global optimum, they should return the same results.</p> <h4 id="what-does-it-represent-to-decompose-the-dynamics">What does it represent to decompose the dynamics?</h4> <p>Now with \(M^{*}_{skew}\), the authors found out the corresponding eigenvectors and eigenvalues and do projections like PCA. Since skew-symmetric matrices only have pure imaginary eigenvalues and the eigenvectors come in <strong>orthogonal, complex conjugate</strong> pairs (thus could think about the decomposition of \(M_{skew}\) as extracting orthogonal planes), the authors find the real projection planes \({u_{i, 1}, u_{i, 2}}\) from the complex conjugate vector pairs \({v_{i, 1}, v_{i, 2}}\) by \(u_{i, 1} = v_{i, 1} + v_{i, 2}, \; u_{i, 1} = v_{i, 1} - v_{i, 2}\) with normalization. Notice that the eigenvector/eigenvalue decomposition logics are different here: For PCA, eigenvectors of the covariance matrix serve as principal components of the underlying data (geometric view: ellipsoid); For jPCA, the eigenvectors stipulate the axes along which the solutions of the underlying dynamical system evolve.</p> <p>To explain this clearly, let’s take an example linear dynamical system problem:</p> <p>[TODO]</p> <p>Matrix exponential</p> <p>(Associate to Suppl.Fig.11 panels d, e)</p> <p>Projections onto jPC’s with largest magnitude of the corresponding eigenvalues (explain what this means) present most significant rotations: higher frequency and more consistency (Supplemantary Figure 4)</p> <h2 id="discussions">Discussions</h2> <p>The extracted neural trajectory is only “quasi-oscillatory”, and usually exists for barely 1 cycle.</p> <p>For many neurons, the preparatory and movement-related single neuron responses do exhibit some quasi-oscillations which last for about 1 cycle (See in below).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_2.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 2 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. Trace colors are according to the strengthof preparatory activities (red with greatest preparotory response). It allows clear view of the evolution of preparatory patterns during the movement. </div> <p>Not sure yet if this explains the observed rotation structure at the population level. However, the population level state-space rotations direcly to neural responses at single unit level: notice that each axis of jPCA exhibits similar patterns as single-neuron response(shown below)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_1.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 1 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. The projections onto jPC's are similar to single neuron response. </div> <p>As Churchland et al argued, this could be explained by the fact that jPC’s capture the most conscipuous patterns in the data, and each jPCA projection is a weighted sum/average of individual-neuron resopnses. Likewise, each single neuron could also be interpreted as a combination of all the jPC projected patterns, except that the intertwined relationship is not obvious if just viewed from a single neuron or a pattern itself.</p> <p>We might argue that perhaps a sufficient condition for the rotation structures is that neurons exhibit multiphasic responses. However, this turns out to be not necessarily true. The authors also tested on a “complex-kinematic model” and recorded EMG. Though both carry multi-phasic reponses, they do not show clear rotation patterns. In the end, as the authors argued, multiphasic responses are not enough: we also need the multi-phasic patterns to have consistent phases for around 90 degrees apart.</p> <p>More exploration on the 90 degrees phase difference here:</p> <p>Another core question is how such rotational pattern generates non-oscilatory movement patterns, e.g. reaching. The authors conducted some further simulation analyses and demonstrated that it’s possible to reconstruct EMG (deltoid) activities from weighted sum of rotations with different magnitude and phases (they call this the generator-model). It’s interesting to see this because EMG does not acquire the latent rotational patterns, yet it could be reconstructed from rotational dynamics. Further quantification analyses also showed that only the neural data and generator-model exhibit rotational patterns, whereas two other models (velocity-tuned, complex-kinematic models (not explained here in this blog)) do not. Since EMG showed only weak rotation, as the authors put, this further illustrates that the latent rotation dynamics do not necessarily result from a muli-phasic reponse, but how such response is constructed.</p> <p>One interesting finding is that the speed of movements is not encoded as the speed of the latent trajectories (they keep similar angular velocity). The amplitude of rotation dictates different movement speeds (for example faster or slower reaches). As Churchland et al. argued, this could be nicely explained since rotations with larger amplitude represent more strongly multi-phasic responses, with which EMG frequently entails larger accelerataions.</p> <p>The rotational structure is dependent upon the initial states (dynamical systems view) specified by the prepratory activities, which captures the condition-dependent activities (here condition refers to experiment conditions). Indeed, to clearly observe such differences, the cross-condition mean is substracted first for each single neuron before PCA/jPCA. The authors also illustrated the effects of keeping the cross-condition mean, as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_11.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 11 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. </div> <p>The goal of mean subtraction is to preserve only data distinguished in different conditions. In panel a. and b., each trace represents a trial-averaged firing rate for one (experiment) condition, with the same coloring scheme by the strength of preparatory activities. The yellow trace indicates the cross-condition mean for that neuron. Panel b. substracts the mean (yellow line being 0 all along) while panel a. keeps it. If without substraction of cross-condition mean, it’s observed that the first jPCA plane captures condition-independent curved trajectories. The authors argued that this is not surprising, since many neurons exhibit similar behaviors across different conditions, which are thus naturally captured by PCA projections. Since jPCA is extracting patterns from PCA space, it’s fairly expected that such patterns are condition independent. Notice that even in panel c., these trajectories still carry curvature, though it’s difficult to pinpoint how it’s theoretically generated. Panel d. displays the jPCA projection onto the second jPC plane (\(jPC3_, \; jPC_4\)) and we do see similar rotational structures dependent upon initial states specified by the preparatory activities. Consequently, to always explore the condition-dependent dynamics, the authors argue to substract cross-condition mean first before PCA/jPCA.</p> <p>As the authors put at the end of the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf">Supplementary Information</a>, the jPCA method could be easily adapted to find the projections for most significant expansion/contraction.</p> <h2 id="conclusions">Conclusions</h2> <p>In summary,</p> <blockquote> <p>1] “state-space rotations produce briefly oscillatory temporal patterns that provide an effective basis for producing multiphasic muscle activity, suggesting that non-periodic movements may be generated via neural mechanisms resembling those that generate rhythmic movement”;</p> <p>2] Preparactory encoders the initial conditions for the underlying dynamical system;</p> </blockquote> <p>The shift of views from single neuron analysis in pursuit of movement/behavior correlates/representaitons, to a dynamical system analysis on population level, is becoming an increasingly populat domain. As we will see later, the other side of the same story, geometry instead of dynamics, will quickly come into the arena under the name of neural manifold. This, with both aboundance of aesthetic elegance and disatisfying lack of mathematical rigor from either topology or differential geometry, will be an eternal theme for the following research, projects, and blogs of mine.</p> <h2 id="open-querries">Open Querries</h2> <p>The annotomical circuitry that leads to the dynamical system/latent trajectory is still unclear.</p> <p>What happens after the 200ms? As shown in Supp Movie 3 below, how would the observed rotations evolve later in time (notice that all the rotations exist for merely 1~1.5 cylces)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_3.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 3 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>, showing the evolution of the projected states. </div> <p>Although in the Discussions section, I briefly talked about the reason for existence of the rotational structures, The problem remains as to how individual oscillations of different phases and amplitudes would result in a consistent rotation.</p> <p>At the algorithmic level, the jPCA method stems from dissecting the rotation component away from the general linear transformation. Can this be further stripped away from PCA?</p> <p>Churchland argued that the dynamics quantified by a skew-symmetric matrix hints at the underlying functionally antisymmetric connectivity. The authors wrote: “a given neural dimension (for example, \(jPC_1\)) positively influences another (for example, \(jPC_2\)), which negatively influences the first. However, it is unclear whether this population-level pattern idrectly reflects a circuit-level dominance of antisymmetric connectivity”</p> <p>For jPCA algorithm, apart from \(\Sigma\) or \(M^*\), is there other way of capturing the data statistics from different perspectives?</p> <p>Notice that within jPCA, the authors made the following decomposition: \(M = M_{symm} + M_{skew}\), which naturally dissects the linear dynamics into expansions/contractions and rotations. I’m wondering whether similar decomposition can be made for general linear transformation \(M \in \mathbb{R}^{k \times l}, \; k \neq l\). This definitely cannot be a dissection into symmetric and skew-symmetric matrices because this general \(M\) is not a square matrix. Moreover, if we want to extend to non-square matrix, we are already out of the realm of dynamical system, since that would require the linear matrix to be square: \(\dot{x}(t) = Mx(t)\). How do we characterize a general linear transformation?</p> <p>Why do dynamical systems specified by symmetric and skew-symmetric matrices encode expansions/contractions and rotations? The reasons for that is the solution for linear dynamical systems exhibits as a matrix exponential, which leads to eventual solutions dependent upon the eigenvalues/eigenvectors. Notice that a symmetric matrix has real eigenvalues and skew-symmetrix matrix has imaginative eigenvalues. One related question I have is: can matrix exponential here in jPCA for extrapolation, potentially applied for data later than the first 200ms? How would the extrapolated dynamical trajectories align with the observed data?</p> <h3 id="hermitian-unitary-and-normal-matrices">Hermitian, unitary, and normal matrices</h3> <p>Churchland and colleagues put the following at the end of <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf">Supplementary Information</a>:</p> <blockquote> <p>“The convenience of being able to eigendecompose a summary matrix and yield orthogonal vectors belongs to the class of normal matrices, which by definition are diagonalizable by a unitary matrix. The class of normal matrices includes symmetric and skewsymmetric matrices, among others. This fact suggests a broader class of PCA variants that are a subject of future work.”</p> </blockquote> <p>In order to understand the inner workings of matrix decomposition, I’d like to review first the following definitions.</p> <blockquote> <p><em>Def</em>: The Hermitian tranpose (or conjugate transpose) of a matrix \(A\) is denoted as \(A^{\dagger} = \bar{A}^{T}\), which the tranpose of the complex conjugate (applied to each entry). Equivalent notations are \(A^H, \; A^{*}\). Notice that for a real matrix \(A \in \mathbb{R}^{n \times n}, \; A^{\dagger} = A^{T}\).</p> <p><em>Def</em>: A matrix is called <strong>Hermitian</strong> (or <strong>self-adjoint</strong>) if \(A = A^{\dagger}\). Notice that A real <strong>Hermitian</strong> matrix is equivalently <strong>symmetric</strong>.</p> <p><em>Def</em>: A matrix \(U\) is called <strong>unitary</strong> if \(U^{\dagger}U = I\) Notice that a real <strong>unitary</strong> matrix is equivalently <strong>orthogonal</strong>.</p> </blockquote> <p>Notice that we also have the following theorems for the above special matrices (more discussions seen in this <a href="https://www.math.purdue.edu/~eremenko/dvi/lect3.26.pdf">lecture note</a>):</p> <blockquote> <p><em>Spectral theorem</em> for Hermitian matrices: For a <strong>Hermitian</strong> matrix,</p> <p>(i) all eigenvalues are real,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>and likewise</p> <blockquote> <p><em>Spectral theorem</em> for unitary matrices: For a <strong>unitary</strong> matrix,</p> <p>(i) all eigenvalues have magnitude 1,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>Consequently, Hermitian and unitary matrices are always <strong>diagonalizable</strong> (indeed some eigenvalues might be the same). Notice that eigenvectors of any matrix corresponding to distinct eigenvalues are linearly independent. Here with Hermitian and unitary matrices they are not only linearly indepedent, but also <strong>orthogonal</strong>.</p> <p>Theorems (i) for both Hermitian and unitary matrices could be proven separately. Since (ii) and (iii) are the same, we might be thinking about looking for a greater class of matrices which include these two types, but carrying with more general properties, entailing (ii) and (iii). Let me introduce the following:</p> <blockquote> <p><em>Def</em>: A normal matrix is a matrix that commutes with its adjoint: \([A, A^{\dagger}] = 0\) where [B, C] = BC - CB</p> </blockquote> <p>Note that Hermitian and unitary matrices are special cases of a normal matrix. Also, the previous definitions will be helpful for the next extension into Lie group/algebra. For normal matrices we have the following:</p> <blockquote> <p><em>Spectral theorem</em> for normal matrices: A matrix is <strong>normal</strong> <em>if and only if</em> there is an orthogonal basis consisting of eigenvectors.</p> </blockquote> <p>From the above theorems we could easily see that a symmetric matrix \(A\) has all real eigenvalues (becuase it’s a Hermitian matrix). Consequently, it has the decomposition:</p> \[A = B \Lambda B^{-1}\] <p>where \(\Lambda\) is a real iagonal matrix, B orthogonal.</p> <p>There’s also a famous fact between Hermitian and unitary matrix:</p> <blockquote> <p>There exists a 1-1 correponsdence between the set of unitary matrices \(U\) and the exponental of the set of Hermitian matrices \(H\), i.e.:</p> \[U = exp(iH)\] </blockquote> <p>The proof is not hard and not displayed here. The message it encodes is important: the unitary matrices are exactly in the format of the exponential map of Hermitian matrices.</p> <p>Similarly, we could show the following for orthogonal matrix: Given \(A\) as a skew-symmetric (real) matrix (\(A^{\dagger} = A^{T} = -A\)), notice first that</p> \[-iA = iA^T = iA^{\dagger} = (-iA)^{\dagger}\] <p>Consequently, \(-iA\) is a Hermitian matrix and thus since</p> \[e^{A} = e^{i(-iA)} = e^{i(-iA)^{\dagger}}\] <p>Then \(e^{A}\) is a unitary matrix. Since \(A\) is real, \(e^{A}\) has to be real. Consequently, \(e^{A}\) is an orthogonal matrix.</p> <p>However, the exponential map of skew symmetric matrices does not result in all orthogonal matrix:</p> \[det(e^A) = e^{trA} = e^{0} = 1\] <p>which means that this way only characterizes rotation matrices (determinant equal to 1, unlike reflection which changes the orientation).</p> <h3 id="relations-with-lie-group-and-lie-algebra">Relations with Lie group and Lie algebra</h3> <p>Since we are talking about dynamics with symmetric and skew-symmetric matrices, how do they relate to Lie group/algebra?</p> <p>Also, since this constrained optimization problem has a unique global optimum (how should we know this)?</p>]]></content><author><name></name></author><category term="Computational Neuroscience"/><category term="Latent Dynamics"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Introduce neural latent dynamics from jPCA]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://jasmineruixiang.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://jasmineruixiang.github.io/blog/2024/tabs</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="f0342125-5c55-4f08-8b0f-92589f227f35" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="f0342125-5c55-4f08-8b0f-92589f227f35" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="96bac663-1d8f-4e46-95b3-8d32ee6f06eb" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="96bac663-1d8f-4e46-95b3-8d32ee6f06eb" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="f06be595-752f-4cc5-a37e-de3a7547b1da" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="f06be595-752f-4cc5-a37e-de3a7547b1da" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://jasmineruixiang.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://jasmineruixiang.github.io/blog/2024/typograms</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://jasmineruixiang.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://jasmineruixiang.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://jasmineruixiang.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://jasmineruixiang.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry><entry><title type="html">a post with advanced image components</title><link href="https://jasmineruixiang.github.io/blog/2024/advanced-images/" rel="alternate" type="text/html" title="a post with advanced image components"/><published>2024-01-27T11:46:00+00:00</published><updated>2024-01-27T11:46:00+00:00</updated><id>https://jasmineruixiang.github.io/blog/2024/advanced-images</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2024/advanced-images/"><![CDATA[<p>This is an example post with advanced image components.</p> <h2 id="image-slider">Image Slider</h2> <p>This is a simple image slider. It uses the <a href="https://swiperjs.com/">Swiper</a> library. Check the <a href="https://swiperjs.com/demos">examples page</a> for more information of what you can achieve with it.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what advanced image components could look like]]></summary></entry><entry><title type="html">a post with vega lite</title><link href="https://jasmineruixiang.github.io/blog/2024/vega-lite/" rel="alternate" type="text/html" title="a post with vega lite"/><published>2024-01-27T00:20:00+00:00</published><updated>2024-01-27T00:20:00+00:00</updated><id>https://jasmineruixiang.github.io/blog/2024/vega-lite</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2024/vega-lite/"><![CDATA[<p>This is an example post with some <a href="https://vega.github.io/vega-lite/">vega lite</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">vega_lite
</span><span class="sb">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}
</code></pre> <p>This plot supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included vega lite code could look like]]></summary></entry><entry><title type="html">a post with geojson</title><link href="https://jasmineruixiang.github.io/blog/2024/geojson-map/" rel="alternate" type="text/html" title="a post with geojson"/><published>2024-01-26T17:57:00+00:00</published><updated>2024-01-26T17:57:00+00:00</updated><id>https://jasmineruixiang.github.io/blog/2024/geojson-map</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2024/geojson-map/"><![CDATA[<p>This is an example post with some <a href="https://geojson.org/">geojson</a> code. The support is provided thanks to <a href="https://leafletjs.com/">Leaflet</a>. To create your own visualization, go to <a href="https://geojson.io/">geojson.io</a>.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">geojson
</span><span class="sb">{
  "type": "FeatureCollection",
  "features": [
    {
      "type": "Feature",
      "properties": {},
      "geometry": {
        "coordinates": [
          [
            [
              -60.11363029935569,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -2.904625022183211
            ]
          ]
        ],
        "type": "Polygon"
      }
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-geojson">{
  "type": "FeatureCollection",
  "features": [
    {
      "type": "Feature",
      "properties": {},
      "geometry": {
        "coordinates": [
          [
            [
              -60.11363029935569,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -3.162613728707967
            ],
            [
              -59.820894493858034,
              -2.904625022183211
            ],
            [
              -60.11363029935569,
              -2.904625022183211
            ]
          ]
        ],
        "type": "Polygon"
      }
    }
  ]
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><category term="maps"/><summary type="html"><![CDATA[this is what included geojson code could look like]]></summary></entry></feed>