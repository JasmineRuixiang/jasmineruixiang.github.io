<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jasmineruixiang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasmineruixiang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-04T03:43:44+00:00</updated><id>https://jasmineruixiang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Geom/Topo/Dynam Mumble(3): Subspace geometry</title><link href="https://jasmineruixiang.github.io/blog/2026/subspace/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(3): Subspace geometry"/><published>2026-02-03T21:49:38+00:00</published><updated>2026-02-03T21:49:38+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/subspace</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/subspace/"><![CDATA[<h2 id="setup">Setup</h2> <p>Suppose we compute PCA on two datasets (e.g., train vs.\ test), and we keep the top-$r$ principal components. Let</p> \[U_{\text{train}} \in \mathbb{R}^{d \times r}, \qquad U_{\text{test}} \in \mathbb{R}^{d \times r},\] <p>where each matrix has <strong>orthonormal columns</strong> (so each is a basis for an $r$-dimensional subspace of $\mathbb{R}^d$).</p> <p>Our first obvious intuition might be to define the <strong>subspace overlap matrix</strong> as the following:</p> \[S = U_{\text{train}}^{\top} U_{\text{test}} \in \mathbb{R}^{r \times r}.\] <p>So, each entry is a dot product between basis vectors:</p> \[S_{ij} = u_i^{(\text{train})} \cdot u_j^{(\text{test})}.\] <p>At first glance, this looks like a direct ‚Äúbasis alignment‚Äù comparison, which is exactly what we aim for. How else could we characterize subspace change other than looking at pairs-wise relationships among two sets of basis vectors? Well, there‚Äôre a few caveats‚Ä¶</p> <hr/> <h2 id="1-why-basis-by-basis-alignment-is-unstable">1] Why basis-by-basis alignment is unstable</h2> <p>Comparing PCA vectors one-by-one (e.g., ‚ÄúPC1 vs.\ PC1‚Äù) is unstable because PCA eigenvectors are <strong>not uniquely defined</strong> in two common cases. The following problems might pop up ‚Äî</p> <h3 id="i-sign-flips">(i) Sign flips</h3> <p>If \(u\) is an eigenvector, then \(-u\) is also an eigenvector.</p> <p>So dot products can flip sign even when the <em>subspace is identical</em>.</p> <h3 id="ii-degenerate--near-degenerate-eigenvalues-rotation-inside-the-subspace">(ii) Degenerate / near-degenerate eigenvalues (rotation inside the subspace)</h3> <p>If</p> \[\lambda_i \approx \lambda_{i+1},\] <p>then the corresponding principal directions inside the 2D span can rotate dramatically under tiny perturbations (noise, finite-sample effects, etc.).</p> <p>This means that even if the <em>span</em> is essentially the same, the individual vectors $u_i$ can change a lot.<br/> So comparing ‚ÄúPC1 to PC1‚Äù is not meaningful.</p> <hr/> <h2 id="2-what-singular-values-do-that-dot-products-dont">2] What singular values do that dot products don‚Äôt</h2> <p>The matrix</p> \[S = U_{\text{train}}^{\top} U_{\text{test}}\] <p>depends on the <em>chosen bases</em> inside each subspace. If we change bases within either subspace via orthogonal transformations:</p> \[U_{\text{train}} \to U_{\text{train}} R_1, \qquad U_{\text{test}} \to U_{\text{test}} R_2,\] <p>where $R_1, R_2 \in \mathbb{R}^{r \times r}$ are orthogonal (including sign flips as a special case), then</p> \[S \to (U_{\text{train}}R_1)^{\top}(U_{\text{test}}R_2) = R_1^{\top} S R_2.\] <p>So the <em>entries</em> of $S$ can change wildly.</p> <h3 id="key-fact-invariance">Key fact (invariance)</h3> <blockquote> <p>The <strong>singular values</strong> of \(S\) are invariant under left/right orthogonal rotations:</p> </blockquote> <ul> <li>Left-multiplying by an orthogonal matrix does not change singular values.</li> <li>Right-multiplying by an orthogonal matrix does not change singular values.</li> </ul> <p>Therefore, even if PCA ‚Äúrelabels,‚Äù flips signs, or rotates the basis vectors within the subspace, the <strong>singular values remain unchanged</strong>.</p> <p>This means singular values capture a property of the <strong>subspaces</strong>, not of the particular eigenvectors chosen.</p> <hr/> <h2 id="3-geometric-meaning-principal-angles">3] Geometric meaning: principal angles</h2> <p>Take the SVD:</p> \[S = Q \Sigma R^{\top},\] <p>where</p> \[\Sigma = \mathrm{diag}(\sigma_1,\dots,\sigma_r).\] <p>A fundamental result is:</p> \[\sigma_i = \cos(\theta_i),\] <p>where $\theta_i$ are the <strong>principal angles</strong> between the two $r$-dimensional subspaces.</p> <p>Interpretation:</p> <ul> <li>$\theta_i = 0 \implies$ perfectly aligned direction exists (since $\cos(\theta_i)=1$)</li> <li>$\theta_i = 90^\circ \implies$ orthogonal direction (since $\cos(\theta_i)=0$)</li> </ul> <p>So the singular values summarize <em>how much overlap</em> the two subspaces have along their best-aligned directions.</p> <hr/> <h2 id="4-why-this-is-the-stable-comparison">4] Why this is the ‚Äústable‚Äù comparison</h2> <p>Think of $U_{\text{train}}$ and $U_{\text{test}}$ as <strong>arbitrary coordinate systems</strong> inside their respective subspaces.</p> <p>A meaningful comparison should ignore that arbitrariness.</p> <p>Principal angles / singular values do exactly this: they compute the <strong>best possible matching</strong> between directions in the two subspaces.</p> <p>Instead of comparing ‚ÄúPC1 $\leftrightarrow$ PC1,‚Äù we solve an optimal alignment problem:</p> \[\max_{\|a\|=\|b\|=1} a^{\top}\bigl(U_{\text{train}}^{\top}U_{\text{test}}\bigr)b,\] <p>and the sequence of best matches yields</p> \[\sigma_1,\sigma_2,\dots\] <p>as the strengths of alignment along the best-aligned directions.</p> <hr/> <h2 id="5-tiny-example-intuition-2d-case">5] Tiny example intuition (2D case)</h2> <p>Suppose both subspaces are actually the same 2D plane in $\mathbb{R}^d$.</p> <p>You could pick:</p> <ul> <li>$U_{\text{train}}$ = standard basis in that plane</li> <li>$U_{\text{test}}$ = same plane but rotated by $45^\circ$ inside it</li> </ul> <p>Then $S$ might look like a rotation matrix:</p> \[S= \begin{pmatrix} \cos 45^\circ &amp; -\sin 45^\circ \\ \sin 45^\circ &amp; \cos 45^\circ \end{pmatrix}.\] <p>The entries are not the identity, so basis-by-basis dot products look ‚Äúnot aligned.‚Äù</p> <p>But the singular values of a rotation matrix are both $1$.</p> <p>So singular values correctly say: <strong>the subspaces are identical</strong>.</p> <p>That‚Äôs the whole point.</p> <hr/> <h2 id="bottom-line">Bottom line</h2> <p>We use singular values of</p> \[U_{\text{train}}^{\top}U_{\text{test}}\] <p>because:</p> <ul> <li>‚úÖ they are invariant to sign flips / rotations / re-ordering of PCA vectors inside the subspace</li> <li>‚úÖ they define principal angles, which are a true subspace-to-subspace comparison</li> <li>‚úÖ they give a stable measure of drift even when eigenvectors are not uniquely defined</li> </ul> <hr/> <h2 id="a-sidenote-connection-to-projection-distance">A sidenote: Connection to projection distance</h2> <p>Let $P_{\text{train}}$ and $P_{\text{test}}$ be the orthogonal projection matrices onto the two subspaces:</p> \[P_{\text{train}} = U_{\text{train}}U_{\text{train}}^{\top}, \qquad P_{\text{test}} = U_{\text{test}}U_{\text{test}}^{\top}.\] <p>Then one can show the Frobenius-distance relationship:</p> \[\|P_{\text{train}} - P_{\text{test}}\|_F^2 = 2r - 2\|U_{\text{train}}^{\top}U_{\text{test}}\|_F^2 = 2\sum_{i=1}^r \sin^2(\theta_i).\]]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Subspace Geometry and Computation]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (4): Connections (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (4): Connections (in progress)"/><published>2026-01-15T15:48:02+00:00</published><updated>2026-01-15T15:48:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(4)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/"><![CDATA[<p>This is episode 4 on the smooth manifold series. Today we will be exploring more on tangent vectors, and another key concept related to tangent spaces for different tangent planes: connections.</p> <h2 id="intuition-directional-derivatives">Intuition: Directional Derivatives</h2> <p>üåê The Connection: Bridging Derivatives from $\mathbb{R}^3$ to Curved Manifolds The concept of a connection is the necessary tool that allows us to perform differential calculus on curved spaces (manifolds), such as the surface of a sphere. It generalizes the familiar idea of the directional derivative from flat Euclidean space ($\mathbb{R}^3$).</p> <ol> <li>Directional Derivatives in Euclidean Space ($\mathbb{R}^3$) In $\mathbb{R}^3$ with Cartesian coordinates $(x, y, z)$, the directional derivative provides a simple way to measure change. The basis vectors $\left{ \mathbf{i}, \mathbf{j}, \mathbf{k} \right}$ (or the equivalent operators $\left{ \frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z} \right}$) are constant, allowing us to define derivatives simply as component-wise partial derivatives. Let $p=(1, 2, 0)$ be a point, $X=(y, -x, 3x)$ be the direction vector field, and $V=(xz, y^2, -2x)$ be a vector field. A. Derivative of a Scalar Function ($D_X f$) This is the directional derivative of a smooth scalar function $f$ in the direction $X$.</li> </ol> <p>Perspective Formula Example Result for f(x,y,z)=xy2+z at p Vector-Based (Calculus) $D_X f = \nabla f \cdot X$ $D_X f(p) = \langle 4, 4, 1 \rangle \cdot \langle 2, -1, 3 \rangle = \mathbf{7}$ Point Derivation (Geometry) $X[f] = X^i \frac{\partial f}{\partial x^i}$ $Xf = (y^3 - 2x^2y + 3x)\big</p> <p>This confirms that in differential geometry, a tangent vector $X$ is rigorously defined as a point derivation‚Äîan operator that mimics the directional derivative by satisfying the Leibniz rule. B. Derivative of a Vector Field ($D_X V$) This derivative measures how the vector field $V$ changes as we move in the direction $X$. In $\mathbb{R}^3$, this is calculated by taking the directional derivative of each component of $V$. Using the vector fields $X$ and $V$: The $k$-th component of the resulting vector $D_X V$ is $(D_X V)^k = \sum_{i} X^i \frac{\partial V^k}{\partial x^i}$. Component 1 (i.e., $k=1$): $(D_X V)^1 = yz + 3x^2$ Component 2 (i.e., $k=2$): $(D_X V)^2 = -2xy$ Component 3 (i.e., $k=3$): $(D_X V)^3 = -2y$ Evaluating at $p=(1, 2, 0)$ gives:</p> <p>\(D_X V(p) = \langle 3, -4, -4 \rangle\)</p> <ol> <li>The General Connection: The Covariant Derivative ($\nabla_X V$) The formula for $D_X V$ fails on a curved manifold $M$ because the tangent spaces $T_p M$ and $T_q M$ at nearby points $p$ and $q$ are distinct. We cannot simply subtract the vector $V(p)$ from $V(q)$. A Connection ($\nabla$) is the rule that provides the necessary ‚Äúcorrection‚Äù to define the derivative intrinsically on $M$. The resulting derivative is called the covariant derivative $\nabla_X V$. A. Definition and Axioms The connection is an operator $\nabla: C^{\infty}(M) \times C^{\infty}(M) \to C^{\infty}(M)$ that maps two vector fields, $X$ and $V$, to a new vector field $\nabla_X V$, satisfying: Linearity over Functions in $X$: $\nabla_{fX} V = f \nabla_X V$ Linearity in $V$: $\nabla_X (aV + bW) = a \nabla_X V + b \nabla_X W$ Leibniz Rule: $\nabla_X (fV) = (Xf) V + f \nabla_X V$ (where $Xf$ is the directional derivative of $f$) B. The Coordinate Form and Christoffel Symbols In local coordinates, the covariant derivative $\nabla_X V$ is defined using the Christoffel symbols ($\Gamma^k_{ij}$), which represent the rate of change of the coordinate basis vectors $\left{ \frac{\partial}{\partial x^i} \right}$:</li> </ol> <p>\((\nabla_X V)^k = \underbrace{X^i \frac{\partial V^k}{\partial x^i}}_{\text{I. Flat-Space Derivative Term}} + \underbrace{X^i \Gamma^k_{ij} V^j}_{\text{II. Curvature Correction Term}}\) The Christoffel symbols $\Gamma^k_{ij}$ are defined by the action of the connection on the basis vectors:</p> <p>\(\nabla_{\frac{\partial}{\partial x^i}} \frac{\partial}{\partial x^j} = \sum_{k} \Gamma^k_{ij} \frac{\partial}{\partial x^k}\) Difference from Euclidean Case: In $\mathbb{R}^3$ with Cartesian coordinates, $\Gamma^k_{ij} = 0$, and the second term vanishes, resulting in $\nabla_X V = D_X V$. On a curved manifold, $\Gamma^k_{ij} \neq 0$, and the correction term is essential.</p> <ol> <li>The Levi-Civita Connection In Riemannian Geometry, a Riemannian metric $g$ is introduced to measure lengths and angles. The Levi-Civita Connection is the unique connection that respects this metric structure. It is defined by two crucial properties: Metric Compatibility: The connection must preserve the metric $g$ under parallel transport.</li> </ol> <p>\(X(g(V, W)) = g(\nabla_X V, W) + g(V, \nabla_X W)\) Zero Torsion: The connection must satisfy:</p> \[\nabla_X Y - \nabla_Y X = [X, Y]\] <p>where $[X, Y]$ is the Lie bracket. The Christoffel symbols of the Levi-Civita Connection are thus entirely determined by the components of the metric $g_{ij}$ and their first derivatives:</p> \[\Gamma^k_{ij} = \frac{1}{2} g^{k\ell} \left( \frac{\partial g_{j\ell}}{\partial x^i} + \frac{\partial g_{i\ell}}{\partial x^j} - \frac{\partial g_{ij}}{\partial x^\ell} \right)\] <h2 id="connections">Connections</h2> <h2 id="christoffel-symbols">Christoffel Symbols</h2> <h2 id="levi-civita-connections">Levi-Civita Connections</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Exploration of Connection]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)"/><published>2026-01-14T20:22:25+00:00</published><updated>2026-01-14T20:22:25+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(3)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/"><![CDATA[<p>This is episode 3 on the smooth manifold series. Today we will be diving into the properties of maps between manifolds. I will first summarize how to understand and compute the differential of a smooth map between manifolds, both abstractly and concretely, culminating in the matrix-valued example<br/> \(F(A) = A^\top A.\)</p> <hr/> <h2 id="1-differential-canonical-definition">1] Differential: canonical definition</h2> <h3 id="11-differential-of-a-smooth-map-intrinsic-definition">1.1 Differential of a Smooth Map (Intrinsic Definition)</h3> <p>Let \(F : M \to N\) be a smooth map between smooth manifolds.</p> <p>For any point $ p \in M $, the <strong>differential</strong> is a linear map which is a <strong>pushforward of derivations</strong>. \(dF_p : T_p M \longrightarrow T_{F(p)} N.\)</p> <h3 id="derivation-based-definition">Derivation-based definition</h3> <p>If $ v \in T_p M $ is a tangent vector viewed as a derivation, then \((dF_p v)(g) := v(g \circ F), \qquad g \in C^\infty(N).\)</p> <p>This definition is <strong>coordinate-free</strong>.</p> <p>It might appear at first both unnecessarily abstract and underestimated as to its computation. We might claim that it is essentially just a Jacobian matrix in local coordinates. However, the essence of this concept resides on its definition to be conceptually a coordinate-independent linear map between tangent spaces.</p> <hr/> <h3 id="12-coordinate-representation-and-the-jacobian">1.2 Coordinate Representation and the Jacobian</h3> <p>To compute $ dF_p $ in practice:</p> <ol> <li>Choose a chart $ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>Choose a chart $ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ol> <p>Define the coordinate expression: \(\tilde F = \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Then: \(\boxed{ dF_p \;\text{is represented by}\; D\tilde F(\varphi(p)) }\)</p> <p>That is, <strong>the Jacobian matrix is the coordinate representation of the differential</strong>.</p> <blockquote> <p>The Jacobian depends on coordinates; the linear map $ dF_p $ does not.</p> </blockquote> <hr/> <h3 id="13-why-this-is-not-just-the-jacobian">1.3 Why this is not ‚Äújust‚Äù the Jacobian</h3> <p>The Jacobian depends on coordinates;<br/> $ dF_p $ does not.</p> <p>More precisely:</p> <ul> <li>$ dF_p $ is a <strong>geometric linear map</strong></li> <li>The Jacobian is a <strong>matrix representation</strong> of that map in chosen bases: \(\frac{\partial \bigl(\psi^1 \circ F,\;\dots,\;\psi^n \circ F\bigr)} {\partial \bigl(x^1,\;\dots,\;x^m\bigr)}\)</li> </ul> <p>If you change charts, the matrix changes by:</p> \[\boxed{ J_{\text{new}} = D\psi\,\cdot\, J_{\text{old}} \,\cdot\, (D\varphi^{-1}) }\] <p>but the underlying linear map $ dF_p $ stays the same.</p> <hr/> <h2 id="2-differential-alternative-interpretation">2] Differential: alternative interpretation</h2> <h3 id="21-curve-based-definition">2.1 Curve-based definition</h3> <p>This is also coordinate-free:</p> <p>If<br/> \(\gamma : (-\varepsilon,\varepsilon) \to M\) is a smooth curve with \(\gamma(0) = p \quad \text{and} \quad \gamma'(0) = v \in T_p M,\) then \(\boxed{ dF_p(v) = (F \circ \gamma)'(0) \in T_{F(p)} N. }\)</p> <p>No coordinates anywhere. This viewpoint is often the most intuitive and is fully equivalent to the derivation definition.</p> <hr/> <h3 id="22-curves-in-local-coordinates">2.2 Curves in local coordinates</h3> <p>Choose charts:</p> <ul> <li>$ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>$ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ul> <p>Define the coordinate representation: \(\tilde F := \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Now define the coordinate curve: \(\tilde\gamma := \varphi \circ \gamma : (-\varepsilon,\varepsilon) \to \mathbb{R}^m.\)</p> <p>Consequently, in coordinates the statement is the following:</p> \[\boxed{ D\tilde F(\varphi(p)) \cdot \tilde\gamma'(0) = D(\psi \circ F \circ \varphi^{-1})(\varphi(p)) \cdot (\varphi \circ \gamma)'(0). }\] <p>In the coordinate formula, \(\gamma'(0) \quad \text{really means} \quad \tilde\gamma'(0) = (\varphi \circ \gamma)'(0).\)</p> <p>Here‚Äôs a diagram that makes everything explicit</p> \[\begin{array}{ccc} T_p M &amp; \xrightarrow{dF_p} &amp; T_{F(p)} N \\ \downarrow d\varphi_p &amp; &amp; \uparrow d\psi^{-1}_{\psi(F(p))} \\ \mathbb{R}^m &amp; \xrightarrow{D\tilde F(\varphi(p))} &amp; \mathbb{R}^n \end{array}\] <p>Thus:</p> <ul> <li>$ \gamma‚Äô(0) $ lives in $ T_p M $</li> <li>$ (\varphi \circ \gamma)‚Äô(0) = d\varphi_p(\gamma‚Äô(0)) \in \mathbb{R}^m $</li> <li>$ D\tilde F(\varphi(p)) $ acts on that coordinate vector</li> </ul> <hr/> <h3 id="23-sidenote-graph-differential">2.3 Sidenote: Graph Differential</h3> <p>The <strong>graph</strong> of $ F $ is \(\Gamma_F = \{ (p, F(p)) \mid p \in M \} \subset M \times N.\)</p> <p>Define the graph map: \(\Phi : M \to M \times N, \quad \Phi(p) = (p, F(p)).\)</p> <p>Its differential is: \(\boxed{ d\Phi_p(v) = (v, dF_p(v)). }\)</p> <p>This is what is often called the <strong>graph differential</strong>.</p> <hr/> <h2 id="3-special-case-maps-between-vector-spaces">3] Special Case: Maps Between Vector Spaces</h2> <h3 id="31-a-great-simplification">3.1 A great simplification</h3> <p>If $ M = \mathbb{R}^m $, $ N = \mathbb{R}^n $, then: \(T_p M \cong \mathbb{R}^m, \quad T_{F(p)} N \cong \mathbb{R}^n.\)</p> <p>In this case:</p> <ul> <li>$ dF_p $ is a linear map $ \mathbb{R}^m \to \mathbb{R}^n $</li> <li>Its matrix is exactly the <strong>Jacobian matrix</strong></li> <li>$ dF_p(H) $ coincides with the <strong>Fr√©chet / directional derivative</strong></li> </ul> <hr/> <h3 id="32-worked-example--fa--atop-a-">3.2 Worked Example: $ F(A) = A^\top A $</h3> <p>Let \(F : \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}, \quad F(A) = A^\top A.\)</p> <p>Since $ \mathbb{R}^{n \times n} $ is a vector space, \(T_A(\mathbb{R}^{n \times n}) \cong \mathbb{R}^{n \times n}.\)</p> <h4 id="321-direct-computation">3.2.1 Direct computation</h4> <p>There are several ways to compute the differential. The most straight-forward method is to do the following (as you might imagine):</p> <p>For \(H \in T_A(\mathbb{R}^{n \times n})\), \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} (A+tH)^\top(A+tH).\)</p> <p>Expanding: \((A+tH)^\top(A+tH) = A^\top A + t(H^\top A + A^\top H) + t^2 H^\top H.\)</p> <p>Thus: \(\boxed{ dF_A(H) = H^\top A + A^\top H. }\)</p> <h4 id="322-curve-based-computation">3.2.2 Curve-based computation</h4> <p>Let \(A(t)\) be a smooth curve with: \(A(0)=A, \quad A'(0)=H.\)</p> <p>Then: \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} A(t)^\top A(t) = H^\top A + A^\top H.\)</p> <p>This makes it clear that the definition of \(dF_A(H)\) is <strong>coordinate-free</strong>.</p> <p>Sidenote: if we restrict \(A\) to symmetric or SPD matrices, what will we see? Or what if we connect this to Riemannian geometry on \(GL(n)\) or \(SPD(n)\)? We‚Äôll come back to this later when we discuss Lie group and Lie algebra.</p> <hr/> <h2 id="4-back-to-classical-regular-surfaces-parametrizations-immersions">4] Back to classical regular surfaces (parametrizations, immersions)</h2> <h3 id="the-question-in-do-carmo-diffgeom">The Question in do Carmo DiffGeom</h3> <p>In do Carmo‚Äôs definition of a <strong>regular surface</strong> in \(\mathbb{R}^3\), a coordinate map \(X : U \subset \mathbb{R}^2 \to \mathbb{R}^3\) is required to satisfy two conditions:</p> <ol> <li>\(X\) is a <strong>differentiable homeomorphism</strong> onto its image.</li> <li>The differential \(dX_p\) is <strong>injective</strong> at every point $p \in U$.</li> </ol> <p>Since \(X\) maps from \(\mathbb{R}^2\) to \(\mathbb{R}^3\), its differential can never be surjective, so injectivity (rank 2) is the meaningful requirement.</p> <p>A natural question arises:</p> <blockquote> <p>If \(X\) is already a differentiable homeomorphism, isn‚Äôt its differential automatically injective?</p> </blockquote> <p>The answer is <strong>no</strong>.</p> <p>Well, to make it explicit, let‚Äôs figure out first what a differentiable homeomorphism actually gives us: If \(X : U \to \mathbb{R}^3\) is a differentiable homeomorphism onto its image, then:</p> <ul> <li>\(X\) is <strong>continuous and injective</strong></li> <li>\(X^{-1}\) is <strong>continuous</strong> (but <em>not</em> necessarily differentiable)</li> <li>Topologically, \(X(U)\) looks like a 2‚Äëdimensional surface</li> </ul> <p>This is a <strong>topological</strong> statement plus differentiability of $X$. It controls <em>points</em>, but says nothing about what happens to <em>directions</em>. Crucially, differentiability of the inverse is <em>not</em> assumed.</p> <p>Fine, but then why injectivity of the differential is not automatically assured? Notice that the differential</p> \[dX_p : \mathbb{R}^2 \to \mathbb{R}^3\] <p>being injective means it has <strong>rank 2</strong> meaning no tangent direction is collapsed. A map can be:</p> <ul> <li>injective,</li> <li>continuous with continuous inverse,</li> <li>differentiable,</li> </ul> <p>and <em>still</em> have rank drop somewhere. Let me give a concrete example:</p> <p>Consider \(X(u,v) = (u^3, v, 0).\)</p> <p>Properties of this map:</p> <ul> <li>It is <strong>injective</strong></li> <li>It is a <strong>homeomorphism onto its image</strong></li> <li>It is differentiable everywhere</li> </ul> <p>However, its differential is \(dX_{(u,v)} = \begin{pmatrix} 3u^2 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix}\)</p> <p>At \($u = 0\), this matrix has <strong>rank 1</strong>, not 2. One tangent direction is squashed. Therefore, this map is <strong>not an immersion</strong>, and it does <strong>not</strong> define a regular surface in do Carmo‚Äôs sense.</p> <hr/> <p>However, you might observe here a seemingly apparent paradox: ‚ÄúBut the image is <strong>Just</strong> a plane!‚Äù</p> <p>Well, the image of \(X(u,v) = (u^3, v, 0)\) is \({(x,y,0) : x,y \in \mathbb{R}},\) which is the entire (xy)-plane indeed. As a <strong>subset</strong> of \(\mathbb{R}^3\), this plane is perfectly flat, so one might expect its tangent plane at every point to be the whole plane. So then why does the tangent collapse under this map?</p> <hr/> <p>The key point here is that here are <strong>two distinct notions</strong> at play:</p> <ol> <li>The tangent plane of a <strong>subset</strong> of \(\mathbb{R}^3\)</li> <li>The tangent plane <strong>defined by a parametrization</strong></li> </ol> <p>In do Carmo‚Äôs approach, tangent planes are defined <em>via parametrizations</em>. The tangent plane at a point is \(T_pS = \operatorname{span}{X_u(p), X_v(p)}.\)</p> <p>For the map above: \(X_u = (3u^2, 0, 0), \quad X_v = (0,1,0).\)</p> <p>At (u=0): \(X_u(0,v) = (0,0,0), \quad X_v(0,v) = (0,1,0),\) so the span is <strong>1‚Äëdimensional</strong>.</p> <p>This means:</p> <blockquote> <p>The parametrization fails to distinguish two independent directions in the parameter domain.</p> </blockquote> <p>Geometrically, the $u$-direction has been crushed.</p> <hr/> <p>But does the plane still have a 2D tangent plane?</p> <p>Yes ‚Äî but <strong>not via this parametrization</strong>.</p> <p>If instead we parametrize the same plane by \(Y(s,t) = (s,t,0),\) then \(dY = \begin{pmatrix} 1 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix},\) which has rank 2 everywhere.</p> <p>So the <em>same subset</em> becomes a <strong>regular surface</strong> under a different map.</p> <hr/> <p>Finally, what does this meana conceptually? The key lesson is:</p> <blockquote> <p><strong>Regularity is not a property of the subset alone ‚Äî it is a property of the subset together with its smooth structure.</strong></p> </blockquote> <p>Different parametrizations can induce:</p> <ul> <li>a <strong>good</strong> smooth structure (immersion)</li> <li>or a <strong>bad</strong> one (rank collapse)</li> </ul> <p>This is why do Carmo requires that <strong>there exists</strong> a local parametrization with injective differential.</p> <hr/> <p>In other words, if you ask ‚ÄúDoes that mean there might exist other maps which make the plane a regular surface?‚Äù, then the answer is Yes ‚Äî absolutely. The plane <em>is</em> a regular surface because such maps exist.</p> <p>Behind that is another question: ‚ÄúDoes immersion entirely depend on which maps we pick?‚Äù Yes!</p> <ul> <li><strong>Immersion is a property of the map</strong>, not of the set.</li> <li>A <strong>regular surface</strong> is a set for which <em>good immersions exist everywhere</em>.</li> </ul> <p>In this seense, do Carmo does separate the conditions, being deliberately modular:</p> <ol> <li><strong>Homeomorphism</strong> ‚Üí good topology (no self‚Äëintersections)</li> <li><strong>Injective differential</strong> ‚Üí good differential geometry</li> </ol> <p>whether neither condition implies the other.</p> <p>Finally, some of key points:</p> <ul> <li>The same subset of \(\mathbb{R}^3\) can support <strong>many different smooth structures</strong></li> <li>Differential geometry only works once a smooth structure is fixed</li> <li>Parametrizations are how do Carmo <em>builds</em> that structure</li> </ul> <p>This is why modern texts often say:</p> <blockquote> <p>‚ÄúA surface is a 2‚Äëdimensional smooth manifold embedded in $$\mathbb{R}^3$.‚Äù</p> </blockquote> <p>Do Carmo reaches this notion <strong>from parametrizations upward</strong>, rather than assuming it at the start.</p> <hr/> <blockquote> <p><strong>Topology sees points.</strong> <strong>Differential geometry sees directions.</strong></p> </blockquote> <p>A homeomorphism controls points. Injectivity of the differential controls directions.</p> <p>You need <strong>both</strong> to get a regular surface.</p> <h2 id="5-association-with-inverse-function-theorem">5] Association with inverse function theorem</h2> <h2 id="6-immersion-submersion">6] Immersion, submersion</h2> <p>Immersion basics</p> <p>Here I want to emphasize one key fact that:</p> <blockquote> <p>immersion imply a nonzero determinant in coordinates</p> </blockquote> <p>Why? Again, let‚Äôs recall that immersion means the differential is injective Let<br/> \(\phi:M^m\to N^n,\qquad m\le n\) and let $p\in M$.<br/> Saying <strong>(\phi) is an immersion at (p)</strong> means the differential \(d\phi_p:T_pM\to T_{\phi(p)}N\) is <strong>injective</strong>.</p> <p>Equivalently (in linear algebra language):<br/> \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Now, if write it in local coordinates ‚Üí Jacobian matrix has rank \(m\) Pick coordinate charts:</p> <ul> <li>on \(M\): \(x=(x^1,\dots,x^m)\) around \(p\)</li> <li>on \(N\): \(y=(y^1,\dots,y^n)\) around \(\phi(p)\)</li> </ul> <p>Then locally \(\phi\) looks like a smooth map between Euclidean spaces: \(y^a = \phi^a(x^1,\dots,x^m),\qquad a=1,\dots,n.\)</p> <p>Its differential in these coordinates is represented by the <strong>Jacobian matrix</strong> \(J(p)=\left(\frac{\partial \phi^a}{\partial x^i}(p)\right)\) which is an (n\times m) matrix.</p> <p>The immersion condition says: \(\operatorname{rank}(J(p))=m.\)</p> <p>So the columns of \(J(p)\) are linearly independent.</p> <p>Now we use a standard linear algebra fact:</p> <blockquote> <p>An \(n\times m\) matrix has rank \(m\) <strong>iff</strong> there exists an \(m\times m\) submatrix (choose \(m\) rows) whose determinant is nonzero.</p> </blockquote> <p>Why?</p> <ul> <li>If <strong>every</strong> \(m\times m\) minor determinant were zero, then <strong>every</strong> set of $m$ rows would be linearly dependent, so the rank would be \(&lt;m\).</li> <li>Since the rank is \(m\), at least one choice of $m$ rows gives an invertible \(m\times m\) matrix.</li> </ul> <p>Concretely: there exist indices \(1\le a_1&lt;\cdots&lt;a_m\le n\) such that the matrix \(\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)_{\alpha,i}\) has \(\det\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)\neq 0.\)</p> <p>If we now <strong>rename/reorder the target coordinates</strong> so that those special indices become \(1,\dots,m\), then we can assume:</p> \[\det\left(\frac{\partial (\phi^1,\dots,\phi^m)}{\partial (x^1,\dots,x^m)}(p)\right)\neq 0.\] <p>That‚Äôs exactly the statement: in coordinates (after renumbering if needed), an immersion gives a nonzero determinant of an \(m\times m\) Jacobian block.</p> <p>In one line summary: an immersion means \(\phi\) ‚Äúdoesn‚Äôt collapse any tangent directions,‚Äù so locally you can find $m$ coordinate functions of \(\phi\) that vary independently ‚Äî and ‚Äúvary independently‚Äù is exactly ‚ÄúJacobian block has nonzero determinant.‚Äù</p> <hr/> <h4 id="side-note-connection-to-the-constant-rank-theorem--local-normal-form-of-an-immersion">Side note: Connection to the Constant Rank Theorem / local normal form of an immersion?</h4> <p>Remember that Constant Rank Theorem (specialized to immersions) says: Let \(\phi:M^m\to N^n\) be smooth, and suppose \(\phi\) is an <strong>immersion at \(p\)</strong>.<br/> That means \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Then the constant rank theorem says:</p> <blockquote> <p>There exist coordinate charts<br/> \((U,x)\ \text{around }p,\qquad (V,y)\ \text{around }\phi(p)\) such that in these coordinates the map becomes \(y\circ \phi\circ x^{-1}(u^1,\dots,u^m) \;=\; (u^1,\dots,u^m,0,\dots,0).\)</p> </blockquote> <p>So locally, \(\phi\) looks like the <strong>standard inclusion</strong> \(\mathbb{R}^m \hookrightarrow \mathbb{R}^n,\qquad u\mapsto (u,0).\)</p> <p>That is the precise geometric meaning of ‚Äúimmersion.‚Äù</p> <p>In these special coordinates, \(\phi^1(u)=u^1,\;\dots,\;\phi^m(u)=u^m,\qquad \phi^{m+1}(u)=0,\dots,\phi^n(u)=0.\)</p> <p>So the Jacobian matrix is literally \(J= \begin{pmatrix} I_m\\ 0 \end{pmatrix}\) (an \(n\times m\) matrix).</p> <p>Now look at the top \(m\times m\) block: \(\frac{\partial(\phi^1,\dots,\phi^m)}{\partial(u^1,\dots,u^m)} = I_m,\) so \(\det(I_m)=1\neq 0.\)</p> <p>That‚Äôs exactly the coordinate statement.</p> <p>How this matches the ‚Äúminor is nonzero‚Äù argument? Before using the constant rank theorem, we only know:</p> <ul> <li>\(J(p)\) has rank \(m\)</li> <li>therefore some \(m\times m\) minor determinant is nonzero</li> </ul> <p>Then the constant rank theorem tells us that we can actually <strong>choose coordinates</strong> so that the ‚Äúgood minor‚Äù becomes the <em>first</em> \(m\) coordinates, and the map becomes \($(u,0)\).</p> <p>So:</p> <ul> <li><strong>Linear algebra fact:</strong> full rank \(\Rightarrow\) some minor \(\neq 0\)</li> <li><strong>Constant rank theorem:</strong> we can change coordinates to make that minor the obvious identity matrix.</li> </ul> <p>The geometric picture (why \((u,0)\) is the right normal form) is that an immersion means \(\phi\) ‚Äúinjects tangent vectors,‚Äù so locally \(\phi(U)\subset N\) is an $m-dimensional ‚Äúsheet‚Äù sitting inside an n-dimensional space. In good coordinates on N, that sheet looks like:</p> \[\{(y^1,\dots,y^n): y^{m+1}=\cdots=y^n=0\},\] <p>i.e. an embedded copy of \(\mathbb{R}^m\\).</p> <p>So locally, \(\phi\) is just a parametrization of that sheet.</p> <h2 id="7-embedding">7] Embedding</h2> <h3 id="72-a-key-difference-between-embedding-and-immersion">7.2 A key difference between embedding and immersion</h3> <p><strong>‚Äúno self-intersections‚Äù is one of the key geometric consequences of being <em>embedded</em></strong> (as opposed to merely <em>immersed</em>). As stated above, if a manifold \(M\) is <strong>embedded</strong> in \(\mathbb{R}^k\), it sits inside \(\mathbb{R}^k\) as a ‚Äúnice subset,‚Äù like a surface you could physically draw without crossing itself. More formally, an <strong>embedding</strong> \(F: M \to \mathbb{R}^k\) means:</p> <ol> <li>\(F\) is a <strong>smooth immersion</strong> (its differential is injective everywhere), and</li> <li>\(F\) is a <strong>homeomorphism onto its image</strong> \(F(M)\) (with the subspace topology).</li> </ol> <p>That second condition is exactly what rules out the classic ‚Äúself-crossing‚Äù pathology. If the image ‚Äúintersects itself‚Äù in the sense that two different points \(p\neq q\in M\) map to the same point in \(\mathbb{R}^k\), i.e. \(F(p)=F(q),\) then \(F\) is <strong>not injective</strong>, so it can‚Äôt be an embedding.</p> <p>So: <strong>an embedded submanifold cannot cross itself as a set in \(\mathbb{R}^k\)</strong>.</p> <p>On the other hand, an <strong>immersion</strong> can look like a manifold with self-crossings in \(\mathbb{R}^k\). An example will be the ‚Äúfigure-eight curve‚Äù in \(\mathbb{R}^2\) can be parametrized smoothly with nonzero derivative everywhere, so it‚Äôs an immersion, but it‚Äôs <strong>not embedded</strong> because it fails injectivity / fails to be a homeomorphism onto its image.</p> <p>In shoft,</p> <ul> <li><strong>Embedded \(\Rightarrow\)</strong> injective + ‚Äútopologically correct‚Äù inclusion<br/> \(\Rightarrow\) <strong>no self-intersections</strong>.</li> <li><strong>Immersed \(\Rightarrow\)</strong> locally nice but can globally overlap<br/> \(\Rightarrow\) <strong>self-intersections possible</strong>.</li> </ul> <h2 id="8-submanifolds">8] Submanifolds</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Explore relations of maps between manifolds]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)"/><published>2025-09-17T23:30:02+00:00</published><updated>2025-09-17T23:30:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(2)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/"><![CDATA[<p>This is episode 2 on the smooth manifold series. Today we will be diving into concepts that appear initially very intuitive at first glance, but then the extended version of which is indeed quite abstract.</p> <h2 id="0-familiar-examples">0] Familiar examples</h2> <p>Let me start with two simple concrete examples to illustrate what tangent vectors and tangent space are. Indeed they match up to our intuition!</p> <p>Let‚Äôs first say that we have a unit circle in \(\mathbb{R}^2\), or basically let‚Äôs denote it as \(S^1\) (this is a standard notation as</p> <p>\(S^{n} = \{ x \in \mathbb{R}^{n+1} \mid |x| = 1 \}\),</p> <p>representing the surface of an \((n+1)\)-dimensional ball). Pick \(p = (1, 0)\). If I ask you what the tangent vector is starting at \(p\) and tangnet to \(S^1\)? Your answer is probably a vector pointing upward or downward with its tail at \(p\). Indeed,</p> <h2 id="1-three-equivalent-definitions-of-tangent-space">1] Three equivalent definitions of Tangent Space</h2> <p>We‚Äôll cover three equivalent definitions tangent space.</p> <h3 id="1-tangent-vectors-are-equivalence-classes-of-curves">[1] Tangent vectors are equivalence classes of curves</h3> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>tangent vectors are equivalence classes of smooth curves through \(p\):</p> \[T_pM = \{\frac{d}{dt}\Big|_{t = 0} \gamma(t) \Big| \gamma: (\epsilon, \epsilon) \rightarrow M, \; \gamma(0) = p \}\] <p>where \(\gamma_1 \sim \gamma_2\) if in some (equivalently, any) coordinate chart \(\phi: U \subset M \rightarrow \mathbb{R}^n\):</p> \[\frac{d}{dt}\Big|_{t = 0} \phi(\gamma_1(t)) = \frac{d}{dt}\Big|_{t = 0} \phi(\gamma_2(t))\] <p>Consequently, each tangent vector is represented by the velocity of a curve through \(p\).</p> <h3 id="2-tangent-vectors-are-derivations-at-p">[2] Tangent vectors are derivations at \(p\)</h3> <p>Let \(M\) be a manifold, the tangent space at \(p \in M\), denoted as \(T_pM\), is the the vector space of all derivations at \(p\). Notice that a derivation at \(p\) is a <strong>linear</strong> operator (at \(p\)):</p> \[D: C^{\infty}_{p}(M) \rightarrow \mathbb{R}\] <p>satisfying the Leibniz rule:</p> \[D(fg) = f(p)D(g) + g(p)D(f), \; \forall f, g \in C^{\infty}(M)\] <p>where \(C^{\infty}_{p}\) is the equivalence class of \((f, U)\), where \(f \in C^{\infty}\) and \(U\) is a neighbourhood of \(p\). Two functions \((f, U)\) and \((g, V)\) are equivalent iff \(\exist \; W \subset U \cap V\) a neighbourhood of \(p\) such that \(f(x) = g(x), \forall x \in W\), where \(g \in C^{\infty}\) and \(V\) is a neighbourhood of \(p\).</p> <p>Intuitively, \(D\) is like a directional derivative operator acting on smooth functions near \(p\). Consequently,</p> \[T_p{M} = \{ D \mid D \; \mathrm{is\ a\ derivation\ at} p \}\] <p>and a tangent vector \(v \in T_p{M}\) is a derivation \(D\). In fact, Tu (Theorem 2.2, An introduction to manifolds) showed that there‚Äôs a bijection between derivations at \(p\) and directional derivaties.</p> <h3 id="3-tangent-vectors-as-equivalences-on-function-germs">[3] Tangent vectors as equivalences on function germs</h3> <h3 id="local-coordinate-description">Local coordinate description</h3> <p>Specifically if we adopt interpretation [2], then with \((U, \phi)\) a chart with coordinates \((x^1, \dots, x^n)\) near \(p\)</p> \[\{ \frac{\partial}{\partial x^1}\Big|_p, \dots, \frac{\partial}{\partial x^n}\Big|_p \}\] <p>form a basis of \(T_pM\).</p> <p>Thus any tangent vector \(v\) can be written uniquely as</p> \[v = \sum_{i = 1}^{n} v^i \frac{\partial}{\partial x^i}\Big|_p\] <p>where \((v^1, \dots, v^n)\) are the components of the vector in this coordinate system.</p> <h2 id="2-equivalence-between-derivations-and-curves">2] Equivalence between derivations and curves</h2> <p>Perhaps not so surprisingly, the above two definitions are compatible and even equivalent to one another.</p> <p>From curves to derivations:</p> <h2 id="3-a-concrete-computational-example">3] A concrete computational example</h2> <p>A coherent walkthrough using the sphere \(S^2\)</p> <p>Let‚Äôs re-emphasize again that a vector field on a Manifold is defined as the following:</p> <p>Let \(M\) be a smooth manifold. A <strong>vector field</strong> on \(M\) is a smooth section/assignment \(p \mapsto X_p \in T_p M\) where \(T_p M\) is the tangent space at \(p\).</p> <p><strong>Fundamental viewpoint</strong>:</p> <blockquote> <p>A tangent vector is a <strong>derivation</strong>: a linear map \(X_p : C^\infty(M) \to \mathbb{R}\) satisfying the Leibniz rule.</p> </blockquote> <p>We‚Äôll also give the local coordinates and coordinate vector fields as the following:</p> <p>Let \((U, \varphi), \quad \varphi : U \subset M \to V \subset \mathbb{R}^n\) be a coordinate chart, with coordinates \((x^1, \dots, x^n).\)</p> <p>Each coordinate \(x^i\) is itself a <strong>function on the manifold</strong>: \(x^i : U \to \mathbb{R}.\)</p> <p>Definition of the ‚ÄúCoordinate Vector Field‚Äù:</p> <p>The coordinate vector field \(\left.\frac{\partial}{\partial x^i}\right|_p\) is defined <strong>intrinsically</strong> by its action on smooth functions: \(\boxed{ \left.\frac{\partial}{\partial x^i}\right|_p(f) := \frac{\partial}{\partial x^i} \big(f \circ \varphi^{-1}\big) \Big(\varphi(p)\Big) }\) where \(x^i\) is the \(i\)-th coordinate on \(\mathbb{R}^n\).</p> <p>Notice that there‚Äôs an abuse of notation (the left ‚Äúpartial‚Äù is what we define, whereas the right partial is the usual partial differentiation). Hopefully, it‚Äôs obvious that the above definition stems from the following basic definition of graph differential (which I will talk more into next time):</p> \[\boxed{ (dF_p(v))(f) := v_p(f \circ F) }\] <p>More importantly, this definition:</p> <ul> <li>uses <strong>only</strong> the chart,</li> <li>does <strong>not</strong> require an embedding and thus does $NOT$ give us components in $\mathbb{R}^n$</li> <li>explains what ‚Äú\(\partial f / \partial x^i\)‚Äù actually means: It means the ordinary partial derivative of the coordinate expression of $f$ after pulling $f$ back to a coordinate chart.Note that nothing is being differentiated on $\mathbb{R}^n$ unless we explicitly choose an embedding (see below).</li> </ul> <hr/> <p>Now, let‚Äôs take a look at the simple example: Sphere \(S^2\) and spherical coordinates. Given the manifold \(S^2 = \{(x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}\)</p> <p>Coordinate chart (away from poles)</p> <p>\((\theta, \varphi)\) with embedding map \(F(\theta,\varphi) = (\sin\varphi\cos\theta,\; \sin\varphi\sin\theta,\; \cos\varphi).\)</p> <p>Here:</p> <ul> <li>\(\theta, \varphi\) are <strong>functions on \(S^2\)</strong>,</li> <li>not abstract variables.</li> </ul> <p>According to the above discussions, the <strong>intrinsic</strong> meaning of \(\partial / \partial \theta\) and \(\partial / \partial \varphi\) are the following:</p> <p>For any \(f : S^2 \to \mathbb{R}\), \(\frac{\partial}{\partial \theta}(f) = \frac{\partial}{\partial \theta} \big(f \circ \varphi^{-1}\big)(\theta,\varphi), \quad \frac{\partial}{\partial \varphi}(f) = \frac{\partial}{\partial \varphi} \big(f \circ \varphi^{-1}\big).\)</p> <p>Again, this is the <strong>definition</strong>, not an interpretation. This offers us one way to calculate the vector field/tangent vectors at $\forall p \in M$.</p> <p>If we set</p> <p>\(f(x, y, z) = z\), then we could pull it back and obtain \((f \circ \phi^{-1})(\theta, \phi) = \cos(\phi)\). Consequently,</p> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X_p(f)= \frac{\partial \cos(\phi)}{\partial \theta} = 0\] <p>at $p = (\theta, \phi)$.</p> <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y_p(f)= \frac{\partial \cos(\phi)}{\partial \phi} = -\sin(\phi)\] <p>at $p = (\theta, \phi)$.</p> <hr/> <p>However, because \(S^2 \subset \mathbb{R}^3\), we may compute concrete representatives by viewing it as an embedded in $\mathbb{R}^3$ and to extrinsically compute the vector fields by pushforward of coordinate basis vectors via the embedding map of the manifold. Consequently, such computation lives in the embedded picture, not the intrinsic definition.</p> \[\frac{\partial}{\partial \theta} = \frac{\partial F}{\partial \theta} = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0)\] \[\frac{\partial}{\partial \varphi} = \frac{\partial F}{\partial \varphi}= (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi)\] <p>These are <strong>actual vectors in \(\mathbb{R}^3\)</strong> tangent to \(S^2\).</p> <blockquote> <p>Important:<br/> This is <strong>not the definition</strong> of coordinate vector fields ‚Äî<br/> it is a <strong>representation</strong> using the embedding.</p> </blockquote> <p>Consequently, we could act on functions without pulling back to coordinate space:</p> <p>Let \(f(p) = z(p)\) be a function on \(S^2\).</p> <p>Choose an extension \(\tilde f(x,y,z) = z \quad\Rightarrow\quad \nabla \tilde f = (0,0,1).\)</p> <p>Here‚Äôs the key observation:</p> <blockquote> <p>Once a tangent vector is represented in \(\mathbb{R}^3\), its action on $f$ is given by the directional derivative of an extension of $f$.</p> </blockquote> <p>Formally, \(\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\) where</p> <ul> <li>$\tilde{f}$ is any smooth extension of $f$ to $\mathbb{R}^3$</li> <li>$X_p \in T_pS^2 \subset \mathbb{R}^3$.</li> </ul> <p>This works because tangent vectors annihilate normal components.</p> <hr/> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X(f)= (0,0,1) \cdot (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0) = 0\] <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y(f)= (0,0,1) \cdot (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi) = -\sin\varphi\] <p>Same results as the intrinsic definition.</p> <p>Note that to be very concrete, in the above examples, at $p = (\theta, \phi)$, the tangent vector is:</p> <p>$X_p = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\;0)$ (we could interpret this as horizontal circles of latitude) and $Y_p = (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\ -\sin\varphi)$. This should make it very clear that we are treating the tangent vectors as actually ‚Äúvectors‚Äù living in $\mathbb{R}^3$ like how we usually refer them to be. With this extrinsic embedding, the general form of a vector field coincides with the intrinsic notion, but more tangible:</p> <p>$X = a(\theta, \phi)\frac{\partial}{\partial \theta} + b(\theta, \phi)\frac{\partial}{\partial \phi}$,</p> <p>then as a vector in $\mathbb{R}^3$: $X_p = a \frac{\partial F}{\partial \theta} + b\frac{\partial F}{\partial \phi}$, and</p> <p>$X_p(f) = \nabla \tilde{f}(p) \cdot X_p$</p> <p>Notice that for this computation no coordinate pullback is required. But to be crystal clear, we do <strong>not</strong> compute the vector field ‚Äúwithout coordinates‚Äù. We simply You replaced intrinsic coordinates on $\mathbb{S}^2$ by ambient Cartesian coordinates in $\mathbb{R}^3$. So coordinates are still there ‚Äî just in a different space.</p> <hr/> <p>Before we continue, let‚Äôs dwell on this formula for a while:</p> \[\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\] <p>This identity guarantees consistency between 1] the derivation definition and 2] the embedded vector representation (Question: is this just chain rule?s). It explains why the extrinsic calculation agrees with the intrinsic definition. If we have a nice embedding, two views are nice. However, if we go to the general abstract case without the ambient space, we could only assort to the intrinsic definition.</p> <hr/> <p>To be rigorous, you may be concerned about the specific use of the function extension here. However, we could show that extensions do not matter, because <strong>‚Äútangent vectors annihilate normal components‚Äù</strong>. Let me explain below:</p> <p>If \(g : \mathbb{R}^3 \to \mathbb{R}\) satisfies \(g|_{S^2} = 0,\) then for all \(v \in T_p S^2\), \(v(g) = 0.\) Equivalently, \(\nabla g(p) \perp T_p\mathbb{S}^2\)</p> <hr/> <h3 id="proof-via-curves">Proof (via curves):</h3> <p>Let \(\gamma(t) \subset S^2\) with \(\gamma(0)=p, \quad \gamma'(0)=v.\)</p> <p>Since \(g(\gamma(t)) = 0\) for all \(t\), \(v(g) = \frac{d}{dt} g(\gamma(t))\big|_{t=0} = 0.\)</p> <hr/> <h3 id="concrete-example">Concrete example</h3> <p>Let \(g(x,y,z) = x^2 + y^2 + z^2 - 1.\)</p> <p>Then: \(\nabla g = (2x,2y,2z)\) which is normal to \(S^2\).</p> <p>For any tangent vector \(v\), \(v(g) = \nabla g \cdot v = 0.\)</p> <hr/> <h3 id="consequence-crucial">Consequence (crucial)</h3> <p>If \(\tilde f_1\) and \(\tilde f_2\) are two extensions of \(f\), then \(g = \tilde{f_1} - \tilde{f_2}\) vanishes on $\mathbb{S}^2$. For any tangent vector $v$:</p> \[v(\tilde f_1) - v(\tilde f_2) = v(\tilde g) = 0\] <p>thus:</p> \[v(\tilde f_1) = v(\tilde f_2) \quad\forall v \in T_p S^2.\] <p>Hence directional derivatives along tangent vectors are <strong>well-defined</strong>: this is why directional derivatives along tangent vectors do not depend on how f extends off the manifold.</p> <p>View from another perspective, this indicates that a function that is identically zero on the manifold cannot change when we move tangentially: so tangent vectors ‚Äúdon‚Äôt see‚Äù normal variations. What I mean by the former statement ‚ÄúTangent vectors annihilate normal components‚Äù is that derivatives along tangent directions ignore any part of a function that only varies off the manifold, because those variations are invisible to curves that stay on the manifold.</p> <hr/> <h3 id="intrinsic-vs-extrinsic-viewpoints-summary">Intrinsic vs Extrinsic Viewpoints (Summary)</h3> <table> <thead> <tr> <th>Aspect</th> <th>Intrinsic</th> <th>Extrinsic</th> </tr> </thead> <tbody> <tr> <td>Tangent vector</td> <td>Derivation</td> <td>Vector in \(\mathbb{R}^3\)</td> </tr> <tr> <td>Definition</td> <td>Via charts</td> <td>Via embedding</td> </tr> <tr> <td>Computation</td> <td>Pullback</td> <td>Directional derivative</td> </tr> <tr> <td>Dependence</td> <td>Coordinate-dependent</td> <td>Embedding-dependent</td> </tr> </tbody> </table> <p>Both viewpoints are <strong>equivalent</strong> when an embedding exists.</p> <hr/> <p>Something to note from the above example:</p> <blockquote> <p>Coordinate vector fields are <strong>defined intrinsically as derivations</strong>,<br/> may be <strong>represented extrinsically</strong> using embeddings,<br/> and act on functions in a way that is <strong>independent of extensions</strong> because tangent vectors annihilate normal components.</p> </blockquote> <p>Through this simple example we already develop a taste of the difference between intrinsic and extrinsic geometry, the abstract definitions and concrete calculations. Such theme will be recurring for our journey in studying differential geometry.</p> <p>One of the advantages of the intrinsic view is that it is independent of coordinate charts. It paves the ground for explaining why</p> <blockquote> <p><strong>Gradients, geodesics, and Lie brackets make sense without embedding (or ever mentioning) the abstract manifold into $\mathbb{R}^n$</strong>.</p> </blockquote> <h2 id="discussions">Discussions</h2> <p>Firstly, note that tangent/cotangent space is an intrinsic? property. A usual image of interpreting tangent space is \(S^2\) embedded in \(\mathbb{R}^3\). Our definition above does not require so, and in fact the shift of perspective from extrinsic into intrinsic properties of geometric objects is a grand evolution starting from Gauss and Rieman.</p> <p>[TODO: An illustrative figure of a sphere embedded in Euclidean space]</p> <p>From derivations to curves:</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Intuition and definition of tangent/cotangent space]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)"/><published>2025-09-17T17:49:33+00:00</published><updated>2025-09-17T17:49:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Today I will cover a beautiful subject in differenetial geometry: Ricci flows and Ricci curvature.</p> <p>Generally, in Riemannian geometry, curvature measures how space bends. For instance, on a sphere, geodesics (shortest paths) come closer together compared to flat space; on a hyperbolic surface, they diverge.</p> <p>Ricci curvature is a particular way of summarizing curvature: Instead of describing how all directions bend (that‚Äôs what the full <strong>Riemann curvature tensor</strong> does), Ricci curvature focuses on <strong>volume distortion</strong>. More concretely, it tells us how the volume of a small geodesic ball deviates from the volume we‚Äôd expect in flat Euclidean space.</p> <p>Intuitively, we could summarize into the following:</p> <blockquote> <p>Positive Ricci curvature (like on a sphere) means geodesics tend to converge, and small balls have less volume than in flat space.</p> <p>Zero Ricci curvature (like in Euclidean space) means geodesics neither converge nor diverge, so volumes match Euclidean.</p> <p>Negative Ricci curvature (like on a hyperbolic space) means geodesics diverge, so small balls have more volume than Euclidean.</p> </blockquote> <p>Mathematically, Ricci curvature is obtained by ‚Äútracing‚Äù the <strong>Riemann curvature tensor</strong>. It compresses information about how different directions curve into a symmetric 2-tensor <code class="language-plaintext highlighter-rouge">Ric</code>.</p> <h3 id="riemannian-geometry-and-tensor">Riemannian Geometry and Tensor</h3> <p>The Riemann tensor is written in the following way:</p> \[R(X, Y)Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X, Y]} Z\] <p>This tensor captures all information about curvature. Succinctly, this is a 4-tensor: \(R_{ijkl}\).</p> <p>The above is an unfair treatment of Riemannian geometry. I‚Äôll have a separate blog on that subject soon.</p> <p>How to understand: Riemannian metric tensor informs the manifold where to expand, shrink, and curve. How does Riemannian metric tensor relate with curvature?</p> <h3 id="ricci-curvature">Ricci Curvature</h3> <p>Based on the Riemann tensor, what is the curvature?</p> <p>To get Ricci curvature, we take a <strong>trace</strong> of the Riemann tensor:</p> \[Ric_{ij} = R^{k}_{ikj} = g^{kl}R_{kilj}\] <p>This reduces the information down to a 2-tensor (like the metric itself). Geometrically, this represents the volume distortion of geodesic balls.</p> <h3 id="ricci-flow">Ricci Flow</h3> <p>Introduced by <code class="language-plaintext highlighter-rouge">Richard Hamilton (1982)</code>, Ricci flow is a process that evolves a Riemannian metric \(g(t)\) over time:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>The factor -2 is just convention (to simplify later computations). We could think of this as a heat equation for geometry: Just as heat diffuses to smooth out temperature differences, Ricci flow smooths out <strong>irregularities</strong> in curvature. As illustrated in the above section of Ricci curvature, we could naturally arrive at the result that positive curvature regions tend to shrink and negative curvature regions expand. Over time, the underlying geometry becomes more ‚Äúregular‚Äù, like ironing out wrinkles.</p> <p>The effect of Ricci flow on curvature could be expressed in the following way. The derivative of scalar curvature \(R\) under this flow is:</p> \[\frac{\partial R}{\partial t} = \Delta R + 2 |Ric|^2\] <p>This resembles a heat equation $(\Delta R)$ plus a positive correction. Consequently, curvature tends to diffuse out but also grows in positive-curvature regions.</p> <h4 id="examples-of-ricci-flow">Examples of Ricci Flow</h4> <p>Perhaps the simplest example is to imagine how a sphere evolves under Ricci flow. We all know that a round sphere has positive Ricci curvature. The Ricci flow equation says:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>Since Ricci is positive, the metric shrinks, and thus makes the sphere to contract uniformly, eventually collapsing to a point. This closely mirrors the idea that positive curvature makes geodesics converge. Under the Ricci flow, it tightens further.</p> <p>Another simple example is the flat torus. Since the Ricci curvature is 0 everywhere,</p> \[\frac{\partial g_{ij}}{\partial t} = 0\] <p>the torus will stay unchanged after the flow forever. This is analogous to heat diffusion on a perfectly uniform temperature field where nothing would effectively changes.</p> <p>Likewise, a hyperbolic surface has negative Ricci curvature. Consequently, under the Ricci flow, the metric would expand and the hyperbolic surface would grow larger and more uniform in curvature.</p> <p>In a nutshell, irregular geometries with bumps or folds (different curvature in different regions) get ‚Äúsmoothed‚Äù over time. All the high-curvature ‚Äúwrinkles‚Äù would get flatten out, like how heat equalizes temperature.</p> <h3 id="application-of-ricci-flow">Application of Ricci Flow</h3> <p>Poincare conjecture, Ricci flow, surgery theory, what Terrence Tao called ‚Äúone of the most impressive recent achievements of modern mathematics‚Äù</p> <p>Poincare conjecture:</p> <blockquote> <p>Any closed 3-manifold that is simply-connected, compact, and boundless is homeomorphic to a 3-sphere.</p> </blockquote> <p>Specifically, Poincare conjecture in higher dimensions has been solved around 1961, and dimension 4 case has been proved by Michael Freedman who by which won Fields medal in 1986. The \(n = 3\) case seemed really difficult to crack and it was only at 2002 that Grisha Perelman proved it using Ricci flow.</p> <p>Very briefly, since a sphere has positive curvature, by applying Ricci flow through time such sphere will contract and eventually vanish. Perelman proved the opposite also holds: if metric goes to 0, it must have been a sphere. To prove Poincare‚Äôs conjecture using Ricci flow,</p> <p>One of the most triumphant use of Ricci flow happens when Grigori Perelman (2002‚Äì2003) to prove <strong>the Poincar√© conjecture</strong> and the more general <strong>Thurston geometrization conjecture</strong>. He showed how Ricci flow with ‚Äúsurgery‚Äù (cutting and patching when singularities form) classifies 3-manifolds.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Introduction of Ricci flows/curvatures]]></summary></entry><entry><title type="html">The Dance of Space (2): Differential Forms (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/" rel="alternate" type="text/html" title="The Dance of Space (2): Differential Forms (in progress)"/><published>2025-09-16T16:34:22+00:00</published><updated>2025-09-16T16:34:22+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Differential_Forms</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/"><![CDATA[<p>The second episode to appreciate the inner workings of space is through the differential forms. Differential forms is not an ancient subject, ‚Ä¶</p> <h3 id="vector-outer-product">Vector Outer Product</h3> <h3 id="wedge-productexterior-derivative">Wedge Product/Exterior Derivative</h3> <h3 id="three-in-one">Three in One</h3> <p>Green‚Äôs theorem, Gauss‚Äô theorem, Stoke‚Äôs theorem.</p> <p>Green‚Äôs theorem:</p> \[\int_{L} Pdx + Qdy = \iint_{D}(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y})dxdy\] <p>Generalized Stoke‚Äôs theorem.</p> <p>A k-form is supposed to be integrated over an oriented k-dimensional manifold</p> <h3 id="fundamental-theorem-of-calculus-ftoc">Fundamental Theorem of Calculus (FTOC)</h3> <p>High dimensional Stoke‚Äôs theorem is exactly the fundamental theorem of calculus (FTOC) in high-dimensional space.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Intuite and define differential forms]]></summary></entry><entry><title type="html">Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Equivalence/" rel="alternate" type="text/html" title="Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)"/><published>2025-09-07T00:23:16+00:00</published><updated>2025-09-07T00:23:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Equivalence</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Equivalence/"><![CDATA[<p>Explain and compare multiple equivalences from differential geometry and topology, including homeomorphism, diffeomorphism, homotopy equivalence, homomorphism, isomorphism, etc.</p> <p>Invariant and equivariant functions (CNN is equivariant).</p> <p>Also discussions on cardinality among sets (including finite and infinite (countably infinite \(N0\)? and uncountably infinite), Hillbert Hotel problem, equipotent sets)</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Topology"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Compare various equivalences in geometry/topology/group theory]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (1): Rigorous Definition (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(1)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (1): Rigorous Definition (in progress)"/><published>2025-08-29T02:32:56+00:00</published><updated>2025-08-29T02:32:56+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(1)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(1)/"><![CDATA[<h3 id="preface">Preface</h3> <p>Starting with this blog, I‚Äôll gradually introduce the core elements and the corresponding theories of (differentiable/smooth) manifold, which is, without any undue exaggeration or affected self-indulgence, the most fundamental arean upon which modern geometry unfolds (other than thoeries of curves and surfaces in \(\mathbb{R}^3\)).</p> <p>It‚Äôs only after I seriously learned manifold that I started to appreciate its elegant geometric intuition (unintuited indeed or not easy to grasp at first glance sometimes) and its abstraction and extensions of our known, visually perceived \(\mathbb{R}^3\) into higher dimensional space. It grants us infinite power to have a glimpse of what we are not quite possible at all for our entire lives to interpret and still acquire the confidence to draw conclusions on properties, hierarchies, and variations in the spave above us.</p> <p>As the first blog of the entire manifold Omnibus series, I aim to cover the rigorous definition of manifold, and a general introduction to its properties and different categories, to set the basis for further rigorous discussions.</p> <p>I would refrain from diverting to much from the many interesting discussions (but will review together the definition) of topological space, topology, open sets, Hausdorff space, and homeomorphism.</p> <h3 id="history-and-motivation">History and motivation</h3> <p>Motivation of manifolds:</p> <p>Geometrically, ‚Ä¶</p> <p>Algebraically, solution sets for polynomial equations and differnetial equations.</p> <p>Immanuel Kant‚Äôs idea on the word manifold.</p> <h3 id="a-snapshot">A snapshot</h3> <p>The following might touch upon numerous unfamiliar terms and concepts, no worries I will cover them all as time unfolds. For now, let‚Äôs get a little intuition and some impressionistic perception of this subject.</p> <p>What exactly is a manifold? The most basic kind of a manifold, a topological (n-dimensional) manifold \(M\) is a topological space locally homeomorphic to \(\mathbb{R}^n\). Specifically, for each point \(p \in M\) there exists a chart around \(p\), which is essentially a pair \((U, \Phi)\) such that \(U \subset M\) is an open set(in the topology sense) containing \(p\) and \(\Phi\) a homeomorphism from \(U\) to an open subset of \(\mathbb{R}^n\). For some technical details, the definition of manifolds would also add two other conditions: 1] the Hausdorff property and 2] second-countability. Any manifold has to be a topological manifold at least.</p> <p>The main focus of this series will be on smooth manifolds, which add some nice properties other than being topological manifolds. A smooth manifold acquires a collection of charts compatible with each other, such that the composition of coordinate functions is a diffeomorphism (they form what‚Äôs called a smooth structure on the topological manifold). This nicely ensures differentiability and enables many basic calculus on the manifolds to be valid operations.</p> <p>Examples of manifold include both objects we know and some more abstractly constructed entities: \(\mathbb{R}^n\), any onpen subsets of a manifold, the product space \(M \times N\) (if \(M\) and \(N\) are manifolds).</p> <p>Another famous family of manifolds comes from compact connected 2-manifolds, specifically closed surface with genus \(g &gt; 0\): for example, the familiar \(S^n\) (\(g = 0\)) and the torus \(S^1 \times S^1\) (\(g = 1\)).</p> <p>If we specify our manifolds to be <code class="language-plaintext highlighter-rouge">non-orientable</code>, we would obtain some even more famous/esoteric objects like the (open) <strong>Mobius strip</strong> (open Mobius strip is non-compact, since the boundary is not included) and <strong>Klein bottle</strong> \(K = I \times I / \sim\). From the pure topology standpoint, they are actually quite basic topological spaces. Needless to mention the famous theorem about <strong>classification of surfaces</strong> which states that every compact connected 2-manifold appears as \(S^2\) glued with tori or Mobius stirps (up to homeomorphism). There‚Äôs a little subtlety here in that there do exist ‚Äúmanifolds with boundaries‚Äù which are locally homeomorphic to \([ 0 \times \infty) \times \mathbb{R}^{n-1}\) instead of \(\mathbb{R}^n\), like Mobius strips in a usual sense.</p> <p>Other more abstract example exist such as the projective planes (frequently showing up in Topology) \(\mathbb{RP}^n\cong S^n/\sim\) (with the equivalence relation \(v \sim -v\)). Notice that there‚Äôre multiple ways to define quotient spaces for \(\mathbb{RP}^n\). This example is usually utilized to illustrate quotient space. The Grassmanian Manifold is also frequently studied: \(Gr_k (\mathbb{R}^m) = \{ \mathrm{k\ dimensional\ vector\ subspaces\ of} \; \mathbb{R}^m \}\), where \(0 \leq k \leq m\). This is a compact manifold with dimensions \(k(m-k)\) (Interested readers might come up with a guess or proof). Interestingly, \(Gr_1 (\mathbb{R}^m) \cong \mathbb{RP}^{m-1}\).</p> <p>Since manifold is literally the backbone of modern differential geometry which has evolved into countless sub-branches and domains, it‚Äôs impossible to even sample a fair amount in these blogs. Consequently, I‚Äôll be mainly focusing the following:</p> <ul> <li>Definitions of smooth manifolds</li> <li>Submanifolds and embeddings <ul> <li>An embedding is an injective smooth map from \(M\) to \(N\) which is also an isormorphism between \(M\) and a submanifold of \(N\)</li> </ul> </li> <li>Tangent and cotangent spaces <ul> <li> \[p \in M, dim(M) = n, T_p N \cong \mathbb{R}^n\] </li> <li>The differential of smooth maps between manifolds \(M, N\) is a <strong>linear</strong> map from \(T_p M\) to \(T_{f(p)}N\)</li> <li>The cotangent space is the dual vector space of \(T_p M\), containing elements as \(d_p f \in T_p^{*}M\), where \(f: M \rightarrow \mathbb{R}\) is smooth</li> </ul> </li> <li>Tangent and cotangent bundles <ul> <li>\(\pi: TM (\mathrm{2n\ dimensional}) \rightarrow M\), where \(\pi^{-1}(\{ p\}) = T_p M\)</li> <li>Example of vector bundle</li> <li>Similarly a cotangent bundle \(Y: T^{*}M \rightarrow M\)</li> </ul> </li> <li>Tangent and cotangent fields <ul> <li>A <strong>section</strong> of the tangent bundle \(X: M \rightarrow TM\) is a tangent field (or tangent vector field, or vector field on \(M\)): \(X(p) \in T_pM\)</li> <li>\(X\) as a vector field, \(X(p) \in T_pM, \forall p \in M\), tracing out a flow on \(M\)</li> <li>This is framed in the language of ODE and flows</li> <li>A <strong>section</strong> of the cotangent bundle is a cotangent field (or 1-forms, or differentials)</li> <li>Cotangent fields are differential 1-forms, if \(f: M \rightarrow \mathbb{R}\) is smooth, \(p \in M\), then \(d_pf \in T_p^{*}M\)</li> <li>Frobenius integrability theorem</li> </ul> </li> <li>Differnetial \(k\)-forms on manifold \(M\) <ul> <li>Tensor bundles and tensor fields</li> <li>A section of the \(k\)th exterior power of the cotangent bundle \(T^{*}M\) is a differential \(k\)-form</li> <li>0-forms: functions on \(M\)</li> <li>1-forms: cotangent fields</li> <li>k-forms: sections of the \(k\)th exterior power of \(T^{*}N\)</li> <li>de Rham derivative and de Rham cohomology groups: \(d: \Omega^{k}M \rightarrow \Omega^{k+1}M\)</li> <li>For \(M\) compact and orientable, \(\omega \in \Omega^n M\), \(\int_{M} \omega \in \mathbb{R}\)</li> </ul> </li> <li>General Stokes‚Äôs Theorem <ul> <li>If M is compact, oriented, with boundary, n-dimensional and \(\omega \in \Omega^{n-1}M\), then \(\int_{M}d\omega = \int_{\partial M} \omega\)</li> </ul> </li> <li>Lie group and Lie algebra <ul> <li>Manifold \(+\) group at the same time</li> </ul> </li> </ul> <h3 id="something-dark-or-darker">Something dark, or darker</h3> <p>One last note to keep before we starting diving in, especially for readers from computational/systems neuroscience who probably have heard of the concept <code class="language-plaintext highlighter-rouge">neural manifold</code>. For the past few decades, as the technologies of simultaneous recordings of multiple neurons quickly evolve, data recorded as an entire neural population has gradually become a solid reality for many subdomains of neuroscience, and the level of data analysis promptly and adaptively shifts from the single-neuron perspective to neural population analysis. Many theories have been developed to cope with such high dimensional data (could be as many as several hundreds), like the dynamical system perspective as <a href="/blog/2025/jPCA/, /blog/2025/Constraint-Learning/">my previous two blogs covered</a>, or neural manifold. This is really not a novel idea, as in many domains there‚Äôs abundance of observation that among the high dimensional data there exists some low dimensional structure that captures the <code class="language-plaintext highlighter-rouge">patterns</code> of data.</p> <p>However, in most cases there‚Äôs virtually nothing lost if we simply swap the word ‚Äúneural manifold‚Äù into, say, ‚Äú<strong>low dimensional structure</strong>‚Äù. What worries me is that at least up to now there is no theoretical proof that a collected neural population is indeed a (at least topological) manifold. Also, from an ontological perspective, because other than employing this word itself, there‚Äôs very few rigorous adaptation or usage of the many theorems and properties associated to a manifold, the true essence behind the entire domain of manifold theory is left untouched at all. Indeed, we might argue that at a figurative level such neural manifold is a nice symbol for building up intuition, but there‚Äôs still a conceptual subtlety that the very first motivation of a manifold is to extend the familiar Euclidean space around us to higher dimensional spaces. In other words, the charm of manifolds shines in the high dimensional space, which, without further notice, might comletely mislead people into linking ‚Äúmanifold‚Äù with ‚Äúlow dimesional space‚Äù. To be fair, in computational/systems neurscience, even when ‚Äúneural manifold‚Äù is used to indicate ‚Äúlow dimensional space‚Äù, it may still be 10 or 30, which for mathematicians are usually ‚Äúhigh‚Äù (high vs low is a relative concept. I don‚Äôt think there‚Äôs a disagreement with regard to the aboslute ‚Äúhigh‚Äù). For neuroscientists, ‚Äúlow dimension‚Äù is low because of its direct comparison to the original space in \(\mathbb{R}^{384}\), for example; for a geometer, both \(\mathbb{R}^{10}\) and \(\mathbb{R}^{384}\) are ‚Äúhigh dimensional space‚Äù because they extend beyond \(\mathbb{R}^{3}\). Finally, manifold not only extends \(\mathbb{R}^n\) to \(\mathbb{R}^N\), where \(n &gt;&gt; N\), but more importantly entirely discards the many constraints of Euclidean space and works with more general spaces (indeed, as in the shift of focus of intrinsic vs extrinsic properties).</p> <p>I never doubt that some low dimensional structure is imbedded in neural population, as countless research has corroborated it. And I deeply appreciate the remarkable wisdom and significant efforts others have paid along this line of research under the name of neural manifold. In the end, I‚Äôm just hoping concepts to stay clear and to minimize the downplay of any important concepts that are both aesthetically appearling and fundamentally stunning. I‚Äôm reluctant to observe those ideas get muddled, mumbled, and straggled among hypes and later turned into buzzwords that, at the down-to-earth applied level, mistakes its potential and hides the key questions regarding how to adapt the abstract manifold concepts into codable algorithms, or at the abstract level noisify our perception of the world (as many other instances already in an awkard situation, like ‚Äúcomputation‚Äù or ‚Äúrepresentation‚Äù).</p> <p>Perhaps I‚Äôm being a little over-reactive?</p> <p>Or‚Ä¶perhaps not so surprising, in recent years among Deep Learning there‚Äôs more and more serious consideration and introduction of embedding geometry/topology into the modeling pipelines (geometric deel learning, interested readers might want to check out Michael Bronstein). I highly enjoy the geometric approaches whether in DL or neuroscience, and I‚Äôm very glad that I live in this era when I could witness the entire trajectory of geometry involvement. Meanwhile, I‚Äôll do my best to clarify things and introduce more into the pure beauty of geometry. Enough sentiment, now let‚Äôs jump in.</p> <h3 id="a-bit-of-topology">A bit of topology</h3> <p>Topological sapce:</p> <p>Homeomorphism:</p> <h3 id="manifold-entry-point-topological-manifold">Manifold entry point: Topological manifold</h3> <p>Definition of a topological manifold:</p> <blockquote> <p>A topological space \(M\) is an \(n\)-dimensional topological manifold if</p> <p>1] \(\forall p \in M\), there exists an open neighborhoood \(U\) (\(p \in U \subset M\)) such that there exists a homeomorphism between \(U\) and an open subset of \(\mathbb{R}^n\)</p> <p>2] \(M\) is Hausdorff</p> <p>3] \(M\) is second-countable</p> </blockquote> <p>The first requirement is the frequently cited notion that a manifold is <em>locally like</em> a Euclidean space, but rigorously defined in terms of topological equivalence. The third condition about second-countability indicates that there exists a countable set of topolgy bases. This is required for guaranteeing the existence of the partition of unity on \(M\) subordinate to any open cover.</p> <p>Conditions 1] and 3] could be combined into one statement:</p> <blockquote> <p>\(M\) is the union of finite or countably infinite open subsets \(U \subset M\) which is homeomorphic to some open subsets of \(\mathbb{R}^n\)</p> </blockquote> <p>Whenever a manifold is mentioned without any prefix, it‚Äôs a topological manifold. More conditions could be specified to add more ‚Äúflavors‚Äù to the manifold: like if we prescribe a metric to the manifold to obtain a Riemannian manifold. But without adding more complicated structure, we will only ask for something simpler to include: the differentiability, which leads to the main subject of this series of blogs: differentiable/smooth manifolds. But before going there, let‚Äôs take a look at few examples that are not manifolds (to also deepen our interpretation of the definition):</p> <h3 id="what-is-not-a-manifold">What is not a manifold?</h3> <p>1] An ‚Äú8‚Äù-shaped structure is not a manifold, since near the center it‚Äôs not homeomorphic to any Euclidean space.</p> <p>2] A long line constructed by the first uncountable ordinal number (this violates the countability criterion)</p> <p>3] A canonical example that fails the Hausdorff condition is a line with two origins: \(X = \mathbb{R}_1 \sqcup \mathbb{R}_2 / \sim\) where \(x \sim y\) if and only if \(x = y \neq 0\) (so the two 0s \(0_1 \in \mathbb{R}_1, 0_2 \in \mathbb{R}_2\) are still distinct). These two 0s cannot be distinguished because they share the same neighbors all the time. Around the origin we cannot tell from the neighbourhood if it‚Äôs \(0_1\) or \(0_2\). The topology forces neighbourhoods to overlap too much such that the Hausdorff property fails. This example could also be extended into 2D versions.</p> <p>Another slightly different counterexample for Hausdorffness is not on separating some point, but glueing spaces together: Consider two copies of Euclidean plane: \(\mathbb{R}^2_1, \mathbb{R}^2_2\), and glue them on the right half plane</p> \[H = \{(x, y) \in \mathbb{R}^2 | x \geq 0 \}\] <p>Equivalently, \(X = \mathbb{R}^2_1 \sqcup \mathbb{R}^2_2 / \sim\), where \((x_1, y_1) \sim (x_2, y_2)\) iff \(x \geq 0\). Obviously, \(X\) is not Hausdorff since there‚Äôs not disjoint neighbourhood around \((0, 0)_1\) and \((0, 0)_2\).</p> <h3 id="collection-of-descriptors-charts-and-atlas">Collection of descriptors: Charts and atlas</h3> <p>The above definition emphasizes the essence of manifold that we could only approach it locally. To derive a global view of a maifold is enabled by taking many local ‚Äúcameras‚Äù and projections together. Consequently, we naturally arrive at the following definition:</p> <blockquote> <p>Def: An n-dimensional chart for \(M\) is a pair \((U, \Phi)\) such that \(U \subset M\) is an open sutset, \(\Phi: U \rightarrow \Phi(U)\) a homoemorphism from \(U\) to an open subset of \(\mathbb{R}^n\).</p> <p>Def: A chart for \(M\) is a collection \(\{U_{\alpha}, \Phi_{\alpha} \}_{\alpha \in A}\) such that \(\bigcup_{\alpha \in A} U_{\alpha} = M\)</p> </blockquote> <p>With the concept of chart and atlas, we now are fully capable of pulling the manifold into different Euclidean frames to analyze them, since a chart \((U, \Phi)\) determines real-valued functions \(x_1, x_2, \dots x_n\) such that \(\Phi(p) = (x_1(p), x_2(p), \dots, x_n(p))\) where \(p \in U \subset M\) and \(x_i: U \rightarrow \mathbb{R}^n\). These \(x_i\) are called the coordinate functions of the selected chart.</p> <p>Some readers might have a concern now. If there exist some charts \((U, \Phi)\) and \((V, \Psi)\) where \(U \cap V \neq \varnothing\), then \(\Phi(U \cap V)\) and \(\Psi(U \cap V)\) will have different coordinates. Would this cause a mess?</p> <h3 id="the-real-arena-smooth-manifold">The real arena: Smooth manifold</h3> <p>Given \(f: u \subset \mathbb{R}^n \rightarrow \mathbb{R}\), where \(u\) is an open set, \(f\) is \(C^k\) if the \(k\)th derivative \(\frac{\partial^k f}{\partial x_j^k}\) exists and is continuous \(\forall 1 \leq j \leq n\). Likewise, for a multivariable function \(f: u \subset \mathbb{R}^n \rightarrow \mathbb{R}^m\), \(f = (f_1, f_2, \dots f_m)\) is \(C^k\) if \(f_i\) is \(C^k \; \forall i\). If we take \(k\) to be \(\infty\), then \(f\) is called a smooth. In my blogs, whenever any function is called differentiable without specifying the \(k\)th order, we will assume \(k\) as \(\infty\) and thus a differentiable function is the same as a smooth function. A differentiable manifold is the same as the smooth manifold, both I‚Äôll use interchangeably.</p> <p>There‚Äôs a another way to define smooth of functions in a coordinate-free manner (with linear approximation other than calculating derivatives (See Extra Notes # 2.))</p> <p>Another important concept is called diffeomorphism. Diffeomorphism in \(\mathbb{R}^n\) goes like this: suppose \(\Phi: u \in \mathbb{R}^n \rightarrow v \in \mathbb{R}^n\)</p> <p>\(u, v\) both being open subsets. \(\Phi\) is a diffeomorphism if \(\Phi\) is smooth, \(\Phi\) exists, and \(\Phi^{-1}\) is also smooth. In that case, if \(y = \Phi(x)\), then the matrix \((\frac{\partial y_i}{\partial x_j})\) is invertible. If a function is a diffeomophism, then does it have to be a homeomorphism?</p> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>The point of adding smooth structure to a topological manifold: this enables us to characterize a real-valued function \(f\) on \(M\) to be smooth or not ‚Äî- for a chart \((U, \Phi)\), \(f\) is smooth when it composed with the inverse map \(\Phi^{-1}: \Phi(U) \rightarrow U\): \(f \circ \Phi^{-1}: \Phi(U) \subset \mathbb{R}^n \rightarrow \mathbb{R}\) is smooth (the composed function \(f \circ \Phi^{-1}\) maps from Euclidean space to Euclidean space so we could solicit the familiar definitions of smoothness/differentiability). Careful readers might have spotted a subtle issue: the above definition of smoothness (the composed function) hinges upon the specific selection of chart. So we are led to use only some charts, not all charts.</p> <h3 id="a-few-others">A few others?</h3>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Rigorous definition of manifold and related topology concepts]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble (1): Exponential map (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Exponential-map/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble (1): Exponential map (in progress)"/><published>2025-08-29T02:25:33+00:00</published><updated>2025-08-29T02:25:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Exponential-map</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Exponential-map/"><![CDATA[<p>Exponential map as dynamical flow in differential geometry and dynamical systems.</p> <h1 id="differential-geoetry-lie-group">Differential geoetry (Lie group)</h1> <h1 id="dynamical-system-linear">Dynamical system (Linear)</h1>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Exponential maps applied in Lie group & dynamical systems]]></summary></entry><entry><title type="html">Learning and Constraints (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Constraint-Learning/" rel="alternate" type="text/html" title="Learning and Constraints (in progress)"/><published>2025-08-25T22:09:34+00:00</published><updated>2025-08-25T22:09:34+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Constraint-Learning</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Constraint-Learning/"><![CDATA[<p>This blog covers two papers which focuses on exploring constraints during learning in monkey‚Äôs behavior from different perspectives. The first one <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a> approaches the constraint as a spatial restriction of neural activities residing upon some low dimensional neural manifold, while the second <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a> deliberately perturbs the motion sequence, the temporal order movement to explore whether the monkeys are capabable of adapting neural dynamics. In terms of methods, these two papers share much in common in terms of latent space calculation, and in order not to make this blog unbearably long, I will elaborate more on the first paper‚Äôs algorithms, while glossing over many details in the second. In total, through juxtaposing these studies, hopefully we could glean some integrated thoughts on the constraints of neural dynamics form both the spatial and temporal perspectives.</p> <p>Both spatial (neural manifold view) and temporal (dynamical system view) on constraints of neural activities.</p> <h1 id="spatial-constraints-sadtler-etal-2014">Spatial constraints (Sadtler et.al. 2014)</h1> <h2 id="experiment-setup">Experiment setup</h2> <p>For <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>, two male Rhesus macaques were trained to perform closed-loop BCI cursor task (Radial 8). Around 85-91 neural units (threshold-crossings) were recorded. The experiment pipeline is demonstrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1a.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 1a in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. Note that on the right hand side the authors already presented where to place the two kinds of perturbation. The color scheme (green, yellow, and black) is consistent throughout figures in this paper. </div> <h2 id="decoding-paradigm">Decoding paradigm</h2> <h3 id="dimensionality-reduction-technique">Dimensionality reduction technique</h3> <p>The control space is just 2D because the decoder output is cursor velocities in \(\mathbb{R}^2\), illustrated as black line in Fig.2 black line (Note: it‚Äôs actually a 2D plane, but here for simplicity shown as a black line (\(\mathbb{R}^1\))).</p> <p>They used Factor Analysis ({cite (Factor-analysis methods for higher-performance neural prostheses) }{cite (Gaussian Process Factor analysis)}; I‚Äôll write a blog on GPFA later) to extract what they called the ‚Äúintrinsic manifold‚Äù, which captures the co-modulation patterns among the recorded neural population. This is shown as the underlying yellow plane in Fig.2 (might be confusing, but it‚Äôs not the 2D control space). Note that at the time of publication, neural manifold was not yet in a popular trend, so the authors briefly characterized the term ‚Äúintrinsic manifold‚Äù with the following illustration:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1b.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 1b in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. . </div> <p>The authors further elaborated on the intrinsic manifold and its associated dimensionality at the end of the paper. For consistency of comparisons, Sadtler et. al. used a <strong>linear</strong> 10-D intrinsic manifold across all days. They then performed some offline analysis to explore if 10 is a legitimate choice. Specifically, they estimated the intrinsic dimensionality (EID) as the peak of maximal cross-validated log-likelihood (LL). In panel a the vertical bars represent the standard error of LL from across 4 cross-validation folds. Panel b. shows EID for all days and both 2 monkeys. The authors also showed the LL difference between 10D manifold vs manifold with EID (panel c., with units being the the number of standard errors of LL for the EID model). From panel c. the authors observed that 89% of days the 10-D manifold only differs within one standard error of LL with the EID manifold. Panel d. indicates the cumulative explained variance by the 10-D manifold. Notice that 10 dimensions already explained almost all neural variance.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_4.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.4 in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <p>The factor analysis method works in the following way (I‚Äôll keep the same notation as the paper). Let‚Äôs assume the high dimensional neural signal (here the z-scored spike counts) acquired every 45ms time bin is denoted as \(u \in \mathbb{R}^{q \times 1}\) (naturally, \(q\) neural units), and \(z \in \mathbb{R}^{10}\) the latent variable. Factor analysis assumes the observed neural activity is related to the unobservable latent variables under a Gaussian distribution:</p> \[\begin{equation} u \mid z \sim N(\Lambda z + \mu, \psi) \end{equation}\] <p>where the latent vector is assumed to come from</p> \[\begin{equation} z \sim N(0, I) \end{equation}\] <p>Here the covariance matrix \(\psi\) is diagonal. Consequently, the intrinsic manifold is defined on the span of the columns of \(\Lambda\), and each column of \(\Lambda\) represents a latent dimension where \(z\) encodes the corresponding projections/coordinates. All three parameters \(\Lambda, \mu, \psi\) are estimated from <strong>Expectation-Maximization (EM)</strong> method (I‚Äôll also write a blog about this later, especially how it as a classical inferenc engine is closely related to Evidence Lower Bound (<strong>ELBO</strong>), a populat loss/objective function for modern-day generative models based on DNN like VAE and Diffusion).</p> <h3 id="intuitive-mapping">Intuitive mapping</h3> <p>The intuitive mapping is selected by fitting a modified Kalman Filter ({cite Wu W. Gao Y., Bayesian population decoding of motor cortical activity using a Kalman filter}). Specifically, for each <strong>z-scored</strong> spike count \(z_t\), after obtaining the posterior mean \(\hat{z}_{t} = E[z_t \mid u_t]\) and <strong>z-scoring</strong> each dimension (these z-scorings are important, which will be stressed a multiple times later), the authors started with the common linear dynamical system (LDS) assumption of Kalman Filter:</p> \[\begin{align} x_t \mid x_{t-1} &amp;\sim N(Ax_{t-1} + b, Q) \\ \hat{z}_t \mid x_t &amp;\sim N(Cx_t + d, R) \end{align}\] <p>The parameters \(A,b,Q,C,d,R\) are obtained by maximum likelihood estimation, where \(x_t\) is the estimate of monkey‚Äôs intended velocity (label for the data). Since the spike counts and the latent factors were both <strong>z-scored</strong> and the calibration kinematics were centered, \(\mu = d = b = 0\).</p> <p>Consequently, by filtering the goal is to estimate \(\hat{x}_{t} = E[x_t \mid \hat{z}_{1}, \;, ... \;, \hat{z}_{t}]\). The authors directly gave out the formula below to express \(\hat{x}_t\) in terms of the final decoded velocity at the previous step \(\hat{x}_{t-1}\) and the current z-scored spike count \(u_{t}\):</p> \[\begin{equation} \hat{x}_t = M_1 \hat{x}_{t-1} + M_2 u_t \end{equation}\] \[\begin{equation} M_1 = A - KCA \end{equation}\] \[\begin{equation} M_2 = K\Sigma_{z}\beta \end{equation}\] \[\begin{equation} \beta = \Lambda^T(\Lambda \Lambda^T + \Psi)^{-1} \end{equation}\] <p>where \(K\) is the steady-state Kalman gain matrix. As part of the process of z-scoring the latent factors, \(\Sigma_z\) is a <strong>diagonal</strong> matrix whose diagonal element (\(p, p\)) refers to the inverse of standard deviation of the \(pth\) factor. Since both spike counts and latent factors are <strong>z-scored</strong>, the perturbed mappings (see in the next section) ‚Äúwould not require a neural unit to fire outside of its observed spike count range‚Äù.</p> <p>The above formula might sound confusing, so I present below a detailed derivation. It‚Äôs not so complicated but readers who are not interested in derivation feel free to skip it.</p> <h4 id="derivation-of-the-iterative-filtering-equation">Derivation of the iterative filtering equation</h4> <p>I‚Äôll derive the above formula (5 - 8) in the following 3 steps. In the end, this is nothing brand new and elusive.</p> <blockquote> <p>Step 1: Obtain the posterior of \(z \mid u\)</p> <p>Step 2: z-score the latents</p> <p>Step 3: Apply Kalman filter</p> </blockquote> <h5 id="step-1-linear-gaussian-system">Step 1: Linear Gaussian system</h5> <p>I‚Äôll start with a well-known fact about linear Gaussian system (this derivation is also the core of Gaussian Process and Kalman Filter; Stop for a second and marvel again at the all-encompassing power of Gaussian distribution). Assume two random vectors \(z \in \mathbb{R}^m\) and \(x \in \mathbb{R}^n\) which follow the Gaussian distribution:</p> \[\begin{equation} p(z) = N(z \mid \mu_z, \Sigma_z) \end{equation}\] \[\begin{equation} p(x \mid z) = N(x \mid Az + b, \Omega) \end{equation}\] <p>The above illustrates a <strong>linear Gaussian system</strong>. Note that \(A \in \mathbb{R}^{n \times m}\). Consequently, the correponsding joint distribution \(p(z, x) = p(z)p(x \mid z)\) is also a Gaussian with an \((m + n)\) dimensional random vector:</p> \[\begin{equation} p(z, x) = N( \begin{bmatrix} z \\ x \end{bmatrix} \mid \tilde{\mu}, \tilde{\Sigma}) \end{equation}\] <p>where</p> \[\begin{align} \tilde{\mu} &amp;= \begin{bmatrix} \mu_z \\ A\mu_z + b \end{bmatrix} \\ \tilde{\Sigma} &amp;= \begin{bmatrix} \Sigma_z &amp; \Sigma_z A^T \\ A\Sigma_z &amp; A\Sigma_z A^T + \Omega \end{bmatrix} \end{align}\] <p>The above could be easily derived from matching the corresponding moments, so I will not show in full details. From this joint Gaussian, we could thus easily continue to write out the posterior distribution:</p> \[\begin{equation} p(z \mid x) = N(z \mid \mu', \Sigma') \end{equation}\] <p>where</p> \[\begin{equation} \mu' = \mu_z + \Sigma_z A^T(A\Sigma_z A^T + \Omega)^{-1}(x - (A \mu_z + b)) \end{equation}\] \[\begin{equation} \Sigma' = \Sigma_z - \Sigma_z A^T(A\Sigma_z A^T + \Omega)^{-1}A\Sigma_z \end{equation}\] <p>The above posterior is known as <strong>Bayes‚Äô rule for Gaussians</strong>. It states that if both the prior \(p(z)\) and the likelihood \(p(x \mid z)\) are Gaussian, so is the posterior \(p(z \mid x)\) (equivalently, Gaussian prior is a <strong>conjugate prior</strong> of Gaussian likelihood or Gaussians are <strong>closed under updating</strong>, <a class="citation" href="#pml2Book">(Murphy, 2023)</a> P29). One interesting fact is that although the posterior mean is a linear function of \(x\), the posterior covariance is entirely independent of \(x\). This is a peculiar property of Gaussian distribution (Interested readers please see more explanations in <a class="citation" href="#pml2Book">(Murphy, 2023)</a> sections 2.3.1.3, 2.3.2.1-2, and 8.2). Finally, keen readers might already perceive the equation (15,16) prelude the form of the Kalman Filter posterior update equations.</p> <p>From the above posterior Gaussian form, by plugging in the notations specified in (1-2) with \(z = z_t, \; x = u_t, \; \mu_z = 0, \; \Sigma_z = I \;, A = \Lambda, \; b = \mu \;, \Omega = \Psi\) we obtain the following:</p> \[\begin{equation} p(z_t \mid u_t) = N(z_t \mid \mu_{post}, \Sigma_{post}) \end{equation}\] <p>where</p> \[\begin{align} \mu_{post} &amp;= 0 + I \Lambda^T(\Psi + \Lambda I \Lambda^T)^{-1}(u_t - (\Lambda 0 + \mu)) \\ &amp;= \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}u_t \end{align}\] <p>and</p> \[\begin{align} \Sigma_{post} &amp;= I - I\Lambda^T(\Psi + \Lambda I \Lambda^T)^{-1}\Lambda I \\ &amp;= I - \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}\Lambda \end{align}\] <p>I deliberately used a different set of notations for the posterior linear Gaussian system, so if you are interested please do this little derivation on your own. Since \(\hat{z}_t = E[z_t \mid u_t]\), from above we know that</p> \[\begin{equation} \hat{z}_t = \Lambda(\Psi + \Lambda \Lambda^T)^{-1}u_t \end{equation}\] <h5 id="step-2-perform-z-scoring">Step 2: Perform z-scoring</h5> <p>The second step is to z-score the posterior mean \(\hat{z}_t\), which, here, is dividing each position of this vector by the corresponding standard deviation:</p> \[\begin{equation} \hat{z}_{t, z-scored} = \begin{bmatrix} \hat{z}_{t}^{1} / \sigma_{1} \\ \hat{z}_{t}^{2} / \sigma_{2} \\ \vdots \\ \hat{z}_{t}^{p} / \sigma_{p} \end{bmatrix} = \begin{bmatrix} \frac{1}{\sigma_1} &amp; 0 &amp; 0 &amp;\cdots &amp; 0 \\ 0 &amp; \frac{1}{\sigma_2} &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; &amp; \ddots &amp; &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \frac{1}{\sigma_{p}} \end{bmatrix} \hat{z}_t = \Sigma_z \hat{z}_t \end{equation}\] <p>For simplicity, for the below I‚Äôll replace \(\hat{z}_{t, z-scored}\) with \(\hat{z}_t\).</p> <h5 id="step-3-apply-kalman-filter">Step 3: Apply Kalman Filter</h5> <p>As in Step 1, let me right down the filtering equation for a pure Kalman Filter based on the Gaussian linear assumption made in (9-10) (For the following I‚Äôll drop off \(b, d\) since they are 0). The notations below might be a little messy, but I want to keep it rigorous. The hat on \(x, P\) refer to estimates not ground truth, and \(t \mid t-1\) indicates prior estimation while \(t \mid t\) indicates posterior estimation at time point \(t\).</p> \[\begin{align} \hat{x}_{t|t-1} &amp;= A\hat{x}_{t-1|t-1} \\ \hat{P}_{t|t-1} &amp;= A\hat{P}_{t-1|t-1}A^T + Q \\ K_t &amp;= \hat{P}_{t|t-1}C^T(C\hat{P}_{t|t-1}C^T + R)^{-1}\\ \hat{x}_{t|t} &amp;= \hat{x}_{t|t-1} + K_t(\hat{z}_{t} - C\hat{x}_{t|t-1}) \\ \hat{P}_{t|t} &amp;= (I - K_tC)\hat{P}_{t|t-1} \end{align}\] <p>Again, we play the trick of substitution, starting with (27):</p> \[\begin{align} \hat{x}_{t\mid t} &amp;= \hat{x}_{t \mid t-1} + K_t(\hat{z}_{t} - C\hat{x}_{t \mid t-1}) \\ &amp;= A\hat{x}_{t-1 \mid t-1} + K_t(\hat{z}_t - CA\hat{x}_{t-1 \mid t-1}) \\ &amp;= (A - K_tCA)\hat{x}_{t-1 \mid t-1} + K_t \hat{z}_t \\ &amp;= (A - K_tCA)\hat{x}_{t-1 \mid t-1} + K_t\Sigma_z( \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}u_t) \\ &amp;= (A - K_tCA)\hat{x}_{t-1 \mid t-1} + K_t\Sigma_z \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}u_t \end{align}\] <p>Finally, if we define \(M_1 = A - KCA\), \(M_2 = K\Sigma_z \beta\), \(\beta = \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}\) (formula (6-8)), we‚Äôd claim what we saw in (5):</p> \[\hat{x}_t = M_1 \hat{x}_{t-1} + M_2 u_t\] <p>What I wrote as \(\hat{x}_{t \mid t}\) is the posterior prediction which the authors denoted \(\hat{x}_{t}\) for simplicity (same logic for \(t-1\)). The only subtlety that remains here is that in the above derivation I used the dynamic Kalman Gain \(K_t\), calculated for every time point $t$. In the paper the authos utilized steady Kalman Gain (that‚Äôs why there‚Äôs no subscript \(t\)) but that‚Äôs basically iterate the above filtering equations many times with the given set of model parameters (\(A,Q,C,R\)) until \(K_t\) converges to some matrix \(K = \lim_{t \rightarrow \infty} K_t\), and then use that matrix for all time steps. Basically, you could just replace \(K\) with \(K_t\) without changing the backbone of the inference structure.</p> <p>Before we jump into the perturbation method, the formula (5) does inform us the central philosophy of Kalman Filter: it integrates model prediction by its specified linear dynamics and the observation together, to arrive at an optimal (I‚Äôll not dive deep into what optimality represents here) posterior inference. Extracting out the linear relationship between prediction at timestep \(t\) and the previous step \(t-1\) together with the observation input would help understanding the perturbation method below.</p> <h2 id="perturbation-method">Perturbation method</h2> <p>Then the core methodology of this study is to change the BCI mapping so that the altered control space would be lying either within or outside of the insintric manifold. The paper does present some confusion as to how intuitive mapping and control space would be distinguished. My interpretation is that the control space refers to the ideal potential neural subspace for which to control the cursor optimally. Since within a short time neural connectivity is kept unaltered, the true intrinsic manifold is approximately invariant and thus the required potential neural subspace might not be reachable. By default the control space/intuitive mapping lies within the intrinsic manifold (that‚Äôs why it‚Äôs called ‚Äúintuitive‚Äù, because that‚Äôs is what the neural network system has learned to achieve).</p> <p>The neuronal connectivity statistics is referred to as the natural co-modulation patterns.</p> <p>In short, a within-manifold perturbation only reoriented the control space such that it still resides in the intrinsic manifold (shown in Fig.3 red line). This does not require monkeys to readapt neuronal connectivity patterns to achieve such new control space. It only altered the function from the intrinsic manifold to cursor kinematics. On the other hand, an outside-manifold perturbation alters the control space allowing it to live off the intrinsic manifold (Fig.3 blue line). Notice that if such outside-manifold perturbation pushes the control space along the orthogonal subspace that passes through the original control space, then the mapping from the neural comodulation patterns to cursor kinematics is preserved (basically just project the altered control space to the intrinsic manifold, then would recover the original control space). However, the underlying comodulation/covariation patterns among the neural population are altered.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1c.png" class="img-fluid rounded z-depth-1" width="45%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.1c in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <p>After the perturbation, the authors observed if the monkeys could eventually learn to readapt to the new mapping, to achieve great cursor control performance. For within-manifold perturbation, the monkeys only need to learn to associate cursor kinemaitcs to a new set of neural comodulation patterns (still within reach because lying in the same intrinsic manifold). However, for outside-manifold perturbation, they had to generate new co-modulation patterns in order to reach outside of the existing intrinsic manifold. Consequently, the authors predicted that within-manifold perturbation is easier to learn compared to outside-manifold perturbation. The authors did find results to back up this claim and since they are not the main focus of this blog, I‚Äôll refer interested readers to the original paper to take a look (FIgure 2).</p> <h3 id="perturbation-as-permutation">Perturbation as permutation</h3> <p>Other than that, I do want to dive deep into how such within/outside-manifold perturbations were implemented. Specifically, from the derivation section readers should be already familiar with equations (5-8), the intuitive mapping. The perturbed mappings are the corresponding modified versions.</p> <p>The within-manifold perturbation still manages to maintain the relationship between neural units to latent factors, but perturb that between latents and cursor kinematics: \(\hat{z}_{t}\) is permuted before going into the Kalman inference (multiplying with the Kalman Gain)(Figure 1 red). Since permutation is simply re-orientation, the within-manifold perturbation is equivalent to re-orientating the coordinate system within the manifold (the manifold is preserved because the row space of \(\Lambda^T\) is preserved (equation (7))). Mathematically,</p> \[\begin{align} \hat{x}_t = M_1 \hat{x}_{t-1} + M_{2, WM}u_t \\ M_{2, WM} = K\eta_{WM}\Sigma_z\beta \end{align}\] <p>and \(\eta_{WM}\) is a \(10 \times 10\) permutation matrix (10 because of the latent dimensionality)</p> <p>For outside-manifold perturbation, it changes the relationship between the neural units and latent factors by permuting \(u_t\) before passing it into factor analysis (Figure 1 blue). Specifically,</p> \[\begin{align} \hat{x}_t = M_1 \hat{x}_{t-1} + M_{2, OM}u_t \\ M_{2, OM} = K\Sigma_z\beta \eta_{OM} \end{align}\] <p>and \(\eta_{OM}\) is a \(q \times q\) permutation matrix (\(q\) is the number of neural units). The underlying logic is that in the <strong>neural space</strong> for \(u_t\), the monkeys might not be able to adapt to conteract the perturbation brought by permutation, since \(\eta_{OM}\) directly acts upon \(u_t\).</p> <h3 id="select-perturbed-mappings">Select perturbed mappings</h3> <p>The authors also devised a clever plan to dictate the specific permutation matrices to choose (for a permutation matrix with size \(k \times k\), there‚Äôre \(k!\) numbers of them) in three steps, with the central goal that the perturbed mapping would not be too difficult nor easy to learn:</p> <h4 id="step-1-find-the-candidate-set">Step 1: Find the candidate set</h4> <p>For within-manifold perturbations, all \(10!\) possible permutation matrices are treated as the candidate set. For outside-manifold perturbations, th strategy differs for two monkeys. For one monkey only permutations of nueral units with largetst modulation depths are selected. For the other monkey, the solution is to randomly put all units with the highest modulation depths into 10 groups of \(m\) each (the rest with low modulation forms an 11th group). The outside-manifold perturbation is formed by permutating these 10 groups instead of each unit (thus \(10!\) in total matching that of within-manifold perturbation).</p> <h4 id="step-2-open-loop-velocities-prediction-per--perturbation">Step 2: Open-loop velocities prediction per perturbation</h4> <p>The second step hinges on estimating the open-loop velocities for each candidate permutation. Specifically, it approximates the decoded cursor kinematics if the monkeys did not learn to adapt:</p> \[\begin{equation} x_{OL}^i = M_{2, P}u_{B}^i \end{equation}\] <p>where \(u_{B}^i\) is the mean z-scored spike counts across all trials on the \(i^{th}\) target(\(8\) in total), and \(P\) represents either \(OM\) or \(WM\). The method here echoes the dissection made explicity in equation(5), where the current step prediction \(\hat{x}_t\) is a linear combination of prediction from last step \(\hat{x}_{t-1}\) and neural activities at the present \(M_2 u_t\).</p> <h4 id="step-3-determine-potential-perturbations">Step 3: Determine potential perturbations</h4> <p>Finally, to determine a perturbation the authors compared the open-loop velocities under the perturbed mapping with those under the intuitive mapping for each target. These velocities should only differ in an acceptable range so the monkeys would not find it too simple nor difficult to learn. The authors quantified this metric by defining a range over differences in velocity angles and magnitude, and chose perturbations that fall in this specified range for all targets.</p> <h2 id="quantifiable-metric">Quantifiable metric</h2> <h3 id="amount-of-the-learning">Amount of the learning</h3> <p>To quantify the potential amount of learning under two perturbation kinds, the authors resorted primarily to two performance metric: (the change of) relative acquisition time and relative success rate across perturbation blocks. Specifically, as shown below, the black dot represents the intuitive mapping, while the red and blue dots indicate the imediate performance just after corresponding perturbations. Red and blue asterisks represent the best performance during the within the perturbation sessions. The dashed line indicates the maximum learning vector \(L_{max}\) (note that it starts on the red dot), and thus the aount of learning (\(A_i \in \mathbb{R}\)) is quantified as the length of the projection of the raw learning vector onto the maximum learning vector, normalized by the length of the maximum learning vector:</p> \[A_i = \frac{L_{raw, i} \cdot L_{max}}{\|L_{max}\|^2}\] <p>where \(i \in \{red, blue\}\). Pictorially, it‚Äôs illustrated as below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_2cd.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.2c and Fig.2d in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <p>Note that the asterisks in the above represent the time point/bin corresponding to <strong>maximal</strong> amount of learning. In real case, for each time bin the authors would pinpoint the end points for the learning vectors (for calculations in details, please refer to the METHODS section of the paper), and compute the amount of learning correspondingly (the red and blue learning vectors might end in different positions with a diferent set of relative acquisition time and success rate up to that time bin).</p> <p>The amount of learning for all sessions was presented above in the right pannel. Notice that a value of 1 indicates ‚Äúcomplete‚Äù learning of the new relationship between the required neural co-modulation and cursor kinematics, reverting to the performance level of the intuitive control, while 0 indicates no learning. The authors did observe that there‚Äôs significant amount of learning for within-manifold perturbations than outside-manifold perturbation. To see changes in success rates and acquisition time during perturbation blocks, instead of a single metric \(L\) as shown above, the authors also plotted them separately as below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Ext_Fig_2.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.2 in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <h3 id="after-effects">After-effects</h3> <p>A second metric Sadtler et. al. employed is observe how the monkeys performed as they reintroduce the intuitive mapping, or the so-called after-effects after washout of the perturbed mapping. Specifically, the after-effect is measured as the amount of performance imparement (tentative: acquisition time, success rate) at the beginning at the wash-out block (like how impairement was measured at the beginning of a perturbation block). A large wash-out effect indicates that the monkeys have learned and adapted to the perturbed mapping. For within-manifold perturbation, the authors did observe brief impaired performance but not so for outside-manifold perturbation, indicating that learning did occur during the former.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Ext_Fig_3.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.3 in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <h3 id="principalcanonical-angles">Principal/canonical angles</h3> <p>To quantify the comparisons between the intuitive and the perturbed mappings, Sadtler et.al. also calculated the principal angles between two linear subspaces. Notice that by formula () above, both subspaces are spanned by the rows of \(M_2\) (\(M_{2, WM}\) for within-manifold perturbation, and \(M_{2, OM}\) for outside-manifold perturbation). Consuquently, the two principal angles specify the maximum and minimum angles of separation between the intuitive and the perturbed control spaces. Notice that since the spike counts are z-scored, the control spaces also center at the origin.</p> <p>Sidenote: How do principal angles relate with principal eigenvalues and principal curvatures?</p> <p>To give a short summary of principal angles calculation:</p> <p>Note the role of SVD decompsition.</p> <h2 id="discussions">Discussions</h2> <p>Since Sadtler et. al. employed closed-looop BCI control, they were able to causally alter the model/map from neural activities to the decoded cursor velocities.</p> <p>This paper highlights a potential methodology of BCI research: since the mapping from neural activities to control correlates is <strong>fully specified</strong>, thus could be causally perturbed to explore the corresponding changes of controlled behavior. This allowed the authors to <strong>design/know apriori</strong> the optimal/required neural activities (specified by the altered mapping) to achieve task success, and thus to observe if animals could generate such neural patterns.</p> <p>An outside-manifold perturbation does not necessarily specify that it lives in orthogonal subspace of the intrinsic manifold. Also, because here the intrinsic manifold is illustrated as a plane, in real world scenario, it is unlikely that it exists as a linear subspace. Consequently, specifying a space that is ‚Äúorthogonal‚Äù to the potential manifold (nonlinear, other than a linear subspace) might be problematic.</p> <p>The amount of learning is entirely dependent upon performance itself, which is difficult to causally link to neural changes.</p> <p>For the amount of learning metric, why would 0 indicate no learning? Orthogonal learning?</p> <p>Notice that the after-effects analysis echoed a lot of research methodology in force-field or curl-field perturbation for cursor control in monkey motor control studies.</p> <p>The authors also showed that learning did not improve through sessions (readers might refer to Extended Figure 4 in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a> for further information).</p> <p>The perspective and consideration that Sadtler et.al took to ensure alternative explanations for the observed distinction of learnability do not hold are informative. I enjoyed its rigorosity, especially when they considered perturbed maps which might be initialy difficult to learn and carefully implement the controls (demonstrate that they have controled). To not diverge from the main focus of this blog, I‚Äôll not cover those dicussions. The way that the authors listed clearly alternative explanations and how they tackled each is a very inviting, powerful, and efficient way of writing.</p> <p>From the methods the authors used, they only estimated a linear manifold. According to {cite}, linear manfolds should require more dimensions than a nonlinear manifold which could explain similar amount of variance.</p> <p>The dissection of control into an estimation of intrinsic manifold (Factor Analysis) and then build an intuitive mapping (decoder, Kalman Filter) to relate the latent factors to cursor kinematics is different from directly mapping neural activities to movement. Such dissection becomes a common theme for the past two decades, with the emphasis on latent manifold structure beecomes increasingly popular. More than a theoretical refinement, practically speaking, such dissection also allows the authors to perform two different types of perturbation.</p> <p>Talk more here and also relate this to the nonlinear manifold paper 2025.</p> <p>For defining the candidate set of potential outside-manifold perturbation for the second monkey, while the group looping strategy is a clever way to equalize possible permutation matrices with within-manifold perturbation (\(10!\)), I‚Äôm not completely persuaded by the logic behind it. When explaning the logic for the second monkey outside-manifold perturbation, the authors stated that the within-maifold perturbation only permutes the neural space with \(10\) dimensions, while the outside-manifold perturbation on average deal with 39 dimensions (number of permuted units). Thus the monkey would have to search for more space for outside-manifold perturbation. I think the subtlety here lies in the fact that within-manifold perturbation is not performed on the level of the ambient neural space, but on the latent 10-dimensional space which is extracted out of the original neural space and each basis vector of the latent space is a <strong>linear combination</strong> of the neural units. Its is not explicitly clear to me whether/how these two spaces should be compared solely based on the dimensonality it carries.</p> <p>Additionally, though I do appreciate the flavor of group assignment (there‚Äôs even a flavor of group action here; anyway, permutation matrices form a permutation group), I am not sure if each of the \(10!\) permutations on groups of neural units is ‚Äúequivalent‚Äù to a permutation among 10 latent axes.</p> <p>Another question is outside-manifold perturbation is not necessarily guaranteed to be outside the manifold?</p> <h3 id="required-changes-in-preferred-directions">Required changes in preferred directions</h3> <p>There‚Äôs one interesting method that I don‚Äôt have enough time to delve into, which is the calculation of changes in preferred directions for each neural unit. This is calculated to make sure that learning two perturbation types would require similar effort for the monkeys to adapt (interested readers might refer to Figure 3b in the paper). The authors began by discussing comparing the columns of \(M_2, \; M_{WM, or OM}\), which reflects how each neural unit impacts the decoded cursor kinematics. This strategy is not adopted because for the second monkey \(M_2\) and \(M_{OM}\) share many columns for the un-permuted columns, making it an unfair comparison. However, it‚Äôs informative if we associate this view of \(M_2 u_t\) by assiging each column of \(M_2\) with a coordinate in \(u_t\) with that mentioned in the principal angles section where rows of \(M_2\) represnet an axis upon which \(u_t\) is projected. These two perspectives which interpret linear matrix multiplication as either a <strong>transformation</strong> (of basis vectors in space) or a <strong>projection</strong> which will be further illustrated in an upcoming post.</p> <p>Then, the authors came up with another technique. They assumed that</p> <blockquote> <p>1] under perturbation the monkeys would still manage to keep the same cursor velocity in the intutive mapping,</p> <p>2] The perturbed firing rates should be as close as possible to those in the intuitive mapping</p> </blockquote> <p>and these two assumptions transform into a constrained optimization problem: Find \(u_p^{i}\) such that its Euclidean distance with \(u_B^{i}\) is minimized when \(M_2u_B^i = M_{2, P}u_p^i\):</p> <blockquote> \[u_p^{i, *} = \arg\min_{u_p^i} ||u_p^i - u_B^i||_2\] <p>s.t. \(M_2u_B^i = M_{2, P}u_p^i\)</p> </blockquote> <p>This can be solved in closed-form with Lagrange multipliers (for rigorosity, the inverse below should be replaced with the Moore-Penrose Pseudoinverse, unless \(M_{2, P}\) is full-rank, which I‚Äôm not sure I could theoretically make that claim). Due to limited space, I‚Äôll not leave the proof to another blog on linear transformation, which I also illustrate its relationship with CCA and another paper (Juan Gallego, trajectory alignment):</p> \[\begin{equation} u_p^i = u_B^i + M_{2, P}^T(M_{2, P}M_{2, P}^T)^{-1}(M_2 - M_{2, P})u_B^i \end{equation}\] <p>where \(u_B^i\) is the mean normalized spike count vector across all trials for each target \(i\) in the basline blocks.</p> <p>Then the authors fit a standard cosine tuning model for each unit \(k\) with all targets:</p> \[\begin{equation} u_B^i(k) = m_k \cdot cos(\theta_i - \theta_B(k)) + b_k \end{equation}\] <p>where for each neural unit \(k\), its preferred direction is encoded as \(\theta_B(k)\), \(m_k\) the depth of modulation, \(b_k\) the model offset, \(\theta_i\) the direction of the \(ith\) target. Apply the same calculation for \(u_P^i\) to obtain the preferred direction \(\theta_P(k)\) for each unit \(k\) under the perturbaed mapping. Finally, the preferred direction changes (for each neural unit) is calculated as:</p> \[\begin{equation} \mid \theta_P(k) - \theta_B(k) \mid \end{equation}\] <h3 id="selection-of-intrinsic-dimensionality">Selection of intrinsic dimensionality</h3> <p>Usually we do not have a coherent and systematic way to detemrine the optimal intrinsic dimensionality. Here for factor analysis, based on its explicit probabilistic inference structure, the authors could easily compute the likelihood for cross-validated data.</p> <h3 id="measurement-of-cumulative-shared-variance">Measurement of cumulative shared variance</h3> <p>Based on equations (13), the original covariance of \(u\) is decomposed (with minor substitutions) into a shared component \(\Lambda \Lambda^T\) and an independent component \(\Psi\). In order to calculate the amount of shared variance along orthogonal directions within the manifold (notice this is a linear manifold), consequently, the authors calculated the eigenvalues of \(\Lambda \Lambda^T\) which present the shared variances, each corresponds to an orthonormalized latent dimension. This is similar to Churchland 2012 the last blog‚Ä¶</p> <h2 id="conclusions">Conclusions</h2> <p>The neural manifold reflects the inherent connectivity which constrains (in a short term) the potentially learnable patterns. Consequently, the neural connectivity network structure dictates possible neural patterns and corresponding behavior repertoire the animals are capable of performing.</p> <p>This paper strengthens my belief in the legit usability of the low dimensional structure among neural population, and more crucially the value of perturbation methods to causally verify the neural manifold. Specifically, the extraction of latent factors, other than directly mapping neural activties to cursor kinematics, not only adds more interpretability to the framework, but also provies a readily distinguishable strategy of within/outside-manifold perturbations. This reminds me of many other models with latent factors in between: (xxx, xxx, xxx, xxx).</p> <h1 id="dynamical-constraints-oby-etal-2025">Dynamical constraints (Oby et.al. 2025)</h1> <p>This paper <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a> presented some surprising facts about neural dynamics. One key question the authors are interested is whether the sequential representation or computation that neural population could perform is temporally locked. The critical assumption is that if neural dynamics does reflect the underlying connectivity, then it should be robust and difficult to alter. More specifically, they tested if such neural population patterns could be produced but in a <strong>reversed</strong> time ordering and the results show otherwise: even when the animals were presented with different visual feedback or strong incentive to change the time order (for example, reversing the time course) of the neural activities, the temporal evolution of the neural dynamics is still robust and difficult to violate (thought I think the claim might be too strong. See more in the discussions).</p> <h2 id="different-views-of-the-high-dimensional-neural-space">Different views of the high dimensional neural space</h2> <div class="row mt-3"> <div class="col-sm-7"> <p> Note that unlike many previous BCI works, neural activities are mapped to curosr __positions__ directly instead of velocities. The modeling framework is similar to <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>, in that high dimensional neural activities <strong>90D</strong> are mapped into <strong>10D</strong> latents by <strong>GPFA</strong> (instead of simply <strong>FA</strong> in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>), and then build corresponding linear maps from <strong>10D</strong> latent factors to <strong>2D</strong> cursor positions using different maps. Since each map is a linear projection of latent factors to <strong>2D</strong> space, it is geoemtrically equivalant to observing the high dimensional signals from a specified angle. The key ingredient of this paper is that the authors found if with some linear <strong>2D</strong> mapping/projection, the neural trajectories are readily flexible and reversible, whereas some other views robustly exhibited no significant change even if the monkeys were inspired to alter the neural trajectories. </p> <p> There're two mappings the authors emphasizd, one is called "movement-intention" (MoveInt) mapping, under which the monkeys could intuitively control the cursor to move between two diametrically placed targets <strong>A</strong> and <strong>B</strong>. Indeed, under MoveInt, the neural trajectories (equivalent to cursor motion here) could go back and forth between <strong>A</strong> and <strong>B</strong> with significant overlapping. This might lead readers to believe that the neural dyanmcis are also reversible. </p> </div> <div class="col-sm-5 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Adapted from Fig. 2 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. a: decoding pipeline. b: Overlapping neural/cursor trajectories under multipl orientations. From this it seems that time courses of neural dynamics are flexible for different orientations (thus including reversing the temporal order). </div> </div> </div> <p>However, under another 2D mapping selected as maximizing the separations (\(SepMax\)) between \(A\) to \(B\) trajectories v.s. \(B\) to \(A\) trajectories, the neural trajectories overlapping by \(MoveInt\) projection are clearly distinguished. This is shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_3.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 3 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. See a. and b. where A-B and B-A trajectories are distinct. Panels c. and d. show results from quantitative metric (discriminability <strong>d'</strong> between midpoints of trajectories signify separation of trajectories). </div> <h3 id="moveint-vs-sepmax-projections">MoveInt vs SepMax projections</h3> <p>Both \(MoveInt\) and \(SepMax\) projections are calculated by fitting a simple linear transformation to the extracted \(10D\) latents \(\hat{z}_t\) to acquire \(2D\) cursor positions \(\hat{x}_t\), which is used to displace the cursor in closed-loop control:</p> \[\begin{equation} \hat{x}_t = W\hat{z}_t + c \end{equation}\] <p>except they have different weight matrices and bias \(W\) and \(c\).</p> <p>For \(MoveInt\) projection, \(\hat{x}_t\) is selected as the label velocity vectors, and thus \(W_{MI} \in \mathbb{R}^{2 \times 10}\) and \(c_{MI} \in \mathbb{R}^{2 \times 1}\) are obtained by the correpsonding linear regression:</p> \[\begin{align} W_{MI} = XZ^{T}(ZZ^T)^{-1} \\ c_{MI} = -W_{MI}(\frac{1}{n}\sum_{t=1}^{n}\hat{z}_t) \end{align}\] <p>where \(Z = [\hat{z}_1, \hat{z}_2, ..., \hat{z}_n] \in \mathbb{R}^{10 \times n}\) represents the collection of \(GPFA\) latent states, and \(X = [x_1, x_2, ..., x_n] \in \mathbb{R}^{2 \times n}\) indicates the intended cursor position. \(n\) is the total number of timesteps during calibration.</p> <p>For \(SepMax\) projection, the goal is to identify subspace upon which cursor trajectories showcased maximal separation during the two-target task from target pair \(A\) to \(B\) and \(B\) to \(A\). The projections is acquired by satisfying the following 3 objectives: 1] maximizing separation between midpoints of both ways (\(\bar{z}_{AB}\) and \(\bar{z}_{BA}\)); 2] minimizing trial-to-trial variance at midpoints (\(\Sigma_{AB}\) and \(\Sigma_{BA}\)); and 3] maximizing projections of \(\bar{z}_{A}\) and \(\bar{z}_{B}\).</p> <p>The algorithm for \(SepMax\) projection could be summarized in the following:</p> <blockquote> <p>1] Compute trial-averaged starting points \(\bar{z}_{A}, \; \bar{z}_{B}\) from \(A\) to \(B\) and \(B\) to \(A\) trials, respectively,</p> <p>2] Calculate midpoint \(m = \frac{\bar{z}_{A} + \bar{z}_{B}}{2}\)</p> <p>3] For a given trial, project the trajectory \(\hat{z}_t\) onto the axis connecting \(\bar{z}_{A}\) and \(\bar{z}_{B}\). Define \(\hat{z}_{t_{c}}\) as the trial midpoint, where \(t_{c}\) is the timepoint where the projection is closest to \(m\),</p> <p>4] Define \(\bar{z}_{AB}\) as the trial-averaged midpoint over \(\hat{z}_{t_{c}}\) from \(A\) to \(B\) trajectories, similarly \(\bar{z}_{BA}\) from \(B\) to \(A\),</p> <p>5] Simiar to 4], calculate covariance \(\Sigma_{AB}\) over \(\hat{z}_{t_{c}}\) for \(A\) to \(B\) trials, and simiarly \(\Sigma_{BA}\) for \(B\) to \(A\) trials,</p> <p>6] Finally, to obtain the ideal \(2D\) projection and find two orthonormal vectors collected in \(P_{SM} = [p_1, p_2] \in \mathbb{R}^{10 \times 2}\), solve the optimization problem below:</p> \[J = -w_{mid}p_{1}^T(\bar{z}_{AB} - \bar{z}_{BA}) + w_{var}p_{1}^T(\Sigma_{AB} + \Sigma_{BA})p_1 - w_{start}p_{2}^T(\bar{z}_B - \bar{z}_A)\] <p>7] To align the space from \(P_{SM}\) with animals‚Äô workspace: \(W_{SM} = AP_{SM}^T, \;, c_{SM} = -AP_{SM}^Tm\)</p> <p>where</p> \[A = R_{\theta}OS\] </blockquote> <p>Note that \(\bar{z}_{A}, \bar{z}_{B}, m, \bar{z}_{AB}, \bar{z}_{BA} \in \mathbb{R}^{10 \times 1}\), \(W_{SM} \in \mathbb{R}^{2 \times 10}, A \in \mathbb{R}^{2 \times 2}, c_{SM} \in \mathbb{R}^{2 \times 1}\) and \(w_{mid}, w_{var}, w_{start}\) are all weighting hyperparameters. The above objective function is composed of 3 separate aims which match exactly the those mentioned in above. Consequently, along \(p_1\) trajectories of different directions are maximally separated, while targets are distinguished on an orthogonal \(p_2\) axis.</p> <p>The goal for step 7] is that after projection the starting points of \(A\) to \(B\) and \(B\) to \(A\) align with the two targets \(A\) and \(B\) in animals‚Äô workspace. \(A\) is comprised with 3 transformations:</p> <blockquote> <p>1) Scaling the axes of \(P_{SM}\) with a diagonal matrix \(S \in \mathbb{R}^{2 \times 2}\) such that distance between \(\bar{z}_A\) and \(\bar{z}_B\) is identical to \(A\) and \(B\) in \(MoveInt\) projection,</p> <p>2) Optionally flip the projection along the \(A - B\) axis with \(O \in \mathbb{R}^{2 \times 2}\) (see more in discussion), such that the controlled cursor movements align with the expected orientation,</p> <p>3) Rotation with \(R_{\theta}\) so the projection aligns with workspace targets \(A\) and \(B\).</p> </blockquote> <p>The above process is illustrated below in panels d. and e. Notice that the above postprocessing is not an isometry, in that a scaling is also applied (\(P_{SM}\)).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Ext_Fig_10.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.10 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. d., e.: demonstration of the above quantities. f.: projection of the midpoints of both way trajectories, <span class="math">\(\bar{z}_{AB}\)</span> and <span class="math">\(\bar{z}_{BA}\)</span>, onto an axis <strong>a</strong> with maximal midpoint separation. The discriminability <strong>d'</strong> is calculated from the projected mean and variance: <span class="math">\(d' = \frac{|\bar{z}_{AB}^Ta - \bar{z}_{BA}^Ta|}{\sqrt{\tfrac{1}{2}a^T(\Sigma_{AB} + \Sigma_{BA})a}}\)</span> </div> <p>After identifying the existence of irreversible neural trajectories, the authors continued to explore how robust time course evolution is, with 3 experiments that built upon the previous ones which increasingly motivated the monkeys to adapt neural dynamics.</p> <h2 id="task-1-visual-feedback-task">Task 1: Visual feedback task</h2> <p>Firstly, the authors gave monkeys the visual feedback of neural trajectory mappings by \(SepMax\) projection (not \(MoveInt\)). Based on the underlying inherent tendency to streighten cursor trajectories, the authors intended to observe if the monkeys would indeed made such adjustments, which would then demonstrate that temporal evolution of neural patterns is flexible. However, the authors found that the cursor trajectories were robust and were independent of which visual feedback the monkeys received. This further corroborates the claim that the monkeys did not have volitional control over certain time order of neural trajectories. The experiment paradigm and results are summarized below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_4.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 4 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. Given visual feedback from the <strong>SepMax</strong> projection (b.), the authors still do not observe straightening of cursor trajectories for <strong>SepMax</strong> projection (c.). In fact, projections in either linear subspace are robust whether which visual feedback is presented (d.). </div> <p>One key remaining question is whether the robustness of constraint exists only in the \(SepMax\) projection subspace, or if it‚Äôs a common phenomena also observable in other dimensions. Consequently, the authors applied random \(2D\) projections of the \(10D\) latents and conducted flow field analysis (I did not have space to go into details, but flow field analysis was employed to quantify the discrimination of trajectories).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Ext_Fig_4.png" class="img-fluid rounded z-depth-1" width="90%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.4 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. Latent factors are projected into <strong>2D</strong> space by random projection matrices and the flow field is calculated. This is repeated for 400 times and the corresponding comparisons of flow fields in the corresponding projections under different feedbacks (<strong>MoveInt</strong> or <strong>SepMax</strong>) are shown in cyan distribution as "other feedback" in g. and h. Note that d. is an example comparison of flow fields for <strong>SepMax</strong> projection. In order to interpret the other-feedback distributions the authors also devised two control groups where no change or maximal change in flow fields is expected. For no-change distribution, subsets of trials from the same feedback is used to calculate flow fields, called "fixed feedback". For maximal change, two distributions are devised: one is reversing the direction of the flow field so they overlap and are maximally different (called "time-reversed", e.), and the other with altered starting targets (called "alternate-target", f.). The difference in flow field is min-max normalized by fixed feedback and time reversed feedback, respectively, the smaller the less difference among flow field comparisons. g. indicates that other feedback distribution is not significantly different from fixed feedback. h. shows that state space highly overlaps between fixed feedback, other feedback, andn time reversal. Taking stock, the other feedback distributions show low flow difference under and hight flow overlapping under different feedbacks, suggesting that the neural trajectories are indeed robust in the <strong>10D</strong> latent space. </div> <p>Note that the above figure only demonstrates that neural trajectories are robust and highly consistent/<strong>constrained</strong> under different feedbacks in many different dimensions, viewed from distinct perspectives/projections. This does not necessarily translate to the unversality of separability of trajectories in different subspaces other than \(SepMax\) projection. It is the former, the inability to alter neural trajectories under whatever visual feedback in some projected space, that the authors refer to as the <strong>constraint</strong>. It is this constraint that the authors found could be extended to many other dimensions (other than just \(SepMax\) projection) by the distribution from random projections (other feedback) shown above.</p> <h2 id="task-2-it-task">Task 2: IT task</h2> <div class="row mt-3"> <div class="col-sm-7 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Adapted from Fig.6 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. Looking at the flow field generated by the previous two-target trials (a.), the authors explored if the monkeys could move back against the flow field (b.), to achieve an "intermediate target" (IT) on the midway (c.). Black lines: single-trial trajectories, thick lines being the trial average. For quantification, initial angles of trajectories were calculated under multiple comparison groups. d.: trajectory to IT (black) v.s. direct path against the flow field (black dashed), compared to trajectory to the blue target (red) v.s.direct path (black dashed), showed as % difference in e. 0% indicates the cursor movement is along the flow field, whereas 100% implies that monkeys could move against the flow field. The authors also performed similar analyses with no-change (f. and g.: initial angles between direct path to IT and early trials (red), with direct path to IT and late trials(dashed red)) and full-change control (h. and i.: initial angles between directh path to IT with different targets (here blue and aqua)) </div> </div> <div class="col-sm-5"> <p> In the second task, the authors explicitly motivated the monkeys to present the neural activities (by the cursor movement) in a time-reversed ordering by motivating them to move against an empirically derived flow field (from past cursor trajectories) towards an "intermediate target" (<strong>IT</strong>) along the path from <strong>A</strong> to <strong>B</strong>. The authors observed that in order to go from target <strong>B</strong> to <strong>IT</strong>, the monkeys did not adopt the time-reversed pathway of <strong>A</strong> to <strong>B</strong>, but followed, at least initially, the path specified by the flow field from <strong>B</strong> to <strong>A</strong> and then reverted back towards <strong>IT</strong>. Consequently, the monkeys failed to reproduce the existing neural activities patterns in the reversed time-ordering, and they did not (initially) violated the flow field. This is quantified by initial angles of trajectories across multiple control/comparison groups ('no-change': early vs. late trials, where there is no expectation of difference; 'full-change': difference of initial angles among trials of different end targets (same starting target) in the <strong>MoveInt</strong> projection), and the authors discovered that the initial angles during the IT task is not significantly different from the 'no-change' condition, but not with 'full-change' condition. This demonstrates that the time course of neural population is robust and not easily modifiable. </p> </div> </div> <h2 id="task-3-instructed-path-task">Task 3: Instructed path task</h2> <p>In the third task, in order to further motivate the monkeys to reverse neural trajectories, the authors appied an ‚Äúinstructed path task‚Äù where a visualb boundary was applied to the time-reversed trajectory. However, the monkeys only minimally modified the cursor trajectories as the bounday was reduced in size, and the trajectories followed the flow field instead of violating it while showcasing the reverting/hooking phenomena in the \(IT\) task. This again demonstrates that the temporal evolution of neural activity patterns is robust and thus it is robustly constrained.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_7.png" class="img-fluid rounded z-depth-1" width="85%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.7 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. The monkeys were required to move the cursor to the IT (as in Task 2) within the visual boundary (a.), with the boundary decreasing in size to motivate monkeys to change trajectories (b., failed trials shown in red). c.: five boundary sizes with different distances from the boundary to the target(dashed line), and trial-averaged trajectories for successful trials only for each boundary. d. and e.: Comparison of the initial angle between direct IT path (black dashed) and the trial-averaged trajectory of all trials (black), and that between direct IT path (black dashed) and trial-averaged trajectories in the two-target taks (task 1, dark red). The distribution of percent difference is similar to the no-change distribution (dark red triangle and line) seen in Fig.6. indicating that even with visual boundaries the cursor trajectories are still robust. </div> <h2 id="discussions-1">Discussions</h2> <p>I like how the experiments are built upon each other to add more constraints and motivation for the monkeys to reverse the time course of neural activities. The quantification methods used (flow field analysis, initial angle comparison among trajectories) are great tools to add to future research. Many adjustments and tweaks of experiment sessions to enable further exploration are interesting to read, like for Task 1 the authors also flipped the projection on the screen along the axis connecting two targets, in order to observe if cursor trajectories would also be reflected or not (details not discussed in this blog, illustration shown below; should be self-explanatory).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Ext_Fig_5.png" class="img-fluid rounded z-depth-1" width="85%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.5 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. When reflection of the projection is added (a., rotation matrix <strong>O</strong>), two possilibies based on whether the neural trajectories were dependent on visual preference (curvature is preserved, Possibility 1) or the underlying network connectivity (curvature is also flipped, Possibility 2). c. shows that the orientation of the cursor trajectories under the reflected <strong>SepMax</strong> feedback in the reflected <strong>SepMax</strong> projection is also flipped compared to those under the <strong>SepMax</strong> feedback in the <strong>SepMax</strong> projection (A typo above the third panel of c., should be "Reflected Separation-Maximizing Projection" instead of "Reflected Separation-Maximizing Feedback"). d.: quantified flow field difference between <strong>SepMax</strong> and reflected <strong>SepMax</strong> projections (under corresopnding feedbacks) is significantly larger than the difference between <strong>SepMax</strong> under <strong>MoveInt</strong> and <strong>SepMax</strong> feedbacks. Consequently, the neural trajectories are determined not by visual preference but underlying connectivity constraints. </div> <p>A dynamical system does not simply allow flowing back.</p> <p>Instead of an abrupt change of the experiment, graduallly apply the changes: not to go to reversa in total in an instant: reminds me of hysterisis. The conclusion seems so strong.</p> <p>Would this be a byproduct of the training process itself? ‚Äî- if the monkeys were trained with visual feedback of the maximum separation view, would that be different?</p> <h3 id="linear-mapping">Linear mapping</h3> <p>Linear manifold, linear mapping, orthogonal/null space</p> <h3 id="dynamical-systems-explanation">Dynamical systems explanation</h3> <p>The authors claimed that the observed time course activities reflect the underlying connectivity among the neural population. They made associations with network models where at any present, the activities of each node is determined by the previous time behavior of all nodes, the network connectivity, and the external input. This is explicitly reflected by the dynamical systems perspective (let‚Äôs not go to SDE for now):</p> \[\begin{equation} \dot{x}(t) = f(x(t)) + u(t) \end{equation}\] <p>Discretizing the above would reveal previous time step dependence, and the dynamics is specified by \(f\) determined by network connectivity. The paper <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a> discussed in <a href="/blog/2025/jPCA/">my earlier post</a> made this the backbone of modeling. From this perspective, it seems not surprising that neural trajectories do not necessarily need to be reversible. One extreme hypothetical case would: Imagine the following simple \(2D\) linear dynamical system:</p> \[\begin{equation} \dot{x}(t) = x(t) \end{equation}\] <p>The phase space could be visualized as below:</p> <p>It‚Äôs not difficult to observe that the vector field on each state \(\dot{x}_t\) is perpendicular to the state itself \(x_t\). Consequently, the dynamics exhibit as pure rotation. If the starting point is \(p_1\), then to move towards \(p_2\), it cannot go counter-clockwise, although it seems closer, but it has to travel through longer distance clockwise and reach \(p_2\).</p> <p>Consequently, to alter the time course would require substantial adjustment of the connectivity itself to change \(f\), which in a short time span is not quite likely readily achievable.</p> <h3 id="constraints-on-the-input">Constraints on the input</h3> <p>Since the monkeys were assumed to freely change their neural activities input to \(M1\), the fact that they were no capable of altering the trajectory indicates that changing inputs could only affect the dynamics in a limited way. The authors hypothesized that perhaps the inputs could only affect certain dmensions of neural dynamics (for example, shown by \(MoveInt\)), or it could not overcome the intrinsic dynamics the neural network carries. s</p> <h3 id="hysteresis-explanation">Hysteresis explanation</h3> <h3 id="eventual-but-not-initial-violation-of-flow-field">Eventual, but not initial, violation of flow field</h3> <h3 id="determinism-and-the-exception">Determinism and the exception</h3> <p>To arrive at a given state \(z(t)\), \(z(t-1)\) is pre-determined. The only exception is attractor and limit cycle?</p> <h3 id="relation-with-rotational-dynamics">Relation with rotational dynamics</h3> <p>The study might remind us of rotational dynamics (cite jPCA paper). Previous study showed that simply by reversing the hand kinematics would not necessarily lead to reversal of the direction of the rotational dynamics (% cite russo2018 %). What‚Äôs even more striking here is that there‚Äôs no arm movement component included here, thus further emphasizing that kinematic movement or somatosensory feedback (as shown coupling many monkey studies) is not a necessary condition for engendering the neural dynamics, which nonetheless carries on an inherent property of the underlying neuronal connectivity.</p> <h3 id="relation-with-low-tangling">Relation with low-tangling</h3> <p>On another hand, the monkeys were motivated to produce neural latents that evolve with high-tangeling, but in fact neural trajectories showed only low tangling.</p> <h3 id="relation-with-output-nullpotent-space">Relation with output-null/potent space</h3> <p>Finally, this does echoe the flavor of output-potent/null subspace (another blog coming soon). Indeed, the distingushed properties of neural dynamics observed in \(MoveInt\) v.s. \(SepMax\) projections echoe the observation that different perspectives of viewing neural dynamics might lead to functionally distinguished inerpretations: preparatory activities encoded in the null-space are not read out as they are projected onto the ‚Äúsame‚Äù coordinates on the readout output-potent space, but they specify the initial conditions to drive the temporal evolution of neural activities along the output-potent space.</p> <h2 id="conclusions-1">Conclusions</h2> <p>The animals‚Äô dynamical structure is robust and highly constrained.</p> <h1 id="discussions-2">Discussions</h1> <p>These two studies offer powerful information that dimensionality reduction could be not just a visualization tool, but a causal summary of the underlying neural connectivity and anatomical constraints, which correlates to the neural computations that neural population could implement.</p> <p>Associating Sadtler 2014 with Churchland 2012, there‚Äôs a common convergence on using matrices to explore transformations. This again reinforces my idea that perhaps group theory needs to be rigorously introduced into neursocience for ‚Ä¶ (also associate with Barack and Kraukauer ‚ÄúTwo views on the cognitive brain‚Äù, which relates <strong>computation</strong> to <strong>transformation of representations</strong> to explain cognitive phenomena)</p> <h1 id="conclusions-2">Conclusions</h1>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Learning"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Neural onstraints, both spatial and temporal]]></summary></entry></feed>