<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jasmineruixiang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasmineruixiang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-06T21:45:41+00:00</updated><id>https://jasmineruixiang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">BCI nitty-gritty (1): Equivalence (or Lack Thereof) between Block-wise and Global Z-Scoring</title><link href="https://jasmineruixiang.github.io/blog/2026/zscore/" rel="alternate" type="text/html" title="BCI nitty-gritty (1): Equivalence (or Lack Thereof) between Block-wise and Global Z-Scoring"/><published>2026-02-06T11:16:09+00:00</published><updated>2026-02-06T11:16:09+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/zscore</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/zscore/"><![CDATA[<p>This short blog provides a detailed, self-contained computational analysis of whether two z-scoring procedures applied to block-structured neural data are equivalent, and if different how.</p> <hr/> <h2 id="1-problem-setup">1] Problem setup</h2> <p>Let‚Äôs say that we have neural data organized into blocks:</p> <ul> <li>Number of blocks: \(B\)</li> <li>Each block has data matrix in shape: \((x_{i, j}^b) \in \mathbb{R}^{t \times n}, \quad b = 1, \dots, B\)</li> <li>\(t\) = number of time points (samples)</li> <li>\(n\) = number of neural features</li> </ul> <p>Z-scoring is performed <strong>feature-wise</strong>, so all derivations below consider <strong>one fixed feature</strong> (column) at a time. The argument applies independently to every feature.</p> <hr/> <h2 id="2-notation-for-a-single-feature">2] Notation for a single feature</h2> <p>For a fixed feature \(j\):</p> <ul> <li> <p>Let \(x_{i,j}^{b} \in \mathbb{R}^{1}\) denote the data at time \(i\) for the feature \(j\) in block \(b\). For the following paragraphs, I will simplify \(x_{:,j}^{b}\) into \(x_{j}^{b} \in \mathbb{R}^{t\times 1}\), and \(x_{i,:}^{b}\) into \(x_{i}^{b} \in \mathbb{R}^{1\times n}\). Naturally, \(x^b \in \mathbb{R}^{t\times n}\) The same for other data matrices. Basically, \(i\) corresponds to the time \(t\), and \(j\) to the number of neurons \(n\).</p> </li> <li> <p>Block-wise mean: \(\mu_{j}^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i, j} \in \mathbb{R}^{1}\) \(\mu ^{b} = [\mu_{1}^{b}, \cdots, \mu_{n}^{b}] \in \mathbb{R}^{1 \times n}\). Or, we could simply write \(\mu ^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i} \in \mathbb{R}^{1}\)</p> </li> <li> <p>Block-wise variance: \((\sigma_{j}^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i, j} - \mu_{j}^b)^2 \in \mathbb{R}^{1}\) \((\sigma ^{b})^2 = [(\sigma_{1}^{b})^2, \cdots, (\sigma_{n}^{b})^2] \in \mathbb{R}^{1 \times n}\) Or, we could simplify the above as \((\sigma ^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i} - \mu^b) \odot (x^{b}_{i} - \mu^b) \in \mathbb{R}^{1}\) where \(\odot\) is the Hadamard product between two vectors (or just elementwise multiplication), defined as \(x \odot y = diag(x)y = (x_i y_i)_i \in \mathbb{R}^{1\times n},\) where \(x,y \in \mathbb{R}^{1\times n}\)</p> </li> </ul> <hr/> <h2 id="3-method-a-block-wise-z-scoring-concatenate-and-then-global-z-scoring">3] Method A: Block-wise z-scoring, concatenate, and then global z-scoring</h2> <h3 id="step-3a-z-score-within-each-block">Step 3a]: Z-score within each block</h3> <p>Each block is normalized independently:</p> \[z^{b}_{i} = \frac{x^{b}_i - \mu^b}{\sigma^b} \in \mathbb{R}^{1\times n}\] <p>Notice that this is element-wise division (to not complicate the symbols, I‚Äôll use this abuse of notation for the following).</p> <p>By construction, for every block \(b\):</p> \[\frac{1}{t}\sum_{i=1}^t z^{b}_{i} = \vec{0} \in \mathbb{R}^{1\times n}, \qquad \frac{1}{t}\sum_{i=1}^t &lt;z^{b}_{i} - \vec{0}, z^{b}_{i} - \vec{0}&gt; = \vec{1} \in \mathbb{R}^{1\times n},\] <p>Consequently, each feature in each block has mean 0 and standard deviation 1. Notice that \(z^b\) observes the same notation rule as I described above.</p> <hr/> <h3 id="step-3b-concatenate-all-normalized-blocks">Step 3b]: Concatenate all normalized blocks</h3> <p>Concatenate all \(z^{b}\) into a single vector of length \(Bt\).</p> <h4 id="global-mean">Global mean</h4> \[\mu' = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B \vec{0} = \vec{0} \in \mathbb{R}^{1\times n}\] <h4 id="global-variance">Global variance</h4> \[(\sigma')^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t &lt;z^{b}_i - \vec{0}, z^{b}_i - \vec{0}&gt; \\ = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) = \frac{1}{B}\sum_{b=1}^B\vec{1} = \vec{1} \in \mathbb{R}^{1\times n}\] <hr/> <h3 id="step-3c-second-z-scoring">Step 3c]: Second z-scoring?</h3> <p>Since the concatenated data already has zero mean and unit variance, adding any other layers of z-scoring has no effect.</p> <p><strong>Final output of Method A:</strong></p> \[\boxed{z^{b} \in \mathbb{R}^{t\times n}}\] <p>for each block. So method A simply returns the block-wise standardized data.</p> <hr/> <h2 id="4-method-b-concatenate-first-then-global-z-scoring">4] Method B: Concatenate first, then global z-scoring</h2> <h3 id="step-4a-concatenate-raw-data">Step 4a]: Concatenate raw data</h3> <p>Concatenate all blocks \(x^{b}\) into a single matrix \(X \in \mathbb{R}^{Bt \times n}\).</p> <hr/> <h3 id="step-4b-compute-global-mean-and-standard-deviation">Step 4b]: Compute global mean and standard deviation</h3> \[\mu = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t x^{b}_i = \frac{1}{B}\sum_{b=1}^B \frac{1}{t}\sum_{i=1}^{t}x_i^b \\ = \frac{1}{B}\sum_{b = 1}^{B} \mu^b\] <p>The global mean is the average of block-wise means (it‚Äôs not hard to show that if each block has different samples, this average will become <em>weighted average</em> by the ratio of the amount of each block‚Äôs data to total data amount).</p> <hr/> <h3 id="step-4c-compute-global-variance">Step 4c]: Compute global variance</h3> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (x^{b}_i - \mu) \odot (x^{b}_i - \mu)\] <p>Expand the product term:</p> \[(x^{b}_i - \mu) \odot (x^{b}_i - \mu) \\ = (x^{b}_i - \mu^b + \mu^b - \mu) \odot (x^{b}_i - \mu^b + \mu^b - \mu) \\ = (x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + 2(x^{b}_i - \mu^b) \odot (\mu^b - \mu) \\ + (\mu^b - \mu) \odot (\mu^b - \mu) \\\] <p>Let‚Äôs see into each of the terms. Notice that when summing over \(i\) in \(\sigma^2\), the cross term vanishes:</p> \[\frac{1}{t}\sum_{i=1}^t ((x^{b}_i - \mu^b) \odot (\mu^b - \mu))\\ = (\frac{1}{t}\sum_{i=1}^t (x^{b}_i - \mu^b)) \odot (\mu^b - \mu)\\ = (\mu^b - \mu^b) \odot (\mu^b - \mu) \\ = \vec{0}\] <p>Thus,</p> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t((x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + (\mu^b - \mu) \odot (\mu^b - \mu)) \\ = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (\mu^b - \mu) \odot (\mu^b - \mu) \\ = \frac{1}{B}\sum_{1}^{B}\frac{1}{t}\sum_{i=1}^{t}(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\\ = \frac{1}{B}\sum_{1}^{B}(\sigma^b)^2 + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\] <p>This is a <strong>variance decomposition</strong> into:</p> <ul> <li>average within-block variance</li> <li>variance of block means (between-block variance)</li> </ul> <p>which meets our intuitive expectation (what else could it be anyway‚Ä¶)</p> <hr/> <h3 id="step-4d-global-z-scoring">Step 4d]: Global z-scoring</h3> <p>Each sample is normalized as:</p> \[y^{b}_i = \frac{x^{b}_i - \mu}{\sigma} \in \mathbb{R}^{1\times n}\] <p>Now if we rewrite the above using block-wise z-scores, since</p> <p>\(z_i^b = \frac{x_i^b - \mu^b}{\sigma^b} \rightarrow x_i^b = \sigma^bz_i^b + \mu^b\), then:</p> \[\boxed{ y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} }\] <p>which reinforces the idea that these all are just linear transformations: Transformations being linear, linear into one another.</p> <hr/> <h2 id="comparison-of-method-a-and-method-b">Comparison of Method A and Method B</h2> <p>Method A output:</p> \[z^{b}_i \in \mathbb{R}^{1\times n}\] <p>Method B output:</p> \[y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} \in \mathbb{R}^{1\times n}\] <p>For the two methods to be identical for all $b,i$, we must have:</p> \[\sigma_b = \sigma \quad \text{and} \quad \mu_b = \mu \quad \forall b\] <p>On the other hand, notice that if we concatenate all \(y^b\) and \(z^b\) together separately into \(Y, Z\), they <strong>BOTH</strong> have feature-wise 0 mean and std 1.</p> <hr/> <h2 id="final-result">Final result</h2> \[\boxed{ \begin{array}{l} \text{The two procedures are NOT equivalent in general, even though} \\ \text{both yield global mean } 0 \text{ and std } 1 \text{ after transformations}. \end{array} }\] <p>They are equivalent <strong>if and only if</strong> every block already has identical feature-wise means and variances.</p> <ul> <li><strong>Method A</strong> removes all block-level mean and variance differences before concatenation.</li> <li><strong>Method B</strong> preserves block-level differences and normalizes relative to the pooled distribution.</li> </ul> <p>Block-wise z-scoring and global z-scoring <strong>do not commute</strong>. . These choices encode different assumptions about whether block identity (e.g., session, subject, condition) should be preserved or discarded. Our choice should be driven by whether block-to-block variability is meaningful signal or nuisance variability in your analysis.</p> <p>Fun quesitons:</p> <ul> <li>1] What if block size \(t\) is not the same across all blocks?</li> <li>2] What are other (useful/effective) ways of normalization which also return a fixed mean/std (0/1, e.g.)?</li> </ul> <p>Practical question: In practice, how much do the statistics from these two methods actually differ? How should we interpret such differences?</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Brain Computer Interface"/><summary type="html"><![CDATA[Zscoring, block vs session level comparisons]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(3): Subspace geometry</title><link href="https://jasmineruixiang.github.io/blog/2026/subspace/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(3): Subspace geometry"/><published>2026-02-03T21:49:38+00:00</published><updated>2026-02-03T21:49:38+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/subspace</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/subspace/"><![CDATA[<h2 id="setup">Setup</h2> <p>Suppose we compute PCA on two datasets (e.g., train vs.\ test), and we keep the top-$r$ principal components. Let</p> \[U_{\text{train}} \in \mathbb{R}^{d \times r}, \qquad U_{\text{test}} \in \mathbb{R}^{d \times r},\] <p>where each matrix has <strong>orthonormal columns</strong> (so each is a basis for an $r$-dimensional subspace of $\mathbb{R}^d$).</p> <p>Our first obvious intuition might be to define the <strong>subspace overlap matrix</strong> as the following:</p> \[S = U_{\text{train}}^{\top} U_{\text{test}} \in \mathbb{R}^{r \times r}.\] <p>So, each entry is a dot product between basis vectors:</p> \[S_{ij} = u_i^{(\text{train})} \cdot u_j^{(\text{test})}.\] <p>At first glance, this looks like a direct ‚Äúbasis alignment‚Äù comparison, which is exactly what we aim for. How else could we characterize subspace change other than looking at pairs-wise relationships among two sets of basis vectors? Well, there‚Äôre a few caveats‚Ä¶</p> <hr/> <h2 id="1-why-basis-by-basis-alignment-is-unstable">1] Why basis-by-basis alignment is unstable</h2> <p>Comparing PCA vectors one-by-one (e.g., ‚ÄúPC1 vs.\ PC1‚Äù) is unstable because PCA eigenvectors are <strong>not uniquely defined</strong> in two common cases. The following problems might pop up ‚Äî</p> <h3 id="i-sign-flips">(i) Sign flips</h3> <p>If \(u\) is an eigenvector, then \(-u\) is also an eigenvector.</p> <p>So dot products can flip sign even when the <em>subspace is identical</em>.</p> <h3 id="ii-degenerate--near-degenerate-eigenvalues-rotation-inside-the-subspace">(ii) Degenerate / near-degenerate eigenvalues (rotation inside the subspace)</h3> <p>If</p> \[\lambda_i \approx \lambda_{i+1},\] <p>then the corresponding principal directions inside the 2D span can rotate dramatically under tiny perturbations (noise, finite-sample effects, etc.).</p> <p>This means that even if the <em>span</em> is essentially the same, the individual vectors $u_i$ can change a lot.<br/> So comparing ‚ÄúPC1 to PC1‚Äù is not meaningful.</p> <hr/> <h2 id="2-what-singular-values-do-that-dot-products-dont">2] What singular values do that dot products don‚Äôt</h2> <p>The matrix</p> \[S = U_{\text{train}}^{\top} U_{\text{test}}\] <p>depends on the <em>chosen bases</em> inside each subspace. If we change bases within either subspace via orthogonal transformations:</p> \[U_{\text{train}} \to U_{\text{train}} R_1, \qquad U_{\text{test}} \to U_{\text{test}} R_2,\] <p>where $R_1, R_2 \in \mathbb{R}^{r \times r}$ are orthogonal (including sign flips as a special case), then</p> \[S \to (U_{\text{train}}R_1)^{\top}(U_{\text{test}}R_2) = R_1^{\top} S R_2.\] <p>So the <em>entries</em> of $S$ can change wildly.</p> <h3 id="key-fact-invariance">Key fact (invariance)</h3> <blockquote> <p>The <strong>singular values</strong> of \(S\) are invariant under left/right orthogonal rotations:</p> </blockquote> <ul> <li>Left-multiplying by an orthogonal matrix does not change singular values.</li> <li>Right-multiplying by an orthogonal matrix does not change singular values.</li> </ul> <p>Therefore, even if PCA ‚Äúrelabels,‚Äù flips signs, or rotates the basis vectors within the subspace, the <strong>singular values remain unchanged</strong>.</p> <p>This means singular values capture a property of the <strong>subspaces</strong>, not of the particular eigenvectors chosen.</p> <hr/> <h2 id="3-geometric-meaning-principal-angles">3] Geometric meaning: principal angles</h2> <p>Take the SVD:</p> \[S = Q \Sigma R^{\top},\] <p>where</p> \[\Sigma = \mathrm{diag}(\sigma_1,\dots,\sigma_r).\] <p>A fundamental result is:</p> \[\sigma_i = \cos(\theta_i),\] <p>where $\theta_i$ are the <strong>principal angles</strong> between the two $r$-dimensional subspaces.</p> <p>Interpretation:</p> <ul> <li>$\theta_i = 0 \implies$ perfectly aligned direction exists (since $\cos(\theta_i)=1$)</li> <li>$\theta_i = 90^\circ \implies$ orthogonal direction (since $\cos(\theta_i)=0$)</li> </ul> <p>So the singular values summarize <em>how much overlap</em> the two subspaces have along their best-aligned directions.</p> <hr/> <h2 id="4-why-this-is-the-stable-comparison">4] Why this is the ‚Äústable‚Äù comparison</h2> <p>Think of $U_{\text{train}}$ and $U_{\text{test}}$ as <strong>arbitrary coordinate systems</strong> inside their respective subspaces.</p> <p>A meaningful comparison should ignore that arbitrariness.</p> <p>Principal angles / singular values do exactly this: they compute the <strong>best possible matching</strong> between directions in the two subspaces.</p> <p>Instead of comparing ‚ÄúPC1 $\leftrightarrow$ PC1,‚Äù we solve an optimal alignment problem:</p> \[\max_{\|a\|=\|b\|=1} a^{\top}\bigl(U_{\text{train}}^{\top}U_{\text{test}}\bigr)b,\] <p>and the sequence of best matches yields</p> \[\sigma_1,\sigma_2,\dots\] <p>as the strengths of alignment along the best-aligned directions.</p> <hr/> <h2 id="5-tiny-example-intuition-2d-case">5] Tiny example intuition (2D case)</h2> <p>Suppose both subspaces are actually the same 2D plane in $\mathbb{R}^d$.</p> <p>You could pick:</p> <ul> <li>$U_{\text{train}}$ = standard basis in that plane</li> <li>$U_{\text{test}}$ = same plane but rotated by $45^\circ$ inside it</li> </ul> <p>Then $S$ might look like a rotation matrix:</p> \[S= \begin{pmatrix} \cos 45^\circ &amp; -\sin 45^\circ \\ \sin 45^\circ &amp; \cos 45^\circ \end{pmatrix}.\] <p>The entries are not the identity, so basis-by-basis dot products look ‚Äúnot aligned.‚Äù</p> <p>But the singular values of a rotation matrix are both $1$.</p> <p>So singular values correctly say: <strong>the subspaces are identical</strong>.</p> <p>That‚Äôs the whole point.</p> <hr/> <h2 id="bottom-line">Bottom line</h2> <p>We use singular values of</p> \[U_{\text{train}}^{\top}U_{\text{test}}\] <p>because:</p> <ul> <li>‚úÖ they are invariant to sign flips / rotations / re-ordering of PCA vectors inside the subspace</li> <li>‚úÖ they define principal angles, which are a true subspace-to-subspace comparison</li> <li>‚úÖ they give a stable measure of drift even when eigenvectors are not uniquely defined</li> </ul> <hr/> <h2 id="a-sidenote-connection-to-projection-distance">A sidenote: Connection to projection distance</h2> <p>Let $P_{\text{train}}$ and $P_{\text{test}}$ be the orthogonal projection matrices onto the two subspaces:</p> \[P_{\text{train}} = U_{\text{train}}U_{\text{train}}^{\top}, \qquad P_{\text{test}} = U_{\text{test}}U_{\text{test}}^{\top}.\] <p>Then one can show the Frobenius-distance relationship:</p> \[\|P_{\text{train}} - P_{\text{test}}\|_F^2 = 2r - 2\|U_{\text{train}}^{\top}U_{\text{test}}\|_F^2 = 2\sum_{i=1}^r \sin^2(\theta_i).\]]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Subspace Geometry and Computation]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (4): Connections (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (4): Connections (in progress)"/><published>2026-01-15T15:48:02+00:00</published><updated>2026-01-15T15:48:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(4)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/"><![CDATA[<p>This is episode 4 on the smooth manifold series. Today we will be exploring more on tangent vectors, and another key concept related to tangent spaces for different tangent planes: connections.</p> <h2 id="intuition-directional-derivatives">Intuition: Directional Derivatives</h2> <p>üåê The Connection: Bridging Derivatives from $\mathbb{R}^3$ to Curved Manifolds The concept of a connection is the necessary tool that allows us to perform differential calculus on curved spaces (manifolds), such as the surface of a sphere. It generalizes the familiar idea of the directional derivative from flat Euclidean space ($\mathbb{R}^3$).</p> <ol> <li>Directional Derivatives in Euclidean Space ($\mathbb{R}^3$) In $\mathbb{R}^3$ with Cartesian coordinates $(x, y, z)$, the directional derivative provides a simple way to measure change. The basis vectors $\left{ \mathbf{i}, \mathbf{j}, \mathbf{k} \right}$ (or the equivalent operators $\left{ \frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z} \right}$) are constant, allowing us to define derivatives simply as component-wise partial derivatives. Let $p=(1, 2, 0)$ be a point, $X=(y, -x, 3x)$ be the direction vector field, and $V=(xz, y^2, -2x)$ be a vector field. A. Derivative of a Scalar Function ($D_X f$) This is the directional derivative of a smooth scalar function $f$ in the direction $X$.</li> </ol> <p>Perspective Formula Example Result for f(x,y,z)=xy2+z at p Vector-Based (Calculus) $D_X f = \nabla f \cdot X$ $D_X f(p) = \langle 4, 4, 1 \rangle \cdot \langle 2, -1, 3 \rangle = \mathbf{7}$ Point Derivation (Geometry) $X[f] = X^i \frac{\partial f}{\partial x^i}$ $Xf = (y^3 - 2x^2y + 3x)\big</p> <p>This confirms that in differential geometry, a tangent vector $X$ is rigorously defined as a point derivation‚Äîan operator that mimics the directional derivative by satisfying the Leibniz rule. B. Derivative of a Vector Field ($D_X V$) This derivative measures how the vector field $V$ changes as we move in the direction $X$. In $\mathbb{R}^3$, this is calculated by taking the directional derivative of each component of $V$. Using the vector fields $X$ and $V$: The $k$-th component of the resulting vector $D_X V$ is $(D_X V)^k = \sum_{i} X^i \frac{\partial V^k}{\partial x^i}$. Component 1 (i.e., $k=1$): $(D_X V)^1 = yz + 3x^2$ Component 2 (i.e., $k=2$): $(D_X V)^2 = -2xy$ Component 3 (i.e., $k=3$): $(D_X V)^3 = -2y$ Evaluating at $p=(1, 2, 0)$ gives:</p> <p>\(D_X V(p) = \langle 3, -4, -4 \rangle\)</p> <ol> <li>The General Connection: The Covariant Derivative ($\nabla_X V$) The formula for $D_X V$ fails on a curved manifold $M$ because the tangent spaces $T_p M$ and $T_q M$ at nearby points $p$ and $q$ are distinct. We cannot simply subtract the vector $V(p)$ from $V(q)$. A Connection ($\nabla$) is the rule that provides the necessary ‚Äúcorrection‚Äù to define the derivative intrinsically on $M$. The resulting derivative is called the covariant derivative $\nabla_X V$. A. Definition and Axioms The connection is an operator $\nabla: C^{\infty}(M) \times C^{\infty}(M) \to C^{\infty}(M)$ that maps two vector fields, $X$ and $V$, to a new vector field $\nabla_X V$, satisfying: Linearity over Functions in $X$: $\nabla_{fX} V = f \nabla_X V$ Linearity in $V$: $\nabla_X (aV + bW) = a \nabla_X V + b \nabla_X W$ Leibniz Rule: $\nabla_X (fV) = (Xf) V + f \nabla_X V$ (where $Xf$ is the directional derivative of $f$) B. The Coordinate Form and Christoffel Symbols In local coordinates, the covariant derivative $\nabla_X V$ is defined using the Christoffel symbols ($\Gamma^k_{ij}$), which represent the rate of change of the coordinate basis vectors $\left{ \frac{\partial}{\partial x^i} \right}$:</li> </ol> <p>\((\nabla_X V)^k = \underbrace{X^i \frac{\partial V^k}{\partial x^i}}_{\text{I. Flat-Space Derivative Term}} + \underbrace{X^i \Gamma^k_{ij} V^j}_{\text{II. Curvature Correction Term}}\) The Christoffel symbols $\Gamma^k_{ij}$ are defined by the action of the connection on the basis vectors:</p> <p>\(\nabla_{\frac{\partial}{\partial x^i}} \frac{\partial}{\partial x^j} = \sum_{k} \Gamma^k_{ij} \frac{\partial}{\partial x^k}\) Difference from Euclidean Case: In $\mathbb{R}^3$ with Cartesian coordinates, $\Gamma^k_{ij} = 0$, and the second term vanishes, resulting in $\nabla_X V = D_X V$. On a curved manifold, $\Gamma^k_{ij} \neq 0$, and the correction term is essential.</p> <ol> <li>The Levi-Civita Connection In Riemannian Geometry, a Riemannian metric $g$ is introduced to measure lengths and angles. The Levi-Civita Connection is the unique connection that respects this metric structure. It is defined by two crucial properties: Metric Compatibility: The connection must preserve the metric $g$ under parallel transport.</li> </ol> <p>\(X(g(V, W)) = g(\nabla_X V, W) + g(V, \nabla_X W)\) Zero Torsion: The connection must satisfy:</p> \[\nabla_X Y - \nabla_Y X = [X, Y]\] <p>where $[X, Y]$ is the Lie bracket. The Christoffel symbols of the Levi-Civita Connection are thus entirely determined by the components of the metric $g_{ij}$ and their first derivatives:</p> \[\Gamma^k_{ij} = \frac{1}{2} g^{k\ell} \left( \frac{\partial g_{j\ell}}{\partial x^i} + \frac{\partial g_{i\ell}}{\partial x^j} - \frac{\partial g_{ij}}{\partial x^\ell} \right)\] <h2 id="connections">Connections</h2> <h2 id="christoffel-symbols">Christoffel Symbols</h2> <h2 id="levi-civita-connections">Levi-Civita Connections</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Exploration of Connection]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)"/><published>2026-01-14T20:22:25+00:00</published><updated>2026-01-14T20:22:25+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(3)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/"><![CDATA[<p>This is episode 3 on the smooth manifold series. Today we will be diving into the properties of maps between manifolds. I will first summarize how to understand and compute the differential of a smooth map between manifolds, both abstractly and concretely, culminating in the matrix-valued example<br/> \(F(A) = A^\top A.\)</p> <hr/> <h2 id="1-differential-canonical-definition">1] Differential: canonical definition</h2> <h3 id="11-differential-of-a-smooth-map-intrinsic-definition">1.1 Differential of a Smooth Map (Intrinsic Definition)</h3> <p>Let \(F : M \to N\) be a smooth map between smooth manifolds.</p> <p>For any point $ p \in M $, the <strong>differential</strong> is a linear map which is a <strong>pushforward of derivations</strong>. \(dF_p : T_p M \longrightarrow T_{F(p)} N.\)</p> <h3 id="derivation-based-definition">Derivation-based definition</h3> <p>If $ v \in T_p M $ is a tangent vector viewed as a derivation, then \((dF_p v)(g) := v(g \circ F), \qquad g \in C^\infty(N).\)</p> <p>This definition is <strong>coordinate-free</strong>.</p> <p>It might appear at first both unnecessarily abstract and underestimated as to its computation. We might claim that it is essentially just a Jacobian matrix in local coordinates. However, the essence of this concept resides on its definition to be conceptually a coordinate-independent linear map between tangent spaces.</p> <hr/> <h3 id="12-coordinate-representation-and-the-jacobian">1.2 Coordinate Representation and the Jacobian</h3> <p>To compute $ dF_p $ in practice:</p> <ol> <li>Choose a chart $ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>Choose a chart $ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ol> <p>Define the coordinate expression: \(\tilde F = \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Then: \(\boxed{ dF_p \;\text{is represented by}\; D\tilde F(\varphi(p)) }\)</p> <p>That is, <strong>the Jacobian matrix is the coordinate representation of the differential</strong>.</p> <blockquote> <p>The Jacobian depends on coordinates; the linear map $ dF_p $ does not.</p> </blockquote> <hr/> <h3 id="13-why-this-is-not-just-the-jacobian">1.3 Why this is not ‚Äújust‚Äù the Jacobian</h3> <p>The Jacobian depends on coordinates;<br/> $ dF_p $ does not.</p> <p>More precisely:</p> <ul> <li>$ dF_p $ is a <strong>geometric linear map</strong></li> <li>The Jacobian is a <strong>matrix representation</strong> of that map in chosen bases: \(\frac{\partial \bigl(\psi^1 \circ F,\;\dots,\;\psi^n \circ F\bigr)} {\partial \bigl(x^1,\;\dots,\;x^m\bigr)}\)</li> </ul> <p>If you change charts, the matrix changes by:</p> \[\boxed{ J_{\text{new}} = D\psi\,\cdot\, J_{\text{old}} \,\cdot\, (D\varphi^{-1}) }\] <p>but the underlying linear map $ dF_p $ stays the same.</p> <hr/> <h2 id="2-differential-alternative-interpretation">2] Differential: alternative interpretation</h2> <h3 id="21-curve-based-definition">2.1 Curve-based definition</h3> <p>This is also coordinate-free:</p> <p>If<br/> \(\gamma : (-\varepsilon,\varepsilon) \to M\) is a smooth curve with \(\gamma(0) = p \quad \text{and} \quad \gamma'(0) = v \in T_p M,\) then \(\boxed{ dF_p(v) = (F \circ \gamma)'(0) \in T_{F(p)} N. }\)</p> <p>No coordinates anywhere. This viewpoint is often the most intuitive and is fully equivalent to the derivation definition.</p> <hr/> <h3 id="22-curves-in-local-coordinates">2.2 Curves in local coordinates</h3> <p>Choose charts:</p> <ul> <li>$ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>$ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ul> <p>Define the coordinate representation: \(\tilde F := \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Now define the coordinate curve: \(\tilde\gamma := \varphi \circ \gamma : (-\varepsilon,\varepsilon) \to \mathbb{R}^m.\)</p> <p>Consequently, in coordinates the statement is the following:</p> \[\boxed{ D\tilde F(\varphi(p)) \cdot \tilde\gamma'(0) = D(\psi \circ F \circ \varphi^{-1})(\varphi(p)) \cdot (\varphi \circ \gamma)'(0). }\] <p>In the coordinate formula, \(\gamma'(0) \quad \text{really means} \quad \tilde\gamma'(0) = (\varphi \circ \gamma)'(0).\)</p> <p>Here‚Äôs a diagram that makes everything explicit</p> \[\begin{array}{ccc} T_p M &amp; \xrightarrow{dF_p} &amp; T_{F(p)} N \\ \downarrow d\varphi_p &amp; &amp; \uparrow d\psi^{-1}_{\psi(F(p))} \\ \mathbb{R}^m &amp; \xrightarrow{D\tilde F(\varphi(p))} &amp; \mathbb{R}^n \end{array}\] <p>Thus:</p> <ul> <li>$ \gamma‚Äô(0) $ lives in $ T_p M $</li> <li>$ (\varphi \circ \gamma)‚Äô(0) = d\varphi_p(\gamma‚Äô(0)) \in \mathbb{R}^m $</li> <li>$ D\tilde F(\varphi(p)) $ acts on that coordinate vector</li> </ul> <hr/> <h3 id="23-sidenote-graph-differential">2.3 Sidenote: Graph Differential</h3> <p>The <strong>graph</strong> of $ F $ is \(\Gamma_F = \{ (p, F(p)) \mid p \in M \} \subset M \times N.\)</p> <p>Define the graph map: \(\Phi : M \to M \times N, \quad \Phi(p) = (p, F(p)).\)</p> <p>Its differential is: \(\boxed{ d\Phi_p(v) = (v, dF_p(v)). }\)</p> <p>This is what is often called the <strong>graph differential</strong>.</p> <hr/> <h2 id="3-special-case-maps-between-vector-spaces">3] Special Case: Maps Between Vector Spaces</h2> <h3 id="31-a-great-simplification">3.1 A great simplification</h3> <p>If $ M = \mathbb{R}^m $, $ N = \mathbb{R}^n $, then: \(T_p M \cong \mathbb{R}^m, \quad T_{F(p)} N \cong \mathbb{R}^n.\)</p> <p>In this case:</p> <ul> <li>$ dF_p $ is a linear map $ \mathbb{R}^m \to \mathbb{R}^n $</li> <li>Its matrix is exactly the <strong>Jacobian matrix</strong></li> <li>$ dF_p(H) $ coincides with the <strong>Fr√©chet / directional derivative</strong></li> </ul> <hr/> <h3 id="32-worked-example--fa--atop-a-">3.2 Worked Example: $ F(A) = A^\top A $</h3> <p>Let \(F : \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}, \quad F(A) = A^\top A.\)</p> <p>Since $ \mathbb{R}^{n \times n} $ is a vector space, \(T_A(\mathbb{R}^{n \times n}) \cong \mathbb{R}^{n \times n}.\)</p> <h4 id="321-direct-computation">3.2.1 Direct computation</h4> <p>There are several ways to compute the differential. The most straight-forward method is to do the following (as you might imagine):</p> <p>For \(H \in T_A(\mathbb{R}^{n \times n})\), \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} (A+tH)^\top(A+tH).\)</p> <p>Expanding: \((A+tH)^\top(A+tH) = A^\top A + t(H^\top A + A^\top H) + t^2 H^\top H.\)</p> <p>Thus: \(\boxed{ dF_A(H) = H^\top A + A^\top H. }\)</p> <h4 id="322-curve-based-computation">3.2.2 Curve-based computation</h4> <p>Let \(A(t)\) be a smooth curve with: \(A(0)=A, \quad A'(0)=H.\)</p> <p>Then: \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} A(t)^\top A(t) = H^\top A + A^\top H.\)</p> <p>This makes it clear that the definition of \(dF_A(H)\) is <strong>coordinate-free</strong>.</p> <p>Sidenote: if we restrict \(A\) to symmetric or SPD matrices, what will we see? Or what if we connect this to Riemannian geometry on \(GL(n)\) or \(SPD(n)\)? We‚Äôll come back to this later when we discuss Lie group and Lie algebra.</p> <hr/> <h2 id="4-back-to-classical-regular-surfaces-parametrizations-immersions">4] Back to classical regular surfaces (parametrizations, immersions)</h2> <h3 id="the-question-in-do-carmo-diffgeom">The Question in do Carmo DiffGeom</h3> <p>In do Carmo‚Äôs definition of a <strong>regular surface</strong> in \(\mathbb{R}^3\), a coordinate map \(X : U \subset \mathbb{R}^2 \to \mathbb{R}^3\) is required to satisfy two conditions:</p> <ol> <li>\(X\) is a <strong>differentiable homeomorphism</strong> onto its image.</li> <li>The differential \(dX_p\) is <strong>injective</strong> at every point $p \in U$.</li> </ol> <p>Since \(X\) maps from \(\mathbb{R}^2\) to \(\mathbb{R}^3\), its differential can never be surjective, so injectivity (rank 2) is the meaningful requirement.</p> <p>A natural question arises:</p> <blockquote> <p>If \(X\) is already a differentiable homeomorphism, isn‚Äôt its differential automatically injective?</p> </blockquote> <p>The answer is <strong>no</strong>.</p> <p>Well, to make it explicit, let‚Äôs figure out first what a differentiable homeomorphism actually gives us: If \(X : U \to \mathbb{R}^3\) is a differentiable homeomorphism onto its image, then:</p> <ul> <li>\(X\) is <strong>continuous and injective</strong></li> <li>\(X^{-1}\) is <strong>continuous</strong> (but <em>not</em> necessarily differentiable)</li> <li>Topologically, \(X(U)\) looks like a 2‚Äëdimensional surface</li> </ul> <p>This is a <strong>topological</strong> statement plus differentiability of $X$. It controls <em>points</em>, but says nothing about what happens to <em>directions</em>. Crucially, differentiability of the inverse is <em>not</em> assumed.</p> <p>Fine, but then why injectivity of the differential is not automatically assured? Notice that the differential</p> \[dX_p : \mathbb{R}^2 \to \mathbb{R}^3\] <p>being injective means it has <strong>rank 2</strong> meaning no tangent direction is collapsed. A map can be:</p> <ul> <li>injective,</li> <li>continuous with continuous inverse,</li> <li>differentiable,</li> </ul> <p>and <em>still</em> have rank drop somewhere. Let me give a concrete example:</p> <p>Consider \(X(u,v) = (u^3, v, 0).\)</p> <p>Properties of this map:</p> <ul> <li>It is <strong>injective</strong></li> <li>It is a <strong>homeomorphism onto its image</strong></li> <li>It is differentiable everywhere</li> </ul> <p>However, its differential is \(dX_{(u,v)} = \begin{pmatrix} 3u^2 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix}\)</p> <p>At \($u = 0\), this matrix has <strong>rank 1</strong>, not 2. One tangent direction is squashed. Therefore, this map is <strong>not an immersion</strong>, and it does <strong>not</strong> define a regular surface in do Carmo‚Äôs sense.</p> <hr/> <p>However, you might observe here a seemingly apparent paradox: ‚ÄúBut the image is <strong>Just</strong> a plane!‚Äù</p> <p>Well, the image of \(X(u,v) = (u^3, v, 0)\) is \({(x,y,0) : x,y \in \mathbb{R}},\) which is the entire (xy)-plane indeed. As a <strong>subset</strong> of \(\mathbb{R}^3\), this plane is perfectly flat, so one might expect its tangent plane at every point to be the whole plane. So then why does the tangent collapse under this map?</p> <hr/> <p>The key point here is that here are <strong>two distinct notions</strong> at play:</p> <ol> <li>The tangent plane of a <strong>subset</strong> of \(\mathbb{R}^3\)</li> <li>The tangent plane <strong>defined by a parametrization</strong></li> </ol> <p>In do Carmo‚Äôs approach, tangent planes are defined <em>via parametrizations</em>. The tangent plane at a point is \(T_pS = \operatorname{span}{X_u(p), X_v(p)}.\)</p> <p>For the map above: \(X_u = (3u^2, 0, 0), \quad X_v = (0,1,0).\)</p> <p>At (u=0): \(X_u(0,v) = (0,0,0), \quad X_v(0,v) = (0,1,0),\) so the span is <strong>1‚Äëdimensional</strong>.</p> <p>This means:</p> <blockquote> <p>The parametrization fails to distinguish two independent directions in the parameter domain.</p> </blockquote> <p>Geometrically, the $u$-direction has been crushed.</p> <hr/> <p>But does the plane still have a 2D tangent plane?</p> <p>Yes ‚Äî but <strong>not via this parametrization</strong>.</p> <p>If instead we parametrize the same plane by \(Y(s,t) = (s,t,0),\) then \(dY = \begin{pmatrix} 1 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix},\) which has rank 2 everywhere.</p> <p>So the <em>same subset</em> becomes a <strong>regular surface</strong> under a different map.</p> <hr/> <p>Finally, what does this meana conceptually? The key lesson is:</p> <blockquote> <p><strong>Regularity is not a property of the subset alone ‚Äî it is a property of the subset together with its smooth structure.</strong></p> </blockquote> <p>Different parametrizations can induce:</p> <ul> <li>a <strong>good</strong> smooth structure (immersion)</li> <li>or a <strong>bad</strong> one (rank collapse)</li> </ul> <p>This is why do Carmo requires that <strong>there exists</strong> a local parametrization with injective differential.</p> <hr/> <p>In other words, if you ask ‚ÄúDoes that mean there might exist other maps which make the plane a regular surface?‚Äù, then the answer is Yes ‚Äî absolutely. The plane <em>is</em> a regular surface because such maps exist.</p> <p>Behind that is another question: ‚ÄúDoes immersion entirely depend on which maps we pick?‚Äù Yes!</p> <ul> <li><strong>Immersion is a property of the map</strong>, not of the set.</li> <li>A <strong>regular surface</strong> is a set for which <em>good immersions exist everywhere</em>.</li> </ul> <p>In this seense, do Carmo does separate the conditions, being deliberately modular:</p> <ol> <li><strong>Homeomorphism</strong> ‚Üí good topology (no self‚Äëintersections)</li> <li><strong>Injective differential</strong> ‚Üí good differential geometry</li> </ol> <p>whether neither condition implies the other.</p> <p>Finally, some of key points:</p> <ul> <li>The same subset of \(\mathbb{R}^3\) can support <strong>many different smooth structures</strong></li> <li>Differential geometry only works once a smooth structure is fixed</li> <li>Parametrizations are how do Carmo <em>builds</em> that structure</li> </ul> <p>This is why modern texts often say:</p> <blockquote> <p>‚ÄúA surface is a 2‚Äëdimensional smooth manifold embedded in $$\mathbb{R}^3$.‚Äù</p> </blockquote> <p>Do Carmo reaches this notion <strong>from parametrizations upward</strong>, rather than assuming it at the start.</p> <hr/> <blockquote> <p><strong>Topology sees points.</strong> <strong>Differential geometry sees directions.</strong></p> </blockquote> <p>A homeomorphism controls points. Injectivity of the differential controls directions.</p> <p>You need <strong>both</strong> to get a regular surface.</p> <h2 id="5-association-with-inverse-function-theorem">5] Association with inverse function theorem</h2> <h2 id="6-immersion-submersion">6] Immersion, submersion</h2> <p>Immersion basics</p> <p>Here I want to emphasize one key fact that:</p> <blockquote> <p>immersion imply a nonzero determinant in coordinates</p> </blockquote> <p>Why? Again, let‚Äôs recall that immersion means the differential is injective Let<br/> \(\phi:M^m\to N^n,\qquad m\le n\) and let $p\in M$.<br/> Saying <strong>(\phi) is an immersion at (p)</strong> means the differential \(d\phi_p:T_pM\to T_{\phi(p)}N\) is <strong>injective</strong>.</p> <p>Equivalently (in linear algebra language):<br/> \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Now, if write it in local coordinates ‚Üí Jacobian matrix has rank \(m\) Pick coordinate charts:</p> <ul> <li>on \(M\): \(x=(x^1,\dots,x^m)\) around \(p\)</li> <li>on \(N\): \(y=(y^1,\dots,y^n)\) around \(\phi(p)\)</li> </ul> <p>Then locally \(\phi\) looks like a smooth map between Euclidean spaces: \(y^a = \phi^a(x^1,\dots,x^m),\qquad a=1,\dots,n.\)</p> <p>Its differential in these coordinates is represented by the <strong>Jacobian matrix</strong> \(J(p)=\left(\frac{\partial \phi^a}{\partial x^i}(p)\right)\) which is an (n\times m) matrix.</p> <p>The immersion condition says: \(\operatorname{rank}(J(p))=m.\)</p> <p>So the columns of \(J(p)\) are linearly independent.</p> <p>Now we use a standard linear algebra fact:</p> <blockquote> <p>An \(n\times m\) matrix has rank \(m\) <strong>iff</strong> there exists an \(m\times m\) submatrix (choose \(m\) rows) whose determinant is nonzero.</p> </blockquote> <p>Why?</p> <ul> <li>If <strong>every</strong> \(m\times m\) minor determinant were zero, then <strong>every</strong> set of $m$ rows would be linearly dependent, so the rank would be \(&lt;m\).</li> <li>Since the rank is \(m\), at least one choice of $m$ rows gives an invertible \(m\times m\) matrix.</li> </ul> <p>Concretely: there exist indices \(1\le a_1&lt;\cdots&lt;a_m\le n\) such that the matrix \(\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)_{\alpha,i}\) has \(\det\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)\neq 0.\)</p> <p>If we now <strong>rename/reorder the target coordinates</strong> so that those special indices become \(1,\dots,m\), then we can assume:</p> \[\det\left(\frac{\partial (\phi^1,\dots,\phi^m)}{\partial (x^1,\dots,x^m)}(p)\right)\neq 0.\] <p>That‚Äôs exactly the statement: in coordinates (after renumbering if needed), an immersion gives a nonzero determinant of an \(m\times m\) Jacobian block.</p> <p>In one line summary: an immersion means \(\phi\) ‚Äúdoesn‚Äôt collapse any tangent directions,‚Äù so locally you can find $m$ coordinate functions of \(\phi\) that vary independently ‚Äî and ‚Äúvary independently‚Äù is exactly ‚ÄúJacobian block has nonzero determinant.‚Äù</p> <hr/> <h4 id="side-note-connection-to-the-constant-rank-theorem--local-normal-form-of-an-immersion">Side note: Connection to the Constant Rank Theorem / local normal form of an immersion?</h4> <p>Remember that Constant Rank Theorem (specialized to immersions) says: Let \(\phi:M^m\to N^n\) be smooth, and suppose \(\phi\) is an <strong>immersion at \(p\)</strong>.<br/> That means \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Then the constant rank theorem says:</p> <blockquote> <p>There exist coordinate charts<br/> \((U,x)\ \text{around }p,\qquad (V,y)\ \text{around }\phi(p)\) such that in these coordinates the map becomes \(y\circ \phi\circ x^{-1}(u^1,\dots,u^m) \;=\; (u^1,\dots,u^m,0,\dots,0).\)</p> </blockquote> <p>So locally, \(\phi\) looks like the <strong>standard inclusion</strong> \(\mathbb{R}^m \hookrightarrow \mathbb{R}^n,\qquad u\mapsto (u,0).\)</p> <p>That is the precise geometric meaning of ‚Äúimmersion.‚Äù</p> <p>In these special coordinates, \(\phi^1(u)=u^1,\;\dots,\;\phi^m(u)=u^m,\qquad \phi^{m+1}(u)=0,\dots,\phi^n(u)=0.\)</p> <p>So the Jacobian matrix is literally \(J= \begin{pmatrix} I_m\\ 0 \end{pmatrix}\) (an \(n\times m\) matrix).</p> <p>Now look at the top \(m\times m\) block: \(\frac{\partial(\phi^1,\dots,\phi^m)}{\partial(u^1,\dots,u^m)} = I_m,\) so \(\det(I_m)=1\neq 0.\)</p> <p>That‚Äôs exactly the coordinate statement.</p> <p>How this matches the ‚Äúminor is nonzero‚Äù argument? Before using the constant rank theorem, we only know:</p> <ul> <li>\(J(p)\) has rank \(m\)</li> <li>therefore some \(m\times m\) minor determinant is nonzero</li> </ul> <p>Then the constant rank theorem tells us that we can actually <strong>choose coordinates</strong> so that the ‚Äúgood minor‚Äù becomes the <em>first</em> \(m\) coordinates, and the map becomes \($(u,0)\).</p> <p>So:</p> <ul> <li><strong>Linear algebra fact:</strong> full rank \(\Rightarrow\) some minor \(\neq 0\)</li> <li><strong>Constant rank theorem:</strong> we can change coordinates to make that minor the obvious identity matrix.</li> </ul> <p>The geometric picture (why \((u,0)\) is the right normal form) is that an immersion means \(\phi\) ‚Äúinjects tangent vectors,‚Äù so locally \(\phi(U)\subset N\) is an $m-dimensional ‚Äúsheet‚Äù sitting inside an n-dimensional space. In good coordinates on N, that sheet looks like:</p> \[\{(y^1,\dots,y^n): y^{m+1}=\cdots=y^n=0\},\] <p>i.e. an embedded copy of \(\mathbb{R}^m\\).</p> <p>So locally, \(\phi\) is just a parametrization of that sheet.</p> <h2 id="7-embedding">7] Embedding</h2> <h3 id="72-a-key-difference-between-embedding-and-immersion">7.2 A key difference between embedding and immersion</h3> <p><strong>‚Äúno self-intersections‚Äù is one of the key geometric consequences of being <em>embedded</em></strong> (as opposed to merely <em>immersed</em>). As stated above, if a manifold \(M\) is <strong>embedded</strong> in \(\mathbb{R}^k\), it sits inside \(\mathbb{R}^k\) as a ‚Äúnice subset,‚Äù like a surface you could physically draw without crossing itself. More formally, an <strong>embedding</strong> \(F: M \to \mathbb{R}^k\) means:</p> <ol> <li>\(F\) is a <strong>smooth immersion</strong> (its differential is injective everywhere), and</li> <li>\(F\) is a <strong>homeomorphism onto its image</strong> \(F(M)\) (with the subspace topology).</li> </ol> <p>That second condition is exactly what rules out the classic ‚Äúself-crossing‚Äù pathology. If the image ‚Äúintersects itself‚Äù in the sense that two different points \(p\neq q\in M\) map to the same point in \(\mathbb{R}^k\), i.e. \(F(p)=F(q),\) then \(F\) is <strong>not injective</strong>, so it can‚Äôt be an embedding.</p> <p>So: <strong>an embedded submanifold cannot cross itself as a set in \(\mathbb{R}^k\)</strong>.</p> <p>On the other hand, an <strong>immersion</strong> can look like a manifold with self-crossings in \(\mathbb{R}^k\). An example will be the ‚Äúfigure-eight curve‚Äù in \(\mathbb{R}^2\) can be parametrized smoothly with nonzero derivative everywhere, so it‚Äôs an immersion, but it‚Äôs <strong>not embedded</strong> because it fails injectivity / fails to be a homeomorphism onto its image.</p> <p>In shoft,</p> <ul> <li><strong>Embedded \(\Rightarrow\)</strong> injective + ‚Äútopologically correct‚Äù inclusion<br/> \(\Rightarrow\) <strong>no self-intersections</strong>.</li> <li><strong>Immersed \(\Rightarrow\)</strong> locally nice but can globally overlap<br/> \(\Rightarrow\) <strong>self-intersections possible</strong>.</li> </ul> <h2 id="8-submanifolds">8] Submanifolds</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Explore relations of maps between manifolds]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)"/><published>2025-09-17T23:30:02+00:00</published><updated>2025-09-17T23:30:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(2)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/"><![CDATA[<p>This is episode 2 on the smooth manifold series. Today we will be diving into concepts that appear initially very intuitive at first glance, but then the extended version of which is indeed quite abstract.</p> <h2 id="0-familiar-examples">0] Familiar examples</h2> <p>Let me start with two simple concrete examples to illustrate what tangent vectors and tangent space are. Indeed they match up to our intuition!</p> <p>Let‚Äôs first say that we have a unit circle in \(\mathbb{R}^2\), or basically let‚Äôs denote it as \(S^1\) (this is a standard notation as</p> <p>\(S^{n} = \{ x \in \mathbb{R}^{n+1} \mid |x| = 1 \}\),</p> <p>representing the surface of an \((n+1)\)-dimensional ball). Pick \(p = (1, 0)\). If I ask you what the tangent vector is starting at \(p\) and tangnet to \(S^1\)? Your answer is probably a vector pointing upward or downward with its tail at \(p\). Indeed,</p> <h2 id="1-three-equivalent-definitions-of-tangent-space">1] Three equivalent definitions of Tangent Space</h2> <p>We‚Äôll cover three equivalent definitions tangent space.</p> <h3 id="1-tangent-vectors-are-equivalence-classes-of-curves">[1] Tangent vectors are equivalence classes of curves</h3> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>tangent vectors are equivalence classes of smooth curves through \(p\):</p> \[T_pM = \{\frac{d}{dt}\Big|_{t = 0} \gamma(t) \Big| \gamma: (\epsilon, \epsilon) \rightarrow M, \; \gamma(0) = p \}\] <p>where \(\gamma_1 \sim \gamma_2\) if in some (equivalently, any) coordinate chart \(\phi: U \subset M \rightarrow \mathbb{R}^n\):</p> \[\frac{d}{dt}\Big|_{t = 0} \phi(\gamma_1(t)) = \frac{d}{dt}\Big|_{t = 0} \phi(\gamma_2(t))\] <p>Consequently, each tangent vector is represented by the velocity of a curve through \(p\).</p> <h3 id="2-tangent-vectors-are-derivations-at-p">[2] Tangent vectors are derivations at \(p\)</h3> <p>Let \(M\) be a manifold, the tangent space at \(p \in M\), denoted as \(T_pM\), is the the vector space of all derivations at \(p\). Notice that a derivation at \(p\) is a <strong>linear</strong> operator (at \(p\)):</p> \[D: C^{\infty}_{p}(M) \rightarrow \mathbb{R}\] <p>satisfying the Leibniz rule:</p> \[D(fg) = f(p)D(g) + g(p)D(f), \; \forall f, g \in C^{\infty}(M)\] <p>where \(C^{\infty}_{p}\) is the equivalence class of \((f, U)\), where \(f \in C^{\infty}\) and \(U\) is a neighbourhood of \(p\). Two functions \((f, U)\) and \((g, V)\) are equivalent iff \(\exist \; W \subset U \cap V\) a neighbourhood of \(p\) such that \(f(x) = g(x), \forall x \in W\), where \(g \in C^{\infty}\) and \(V\) is a neighbourhood of \(p\).</p> <p>Intuitively, \(D\) is like a directional derivative operator acting on smooth functions near \(p\). Consequently,</p> \[T_p{M} = \{ D \mid D \; \mathrm{is\ a\ derivation\ at} p \}\] <p>and a tangent vector \(v \in T_p{M}\) is a derivation \(D\). In fact, Tu (Theorem 2.2, An introduction to manifolds) showed that there‚Äôs a bijection between derivations at \(p\) and directional derivaties.</p> <h3 id="3-tangent-vectors-as-equivalences-on-function-germs">[3] Tangent vectors as equivalences on function germs</h3> <h3 id="local-coordinate-description">Local coordinate description</h3> <p>Specifically if we adopt interpretation [2], then with \((U, \phi)\) a chart with coordinates \((x^1, \dots, x^n)\) near \(p\)</p> \[\{ \frac{\partial}{\partial x^1}\Big|_p, \dots, \frac{\partial}{\partial x^n}\Big|_p \}\] <p>form a basis of \(T_pM\).</p> <p>Thus any tangent vector \(v\) can be written uniquely as</p> \[v = \sum_{i = 1}^{n} v^i \frac{\partial}{\partial x^i}\Big|_p\] <p>where \((v^1, \dots, v^n)\) are the components of the vector in this coordinate system.</p> <h2 id="2-equivalence-between-derivations-and-curves">2] Equivalence between derivations and curves</h2> <p>Perhaps not so surprisingly, the above two definitions are compatible and even equivalent to one another.</p> <p>From curves to derivations:</p> <h2 id="3-a-concrete-computational-example">3] A concrete computational example</h2> <p>A coherent walkthrough using the sphere \(S^2\)</p> <p>Let‚Äôs re-emphasize again that a vector field on a Manifold is defined as the following:</p> <p>Let \(M\) be a smooth manifold. A <strong>vector field</strong> on \(M\) is a smooth section/assignment \(p \mapsto X_p \in T_p M\) where \(T_p M\) is the tangent space at \(p\).</p> <p><strong>Fundamental viewpoint</strong>:</p> <blockquote> <p>A tangent vector is a <strong>derivation</strong>: a linear map \(X_p : C^\infty(M) \to \mathbb{R}\) satisfying the Leibniz rule.</p> </blockquote> <p>We‚Äôll also give the local coordinates and coordinate vector fields as the following:</p> <p>Let \((U, \varphi), \quad \varphi : U \subset M \to V \subset \mathbb{R}^n\) be a coordinate chart, with coordinates \((x^1, \dots, x^n).\)</p> <p>Each coordinate \(x^i\) is itself a <strong>function on the manifold</strong>: \(x^i : U \to \mathbb{R}.\)</p> <p>Definition of the ‚ÄúCoordinate Vector Field‚Äù:</p> <p>The coordinate vector field \(\left.\frac{\partial}{\partial x^i}\right|_p\) is defined <strong>intrinsically</strong> by its action on smooth functions: \(\boxed{ \left.\frac{\partial}{\partial x^i}\right|_p(f) := \frac{\partial}{\partial x^i} \big(f \circ \varphi^{-1}\big) \Big(\varphi(p)\Big) }\) where \(x^i\) is the \(i\)-th coordinate on \(\mathbb{R}^n\).</p> <p>Notice that there‚Äôs an abuse of notation (the left ‚Äúpartial‚Äù is what we define, whereas the right partial is the usual partial differentiation). Hopefully, it‚Äôs obvious that the above definition stems from the following basic definition of graph differential (which I will talk more into next time):</p> \[\boxed{ (dF_p(v))(f) := v_p(f \circ F) }\] <p>More importantly, this definition:</p> <ul> <li>uses <strong>only</strong> the chart,</li> <li>does <strong>not</strong> require an embedding and thus does $NOT$ give us components in $\mathbb{R}^n$</li> <li>explains what ‚Äú\(\partial f / \partial x^i\)‚Äù actually means: It means the ordinary partial derivative of the coordinate expression of $f$ after pulling $f$ back to a coordinate chart.Note that nothing is being differentiated on $\mathbb{R}^n$ unless we explicitly choose an embedding (see below).</li> </ul> <hr/> <p>Now, let‚Äôs take a look at the simple example: Sphere \(S^2\) and spherical coordinates. Given the manifold \(S^2 = \{(x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}\)</p> <p>Coordinate chart (away from poles)</p> <p>\((\theta, \varphi)\) with embedding map \(F(\theta,\varphi) = (\sin\varphi\cos\theta,\; \sin\varphi\sin\theta,\; \cos\varphi).\)</p> <p>Here:</p> <ul> <li>\(\theta, \varphi\) are <strong>functions on \(S^2\)</strong>,</li> <li>not abstract variables.</li> </ul> <p>According to the above discussions, the <strong>intrinsic</strong> meaning of \(\partial / \partial \theta\) and \(\partial / \partial \varphi\) are the following:</p> <p>For any \(f : S^2 \to \mathbb{R}\), \(\frac{\partial}{\partial \theta}(f) = \frac{\partial}{\partial \theta} \big(f \circ \varphi^{-1}\big)(\theta,\varphi), \quad \frac{\partial}{\partial \varphi}(f) = \frac{\partial}{\partial \varphi} \big(f \circ \varphi^{-1}\big).\)</p> <p>Again, this is the <strong>definition</strong>, not an interpretation. This offers us one way to calculate the vector field/tangent vectors at $\forall p \in M$.</p> <p>If we set</p> <p>\(f(x, y, z) = z\), then we could pull it back and obtain \((f \circ \phi^{-1})(\theta, \phi) = \cos(\phi)\). Consequently,</p> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X_p(f)= \frac{\partial \cos(\phi)}{\partial \theta} = 0\] <p>at $p = (\theta, \phi)$.</p> <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y_p(f)= \frac{\partial \cos(\phi)}{\partial \phi} = -\sin(\phi)\] <p>at $p = (\theta, \phi)$.</p> <hr/> <p>However, because \(S^2 \subset \mathbb{R}^3\), we may compute concrete representatives by viewing it as an embedded in $\mathbb{R}^3$ and to extrinsically compute the vector fields by pushforward of coordinate basis vectors via the embedding map of the manifold. Consequently, such computation lives in the embedded picture, not the intrinsic definition.</p> \[\frac{\partial}{\partial \theta} = \frac{\partial F}{\partial \theta} = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0)\] \[\frac{\partial}{\partial \varphi} = \frac{\partial F}{\partial \varphi}= (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi)\] <p>These are <strong>actual vectors in \(\mathbb{R}^3\)</strong> tangent to \(S^2\).</p> <blockquote> <p>Important:<br/> This is <strong>not the definition</strong> of coordinate vector fields ‚Äî<br/> it is a <strong>representation</strong> using the embedding.</p> </blockquote> <p>Consequently, we could act on functions without pulling back to coordinate space:</p> <p>Let \(f(p) = z(p)\) be a function on \(S^2\).</p> <p>Choose an extension \(\tilde f(x,y,z) = z \quad\Rightarrow\quad \nabla \tilde f = (0,0,1).\)</p> <p>Here‚Äôs the key observation:</p> <blockquote> <p>Once a tangent vector is represented in \(\mathbb{R}^3\), its action on $f$ is given by the directional derivative of an extension of $f$.</p> </blockquote> <p>Formally, \(\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\) where</p> <ul> <li>$\tilde{f}$ is any smooth extension of $f$ to $\mathbb{R}^3$</li> <li>$X_p \in T_pS^2 \subset \mathbb{R}^3$.</li> </ul> <p>This works because tangent vectors annihilate normal components.</p> <hr/> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X(f)= (0,0,1) \cdot (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0) = 0\] <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y(f)= (0,0,1) \cdot (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi) = -\sin\varphi\] <p>Same results as the intrinsic definition.</p> <p>Note that to be very concrete, in the above examples, at $p = (\theta, \phi)$, the tangent vector is:</p> <p>$X_p = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\;0)$ (we could interpret this as horizontal circles of latitude) and $Y_p = (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\ -\sin\varphi)$. This should make it very clear that we are treating the tangent vectors as actually ‚Äúvectors‚Äù living in $\mathbb{R}^3$ like how we usually refer them to be. With this extrinsic embedding, the general form of a vector field coincides with the intrinsic notion, but more tangible:</p> <p>$X = a(\theta, \phi)\frac{\partial}{\partial \theta} + b(\theta, \phi)\frac{\partial}{\partial \phi}$,</p> <p>then as a vector in $\mathbb{R}^3$: $X_p = a \frac{\partial F}{\partial \theta} + b\frac{\partial F}{\partial \phi}$, and</p> <p>$X_p(f) = \nabla \tilde{f}(p) \cdot X_p$</p> <p>Notice that for this computation no coordinate pullback is required. But to be crystal clear, we do <strong>not</strong> compute the vector field ‚Äúwithout coordinates‚Äù. We simply You replaced intrinsic coordinates on $\mathbb{S}^2$ by ambient Cartesian coordinates in $\mathbb{R}^3$. So coordinates are still there ‚Äî just in a different space.</p> <hr/> <p>Before we continue, let‚Äôs dwell on this formula for a while:</p> \[\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\] <p>This identity guarantees consistency between 1] the derivation definition and 2] the embedded vector representation (Question: is this just chain rule?s). It explains why the extrinsic calculation agrees with the intrinsic definition. If we have a nice embedding, two views are nice. However, if we go to the general abstract case without the ambient space, we could only assort to the intrinsic definition.</p> <hr/> <p>To be rigorous, you may be concerned about the specific use of the function extension here. However, we could show that extensions do not matter, because <strong>‚Äútangent vectors annihilate normal components‚Äù</strong>. Let me explain below:</p> <p>If \(g : \mathbb{R}^3 \to \mathbb{R}\) satisfies \(g|_{S^2} = 0,\) then for all \(v \in T_p S^2\), \(v(g) = 0.\) Equivalently, \(\nabla g(p) \perp T_p\mathbb{S}^2\)</p> <hr/> <h3 id="proof-via-curves">Proof (via curves):</h3> <p>Let \(\gamma(t) \subset S^2\) with \(\gamma(0)=p, \quad \gamma'(0)=v.\)</p> <p>Since \(g(\gamma(t)) = 0\) for all \(t\), \(v(g) = \frac{d}{dt} g(\gamma(t))\big|_{t=0} = 0.\)</p> <hr/> <h3 id="concrete-example">Concrete example</h3> <p>Let \(g(x,y,z) = x^2 + y^2 + z^2 - 1.\)</p> <p>Then: \(\nabla g = (2x,2y,2z)\) which is normal to \(S^2\).</p> <p>For any tangent vector \(v\), \(v(g) = \nabla g \cdot v = 0.\)</p> <hr/> <h3 id="consequence-crucial">Consequence (crucial)</h3> <p>If \(\tilde f_1\) and \(\tilde f_2\) are two extensions of \(f\), then \(g = \tilde{f_1} - \tilde{f_2}\) vanishes on $\mathbb{S}^2$. For any tangent vector $v$:</p> \[v(\tilde f_1) - v(\tilde f_2) = v(\tilde g) = 0\] <p>thus:</p> \[v(\tilde f_1) = v(\tilde f_2) \quad\forall v \in T_p S^2.\] <p>Hence directional derivatives along tangent vectors are <strong>well-defined</strong>: this is why directional derivatives along tangent vectors do not depend on how f extends off the manifold.</p> <p>View from another perspective, this indicates that a function that is identically zero on the manifold cannot change when we move tangentially: so tangent vectors ‚Äúdon‚Äôt see‚Äù normal variations. What I mean by the former statement ‚ÄúTangent vectors annihilate normal components‚Äù is that derivatives along tangent directions ignore any part of a function that only varies off the manifold, because those variations are invisible to curves that stay on the manifold.</p> <hr/> <h3 id="intrinsic-vs-extrinsic-viewpoints-summary">Intrinsic vs Extrinsic Viewpoints (Summary)</h3> <table> <thead> <tr> <th>Aspect</th> <th>Intrinsic</th> <th>Extrinsic</th> </tr> </thead> <tbody> <tr> <td>Tangent vector</td> <td>Derivation</td> <td>Vector in \(\mathbb{R}^3\)</td> </tr> <tr> <td>Definition</td> <td>Via charts</td> <td>Via embedding</td> </tr> <tr> <td>Computation</td> <td>Pullback</td> <td>Directional derivative</td> </tr> <tr> <td>Dependence</td> <td>Coordinate-dependent</td> <td>Embedding-dependent</td> </tr> </tbody> </table> <p>Both viewpoints are <strong>equivalent</strong> when an embedding exists.</p> <hr/> <p>Something to note from the above example:</p> <blockquote> <p>Coordinate vector fields are <strong>defined intrinsically as derivations</strong>,<br/> may be <strong>represented extrinsically</strong> using embeddings,<br/> and act on functions in a way that is <strong>independent of extensions</strong> because tangent vectors annihilate normal components.</p> </blockquote> <p>Through this simple example we already develop a taste of the difference between intrinsic and extrinsic geometry, the abstract definitions and concrete calculations. Such theme will be recurring for our journey in studying differential geometry.</p> <p>One of the advantages of the intrinsic view is that it is independent of coordinate charts. It paves the ground for explaining why</p> <blockquote> <p><strong>Gradients, geodesics, and Lie brackets make sense without embedding (or ever mentioning) the abstract manifold into $\mathbb{R}^n$</strong>.</p> </blockquote> <h2 id="discussions">Discussions</h2> <p>Firstly, note that tangent/cotangent space is an intrinsic? property. A usual image of interpreting tangent space is \(S^2\) embedded in \(\mathbb{R}^3\). Our definition above does not require so, and in fact the shift of perspective from extrinsic into intrinsic properties of geometric objects is a grand evolution starting from Gauss and Rieman.</p> <p>[TODO: An illustrative figure of a sphere embedded in Euclidean space]</p> <p>From derivations to curves:</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Intuition and definition of tangent/cotangent space]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)"/><published>2025-09-17T17:49:33+00:00</published><updated>2025-09-17T17:49:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Today I will cover a beautiful subject in differenetial geometry: Ricci flows and Ricci curvature.</p> <p>Generally, in Riemannian geometry, curvature measures how space bends. For instance, on a sphere, geodesics (shortest paths) come closer together compared to flat space; on a hyperbolic surface, they diverge.</p> <p>Ricci curvature is a particular way of summarizing curvature: Instead of describing how all directions bend (that‚Äôs what the full <strong>Riemann curvature tensor</strong> does), Ricci curvature focuses on <strong>volume distortion</strong>. More concretely, it tells us how the volume of a small geodesic ball deviates from the volume we‚Äôd expect in flat Euclidean space.</p> <p>Intuitively, we could summarize into the following:</p> <blockquote> <p>Positive Ricci curvature (like on a sphere) means geodesics tend to converge, and small balls have less volume than in flat space.</p> <p>Zero Ricci curvature (like in Euclidean space) means geodesics neither converge nor diverge, so volumes match Euclidean.</p> <p>Negative Ricci curvature (like on a hyperbolic space) means geodesics diverge, so small balls have more volume than Euclidean.</p> </blockquote> <p>Mathematically, Ricci curvature is obtained by ‚Äútracing‚Äù the <strong>Riemann curvature tensor</strong>. It compresses information about how different directions curve into a symmetric 2-tensor <code class="language-plaintext highlighter-rouge">Ric</code>.</p> <h3 id="riemannian-geometry-and-tensor">Riemannian Geometry and Tensor</h3> <p>The Riemann tensor is written in the following way:</p> \[R(X, Y)Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X, Y]} Z\] <p>This tensor captures all information about curvature. Succinctly, this is a 4-tensor: \(R_{ijkl}\).</p> <p>The above is an unfair treatment of Riemannian geometry. I‚Äôll have a separate blog on that subject soon.</p> <p>How to understand: Riemannian metric tensor informs the manifold where to expand, shrink, and curve. How does Riemannian metric tensor relate with curvature?</p> <h3 id="ricci-curvature">Ricci Curvature</h3> <p>Based on the Riemann tensor, what is the curvature?</p> <p>To get Ricci curvature, we take a <strong>trace</strong> of the Riemann tensor:</p> \[Ric_{ij} = R^{k}_{ikj} = g^{kl}R_{kilj}\] <p>This reduces the information down to a 2-tensor (like the metric itself). Geometrically, this represents the volume distortion of geodesic balls.</p> <h3 id="ricci-flow">Ricci Flow</h3> <p>Introduced by <code class="language-plaintext highlighter-rouge">Richard Hamilton (1982)</code>, Ricci flow is a process that evolves a Riemannian metric \(g(t)\) over time:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>The factor -2 is just convention (to simplify later computations). We could think of this as a heat equation for geometry: Just as heat diffuses to smooth out temperature differences, Ricci flow smooths out <strong>irregularities</strong> in curvature. As illustrated in the above section of Ricci curvature, we could naturally arrive at the result that positive curvature regions tend to shrink and negative curvature regions expand. Over time, the underlying geometry becomes more ‚Äúregular‚Äù, like ironing out wrinkles.</p> <p>The effect of Ricci flow on curvature could be expressed in the following way. The derivative of scalar curvature \(R\) under this flow is:</p> \[\frac{\partial R}{\partial t} = \Delta R + 2 |Ric|^2\] <p>This resembles a heat equation $(\Delta R)$ plus a positive correction. Consequently, curvature tends to diffuse out but also grows in positive-curvature regions.</p> <h4 id="examples-of-ricci-flow">Examples of Ricci Flow</h4> <p>Perhaps the simplest example is to imagine how a sphere evolves under Ricci flow. We all know that a round sphere has positive Ricci curvature. The Ricci flow equation says:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>Since Ricci is positive, the metric shrinks, and thus makes the sphere to contract uniformly, eventually collapsing to a point. This closely mirrors the idea that positive curvature makes geodesics converge. Under the Ricci flow, it tightens further.</p> <p>Another simple example is the flat torus. Since the Ricci curvature is 0 everywhere,</p> \[\frac{\partial g_{ij}}{\partial t} = 0\] <p>the torus will stay unchanged after the flow forever. This is analogous to heat diffusion on a perfectly uniform temperature field where nothing would effectively changes.</p> <p>Likewise, a hyperbolic surface has negative Ricci curvature. Consequently, under the Ricci flow, the metric would expand and the hyperbolic surface would grow larger and more uniform in curvature.</p> <p>In a nutshell, irregular geometries with bumps or folds (different curvature in different regions) get ‚Äúsmoothed‚Äù over time. All the high-curvature ‚Äúwrinkles‚Äù would get flatten out, like how heat equalizes temperature.</p> <h3 id="application-of-ricci-flow">Application of Ricci Flow</h3> <p>Poincare conjecture, Ricci flow, surgery theory, what Terrence Tao called ‚Äúone of the most impressive recent achievements of modern mathematics‚Äù</p> <p>Poincare conjecture:</p> <blockquote> <p>Any closed 3-manifold that is simply-connected, compact, and boundless is homeomorphic to a 3-sphere.</p> </blockquote> <p>Specifically, Poincare conjecture in higher dimensions has been solved around 1961, and dimension 4 case has been proved by Michael Freedman who by which won Fields medal in 1986. The \(n = 3\) case seemed really difficult to crack and it was only at 2002 that Grisha Perelman proved it using Ricci flow.</p> <p>Very briefly, since a sphere has positive curvature, by applying Ricci flow through time such sphere will contract and eventually vanish. Perelman proved the opposite also holds: if metric goes to 0, it must have been a sphere. To prove Poincare‚Äôs conjecture using Ricci flow,</p> <p>One of the most triumphant use of Ricci flow happens when Grigori Perelman (2002‚Äì2003) to prove <strong>the Poincar√© conjecture</strong> and the more general <strong>Thurston geometrization conjecture</strong>. He showed how Ricci flow with ‚Äúsurgery‚Äù (cutting and patching when singularities form) classifies 3-manifolds.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Introduction of Ricci flows/curvatures]]></summary></entry><entry><title type="html">The Dance of Space (2): Differential Forms (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/" rel="alternate" type="text/html" title="The Dance of Space (2): Differential Forms (in progress)"/><published>2025-09-16T16:34:22+00:00</published><updated>2025-09-16T16:34:22+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Differential_Forms</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/"><![CDATA[<p>The second episode to appreciate the inner workings of space is through the differential forms. Differential forms is not an ancient subject, ‚Ä¶</p> <h3 id="vector-outer-product">Vector Outer Product</h3> <h3 id="wedge-productexterior-derivative">Wedge Product/Exterior Derivative</h3> <h3 id="three-in-one">Three in One</h3> <p>Green‚Äôs theorem, Gauss‚Äô theorem, Stoke‚Äôs theorem.</p> <p>Green‚Äôs theorem:</p> \[\int_{L} Pdx + Qdy = \iint_{D}(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y})dxdy\] <p>Generalized Stoke‚Äôs theorem.</p> <p>A k-form is supposed to be integrated over an oriented k-dimensional manifold</p> <h3 id="fundamental-theorem-of-calculus-ftoc">Fundamental Theorem of Calculus (FTOC)</h3> <p>High dimensional Stoke‚Äôs theorem is exactly the fundamental theorem of calculus (FTOC) in high-dimensional space.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Intuite and define differential forms]]></summary></entry><entry><title type="html">Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Equivalence/" rel="alternate" type="text/html" title="Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)"/><published>2025-09-07T00:23:16+00:00</published><updated>2025-09-07T00:23:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Equivalence</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Equivalence/"><![CDATA[<p>Explain and compare multiple equivalences from differential geometry and topology, including homeomorphism, diffeomorphism, homotopy equivalence, homomorphism, isomorphism, etc.</p> <p>Invariant and equivariant functions (CNN is equivariant).</p> <p>Also discussions on cardinality among sets (including finite and infinite (countably infinite \(N0\)? and uncountably infinite), Hillbert Hotel problem, equipotent sets)</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Topology"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Compare various equivalences in geometry/topology/group theory]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (1): Rigorous Definition (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(1)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (1): Rigorous Definition (in progress)"/><published>2025-08-29T02:32:56+00:00</published><updated>2025-08-29T02:32:56+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(1)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(1)/"><![CDATA[<h3 id="preface">Preface</h3> <p>Starting with this blog, I‚Äôll gradually introduce the core elements and the corresponding theories of (differentiable/smooth) manifold, which is, without any undue exaggeration or affected self-indulgence, the most fundamental arean upon which modern geometry unfolds (other than thoeries of curves and surfaces in \(\mathbb{R}^3\)).</p> <p>It‚Äôs only after I seriously learned manifold that I started to appreciate its elegant geometric intuition (unintuited indeed or not easy to grasp at first glance sometimes) and its abstraction and extensions of our known, visually perceived \(\mathbb{R}^3\) into higher dimensional space. It grants us infinite power to have a glimpse of what we are not quite possible at all for our entire lives to interpret and still acquire the confidence to draw conclusions on properties, hierarchies, and variations in the spave above us.</p> <p>As the first blog of the entire manifold Omnibus series, I aim to cover the rigorous definition of manifold, and a general introduction to its properties and different categories, to set the basis for further rigorous discussions.</p> <p>I would refrain from diverting to much from the many interesting discussions (but will review together the definition) of topological space, topology, open sets, Hausdorff space, and homeomorphism.</p> <h3 id="history-and-motivation">History and motivation</h3> <p>Motivation of manifolds:</p> <p>Geometrically, ‚Ä¶</p> <p>Algebraically, solution sets for polynomial equations and differnetial equations.</p> <p>Immanuel Kant‚Äôs idea on the word manifold.</p> <h3 id="a-snapshot">A snapshot</h3> <p>The following might touch upon numerous unfamiliar terms and concepts, no worries I will cover them all as time unfolds. For now, let‚Äôs get a little intuition and some impressionistic perception of this subject.</p> <p>What exactly is a manifold? The most basic kind of a manifold, a topological (n-dimensional) manifold \(M\) is a topological space locally homeomorphic to \(\mathbb{R}^n\). Specifically, for each point \(p \in M\) there exists a chart around \(p\), which is essentially a pair \((U, \Phi)\) such that \(U \subset M\) is an open set(in the topology sense) containing \(p\) and \(\Phi\) a homeomorphism from \(U\) to an open subset of \(\mathbb{R}^n\). For some technical details, the definition of manifolds would also add two other conditions: 1] the Hausdorff property and 2] second-countability. Any manifold has to be a topological manifold at least.</p> <p>The main focus of this series will be on smooth manifolds, which add some nice properties other than being topological manifolds. A smooth manifold acquires a collection of charts compatible with each other, such that the composition of coordinate functions is a diffeomorphism (they form what‚Äôs called a smooth structure on the topological manifold). This nicely ensures differentiability and enables many basic calculus on the manifolds to be valid operations.</p> <p>Examples of manifold include both objects we know and some more abstractly constructed entities: \(\mathbb{R}^n\), any onpen subsets of a manifold, the product space \(M \times N\) (if \(M\) and \(N\) are manifolds).</p> <p>Another famous family of manifolds comes from compact connected 2-manifolds, specifically closed surface with genus \(g &gt; 0\): for example, the familiar \(S^n\) (\(g = 0\)) and the torus \(S^1 \times S^1\) (\(g = 1\)).</p> <p>If we specify our manifolds to be <code class="language-plaintext highlighter-rouge">non-orientable</code>, we would obtain some even more famous/esoteric objects like the (open) <strong>Mobius strip</strong> (open Mobius strip is non-compact, since the boundary is not included) and <strong>Klein bottle</strong> \(K = I \times I / \sim\). From the pure topology standpoint, they are actually quite basic topological spaces. Needless to mention the famous theorem about <strong>classification of surfaces</strong> which states that every compact connected 2-manifold appears as \(S^2\) glued with tori or Mobius stirps (up to homeomorphism). There‚Äôs a little subtlety here in that there do exist ‚Äúmanifolds with boundaries‚Äù which are locally homeomorphic to \([ 0 \times \infty) \times \mathbb{R}^{n-1}\) instead of \(\mathbb{R}^n\), like Mobius strips in a usual sense.</p> <p>Other more abstract example exist such as the projective planes (frequently showing up in Topology) \(\mathbb{RP}^n\cong S^n/\sim\) (with the equivalence relation \(v \sim -v\)). Notice that there‚Äôre multiple ways to define quotient spaces for \(\mathbb{RP}^n\). This example is usually utilized to illustrate quotient space. The Grassmanian Manifold is also frequently studied: \(Gr_k (\mathbb{R}^m) = \{ \mathrm{k\ dimensional\ vector\ subspaces\ of} \; \mathbb{R}^m \}\), where \(0 \leq k \leq m\). This is a compact manifold with dimensions \(k(m-k)\) (Interested readers might come up with a guess or proof). Interestingly, \(Gr_1 (\mathbb{R}^m) \cong \mathbb{RP}^{m-1}\).</p> <p>Since manifold is literally the backbone of modern differential geometry which has evolved into countless sub-branches and domains, it‚Äôs impossible to even sample a fair amount in these blogs. Consequently, I‚Äôll be mainly focusing the following:</p> <ul> <li>Definitions of smooth manifolds</li> <li>Submanifolds and embeddings <ul> <li>An embedding is an injective smooth map from \(M\) to \(N\) which is also an isormorphism between \(M\) and a submanifold of \(N\)</li> </ul> </li> <li>Tangent and cotangent spaces <ul> <li> \[p \in M, dim(M) = n, T_p N \cong \mathbb{R}^n\] </li> <li>The differential of smooth maps between manifolds \(M, N\) is a <strong>linear</strong> map from \(T_p M\) to \(T_{f(p)}N\)</li> <li>The cotangent space is the dual vector space of \(T_p M\), containing elements as \(d_p f \in T_p^{*}M\), where \(f: M \rightarrow \mathbb{R}\) is smooth</li> </ul> </li> <li>Tangent and cotangent bundles <ul> <li>\(\pi: TM (\mathrm{2n\ dimensional}) \rightarrow M\), where \(\pi^{-1}(\{ p\}) = T_p M\)</li> <li>Example of vector bundle</li> <li>Similarly a cotangent bundle \(Y: T^{*}M \rightarrow M\)</li> </ul> </li> <li>Tangent and cotangent fields <ul> <li>A <strong>section</strong> of the tangent bundle \(X: M \rightarrow TM\) is a tangent field (or tangent vector field, or vector field on \(M\)): \(X(p) \in T_pM\)</li> <li>\(X\) as a vector field, \(X(p) \in T_pM, \forall p \in M\), tracing out a flow on \(M\)</li> <li>This is framed in the language of ODE and flows</li> <li>A <strong>section</strong> of the cotangent bundle is a cotangent field (or 1-forms, or differentials)</li> <li>Cotangent fields are differential 1-forms, if \(f: M \rightarrow \mathbb{R}\) is smooth, \(p \in M\), then \(d_pf \in T_p^{*}M\)</li> <li>Frobenius integrability theorem</li> </ul> </li> <li>Differnetial \(k\)-forms on manifold \(M\) <ul> <li>Tensor bundles and tensor fields</li> <li>A section of the \(k\)th exterior power of the cotangent bundle \(T^{*}M\) is a differential \(k\)-form</li> <li>0-forms: functions on \(M\)</li> <li>1-forms: cotangent fields</li> <li>k-forms: sections of the \(k\)th exterior power of \(T^{*}N\)</li> <li>de Rham derivative and de Rham cohomology groups: \(d: \Omega^{k}M \rightarrow \Omega^{k+1}M\)</li> <li>For \(M\) compact and orientable, \(\omega \in \Omega^n M\), \(\int_{M} \omega \in \mathbb{R}\)</li> </ul> </li> <li>General Stokes‚Äôs Theorem <ul> <li>If M is compact, oriented, with boundary, n-dimensional and \(\omega \in \Omega^{n-1}M\), then \(\int_{M}d\omega = \int_{\partial M} \omega\)</li> </ul> </li> <li>Lie group and Lie algebra <ul> <li>Manifold \(+\) group at the same time</li> </ul> </li> </ul> <h3 id="something-dark-or-darker">Something dark, or darker</h3> <p>One last note to keep before we starting diving in, especially for readers from computational/systems neuroscience who probably have heard of the concept <code class="language-plaintext highlighter-rouge">neural manifold</code>. For the past few decades, as the technologies of simultaneous recordings of multiple neurons quickly evolve, data recorded as an entire neural population has gradually become a solid reality for many subdomains of neuroscience, and the level of data analysis promptly and adaptively shifts from the single-neuron perspective to neural population analysis. Many theories have been developed to cope with such high dimensional data (could be as many as several hundreds), like the dynamical system perspective as <a href="/blog/2025/jPCA/, /blog/2025/Constraint-Learning/">my previous two blogs covered</a>, or neural manifold. This is really not a novel idea, as in many domains there‚Äôs abundance of observation that among the high dimensional data there exists some low dimensional structure that captures the <code class="language-plaintext highlighter-rouge">patterns</code> of data.</p> <p>However, in most cases there‚Äôs virtually nothing lost if we simply swap the word ‚Äúneural manifold‚Äù into, say, ‚Äú<strong>low dimensional structure</strong>‚Äù. What worries me is that at least up to now there is no theoretical proof that a collected neural population is indeed a (at least topological) manifold. Also, from an ontological perspective, because other than employing this word itself, there‚Äôs very few rigorous adaptation or usage of the many theorems and properties associated to a manifold, the true essence behind the entire domain of manifold theory is left untouched at all. Indeed, we might argue that at a figurative level such neural manifold is a nice symbol for building up intuition, but there‚Äôs still a conceptual subtlety that the very first motivation of a manifold is to extend the familiar Euclidean space around us to higher dimensional spaces. In other words, the charm of manifolds shines in the high dimensional space, which, without further notice, might comletely mislead people into linking ‚Äúmanifold‚Äù with ‚Äúlow dimesional space‚Äù. To be fair, in computational/systems neurscience, even when ‚Äúneural manifold‚Äù is used to indicate ‚Äúlow dimensional space‚Äù, it may still be 10 or 30, which for mathematicians are usually ‚Äúhigh‚Äù (high vs low is a relative concept. I don‚Äôt think there‚Äôs a disagreement with regard to the aboslute ‚Äúhigh‚Äù). For neuroscientists, ‚Äúlow dimension‚Äù is low because of its direct comparison to the original space in \(\mathbb{R}^{384}\), for example; for a geometer, both \(\mathbb{R}^{10}\) and \(\mathbb{R}^{384}\) are ‚Äúhigh dimensional space‚Äù because they extend beyond \(\mathbb{R}^{3}\). Finally, manifold not only extends \(\mathbb{R}^n\) to \(\mathbb{R}^N\), where \(n &gt;&gt; N\), but more importantly entirely discards the many constraints of Euclidean space and works with more general spaces (indeed, as in the shift of focus of intrinsic vs extrinsic properties).</p> <p>I never doubt that some low dimensional structure is imbedded in neural population, as countless research has corroborated it. And I deeply appreciate the remarkable wisdom and significant efforts others have paid along this line of research under the name of neural manifold. In the end, I‚Äôm just hoping concepts to stay clear and to minimize the downplay of any important concepts that are both aesthetically appearling and fundamentally stunning. I‚Äôm reluctant to observe those ideas get muddled, mumbled, and straggled among hypes and later turned into buzzwords that, at the down-to-earth applied level, mistakes its potential and hides the key questions regarding how to adapt the abstract manifold concepts into codable algorithms, or at the abstract level noisify our perception of the world (as many other instances already in an awkard situation, like ‚Äúcomputation‚Äù or ‚Äúrepresentation‚Äù).</p> <p>Perhaps I‚Äôm being a little over-reactive?</p> <p>Or‚Ä¶perhaps not so surprising, in recent years among Deep Learning there‚Äôs more and more serious consideration and introduction of embedding geometry/topology into the modeling pipelines (geometric deel learning, interested readers might want to check out Michael Bronstein). I highly enjoy the geometric approaches whether in DL or neuroscience, and I‚Äôm very glad that I live in this era when I could witness the entire trajectory of geometry involvement. Meanwhile, I‚Äôll do my best to clarify things and introduce more into the pure beauty of geometry. Enough sentiment, now let‚Äôs jump in.</p> <h3 id="a-bit-of-topology">A bit of topology</h3> <p>Topological sapce:</p> <p>Homeomorphism:</p> <h3 id="manifold-entry-point-topological-manifold">Manifold entry point: Topological manifold</h3> <p>Definition of a topological manifold:</p> <blockquote> <p>A topological space \(M\) is an \(n\)-dimensional topological manifold if</p> <p>1] \(\forall p \in M\), there exists an open neighborhoood \(U\) (\(p \in U \subset M\)) such that there exists a homeomorphism between \(U\) and an open subset of \(\mathbb{R}^n\)</p> <p>2] \(M\) is Hausdorff</p> <p>3] \(M\) is second-countable</p> </blockquote> <p>The first requirement is the frequently cited notion that a manifold is <em>locally like</em> a Euclidean space, but rigorously defined in terms of topological equivalence. The third condition about second-countability indicates that there exists a countable set of topolgy bases. This is required for guaranteeing the existence of the partition of unity on \(M\) subordinate to any open cover.</p> <p>Conditions 1] and 3] could be combined into one statement:</p> <blockquote> <p>\(M\) is the union of finite or countably infinite open subsets \(U \subset M\) which is homeomorphic to some open subsets of \(\mathbb{R}^n\)</p> </blockquote> <p>Whenever a manifold is mentioned without any prefix, it‚Äôs a topological manifold. More conditions could be specified to add more ‚Äúflavors‚Äù to the manifold: like if we prescribe a metric to the manifold to obtain a Riemannian manifold. But without adding more complicated structure, we will only ask for something simpler to include: the differentiability, which leads to the main subject of this series of blogs: differentiable/smooth manifolds. But before going there, let‚Äôs take a look at few examples that are not manifolds (to also deepen our interpretation of the definition):</p> <h3 id="what-is-not-a-manifold">What is not a manifold?</h3> <p>1] An ‚Äú8‚Äù-shaped structure is not a manifold, since near the center it‚Äôs not homeomorphic to any Euclidean space.</p> <p>2] A long line constructed by the first uncountable ordinal number (this violates the countability criterion)</p> <p>3] A canonical example that fails the Hausdorff condition is a line with two origins: \(X = \mathbb{R}_1 \sqcup \mathbb{R}_2 / \sim\) where \(x \sim y\) if and only if \(x = y \neq 0\) (so the two 0s \(0_1 \in \mathbb{R}_1, 0_2 \in \mathbb{R}_2\) are still distinct). These two 0s cannot be distinguished because they share the same neighbors all the time. Around the origin we cannot tell from the neighbourhood if it‚Äôs \(0_1\) or \(0_2\). The topology forces neighbourhoods to overlap too much such that the Hausdorff property fails. This example could also be extended into 2D versions.</p> <p>Another slightly different counterexample for Hausdorffness is not on separating some point, but glueing spaces together: Consider two copies of Euclidean plane: \(\mathbb{R}^2_1, \mathbb{R}^2_2\), and glue them on the right half plane</p> \[H = \{(x, y) \in \mathbb{R}^2 | x \geq 0 \}\] <p>Equivalently, \(X = \mathbb{R}^2_1 \sqcup \mathbb{R}^2_2 / \sim\), where \((x_1, y_1) \sim (x_2, y_2)\) iff \(x \geq 0\). Obviously, \(X\) is not Hausdorff since there‚Äôs not disjoint neighbourhood around \((0, 0)_1\) and \((0, 0)_2\).</p> <h3 id="collection-of-descriptors-charts-and-atlas">Collection of descriptors: Charts and atlas</h3> <p>The above definition emphasizes the essence of manifold that we could only approach it locally. To derive a global view of a maifold is enabled by taking many local ‚Äúcameras‚Äù and projections together. Consequently, we naturally arrive at the following definition:</p> <blockquote> <p>Def: An n-dimensional chart for \(M\) is a pair \((U, \Phi)\) such that \(U \subset M\) is an open sutset, \(\Phi: U \rightarrow \Phi(U)\) a homoemorphism from \(U\) to an open subset of \(\mathbb{R}^n\).</p> <p>Def: A chart for \(M\) is a collection \(\{U_{\alpha}, \Phi_{\alpha} \}_{\alpha \in A}\) such that \(\bigcup_{\alpha \in A} U_{\alpha} = M\)</p> </blockquote> <p>With the concept of chart and atlas, we now are fully capable of pulling the manifold into different Euclidean frames to analyze them, since a chart \((U, \Phi)\) determines real-valued functions \(x_1, x_2, \dots x_n\) such that \(\Phi(p) = (x_1(p), x_2(p), \dots, x_n(p))\) where \(p \in U \subset M\) and \(x_i: U \rightarrow \mathbb{R}^n\). These \(x_i\) are called the coordinate functions of the selected chart.</p> <p>Some readers might have a concern now. If there exist some charts \((U, \Phi)\) and \((V, \Psi)\) where \(U \cap V \neq \varnothing\), then \(\Phi(U \cap V)\) and \(\Psi(U \cap V)\) will have different coordinates. Would this cause a mess?</p> <h3 id="the-real-arena-smooth-manifold">The real arena: Smooth manifold</h3> <p>Given \(f: u \subset \mathbb{R}^n \rightarrow \mathbb{R}\), where \(u\) is an open set, \(f\) is \(C^k\) if the \(k\)th derivative \(\frac{\partial^k f}{\partial x_j^k}\) exists and is continuous \(\forall 1 \leq j \leq n\). Likewise, for a multivariable function \(f: u \subset \mathbb{R}^n \rightarrow \mathbb{R}^m\), \(f = (f_1, f_2, \dots f_m)\) is \(C^k\) if \(f_i\) is \(C^k \; \forall i\). If we take \(k\) to be \(\infty\), then \(f\) is called a smooth. In my blogs, whenever any function is called differentiable without specifying the \(k\)th order, we will assume \(k\) as \(\infty\) and thus a differentiable function is the same as a smooth function. A differentiable manifold is the same as the smooth manifold, both I‚Äôll use interchangeably.</p> <p>There‚Äôs a another way to define smooth of functions in a coordinate-free manner (with linear approximation other than calculating derivatives (See Extra Notes # 2.))</p> <p>Another important concept is called diffeomorphism. Diffeomorphism in \(\mathbb{R}^n\) goes like this: suppose \(\Phi: u \in \mathbb{R}^n \rightarrow v \in \mathbb{R}^n\)</p> <p>\(u, v\) both being open subsets. \(\Phi\) is a diffeomorphism if \(\Phi\) is smooth, \(\Phi\) exists, and \(\Phi^{-1}\) is also smooth. In that case, if \(y = \Phi(x)\), then the matrix \((\frac{\partial y_i}{\partial x_j})\) is invertible. If a function is a diffeomophism, then does it have to be a homeomorphism?</p> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>The point of adding smooth structure to a topological manifold: this enables us to characterize a real-valued function \(f\) on \(M\) to be smooth or not ‚Äî- for a chart \((U, \Phi)\), \(f\) is smooth when it composed with the inverse map \(\Phi^{-1}: \Phi(U) \rightarrow U\): \(f \circ \Phi^{-1}: \Phi(U) \subset \mathbb{R}^n \rightarrow \mathbb{R}\) is smooth (the composed function \(f \circ \Phi^{-1}\) maps from Euclidean space to Euclidean space so we could solicit the familiar definitions of smoothness/differentiability). Careful readers might have spotted a subtle issue: the above definition of smoothness (the composed function) hinges upon the specific selection of chart. So we are led to use only some charts, not all charts.</p> <h3 id="a-few-others">A few others?</h3>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Rigorous definition of manifold and related topology concepts]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble (1): Exponential map (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Exponential-map/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble (1): Exponential map (in progress)"/><published>2025-08-29T02:25:33+00:00</published><updated>2025-08-29T02:25:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Exponential-map</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Exponential-map/"><![CDATA[<p>Exponential map as dynamical flow in differential geometry and dynamical systems.</p> <h1 id="differential-geoetry-lie-group">Differential geoetry (Lie group)</h1> <h1 id="dynamical-system-linear">Dynamical system (Linear)</h1>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Exponential maps applied in Lie group & dynamical systems]]></summary></entry></feed>