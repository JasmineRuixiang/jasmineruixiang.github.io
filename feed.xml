<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jasmineruixiang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasmineruixiang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-13T22:04:28+00:00</updated><id>https://jasmineruixiang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Manifold and Riemannian Geometry (X): Connections, Parallel Transport, and Geodesics (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(X)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (X): Connections, Parallel Transport, and Geodesics (in progress)"/><published>2026-02-13T16:36:23+00:00</published><updated>2026-02-13T16:36:23+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(X)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(X)/"><![CDATA[<p>This is episode X on the Manifold and Riemannian Geometry series. Today we will be exploring more on tangent vectors, vector fields, another key concept about how to relate tangent spaces together, and many other derivative notions crucial for the development of Riemannian Geometry.</p> <h2 id="intuition-directional-derivatives">Intuition: Directional Derivatives</h2> <p>üåê The Connection: Bridging Derivatives from $\mathbb{R}^3$ to Curved Manifolds The concept of a connection is the necessary tool that allows us to perform differential calculus on curved spaces (manifolds), such as the surface of a sphere. It generalizes the familiar idea of the directional derivative from flat Euclidean space ($\mathbb{R}^3$).</p> <ol> <li>Directional Derivatives in Euclidean Space ($\mathbb{R}^3$) In $\mathbb{R}^3$ with Cartesian coordinates $(x, y, z)$, the directional derivative provides a simple way to measure change. The basis vectors $\left{ \mathbf{i}, \mathbf{j}, \mathbf{k} \right}$ (or the equivalent operators $\left{ \frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z} \right}$) are constant, allowing us to define derivatives simply as component-wise partial derivatives. Let $p=(1, 2, 0)$ be a point, $X=(y, -x, 3x)$ be the direction vector field, and $V=(xz, y^2, -2x)$ be a vector field. A. Derivative of a Scalar Function ($D_X f$) This is the directional derivative of a smooth scalar function $f$ in the direction $X$.</li> </ol> <p>Perspective Formula Example Result for f(x,y,z)=xy2+z at p Vector-Based (Calculus) $D_X f = \nabla f \cdot X$ $D_X f(p) = \langle 4, 4, 1 \rangle \cdot \langle 2, -1, 3 \rangle = \mathbf{7}$ Point Derivation (Geometry) $X[f] = X^i \frac{\partial f}{\partial x^i}$ $Xf = (y^3 - 2x^2y + 3x)\big</p> <p>This confirms that in differential geometry, a tangent vector $X$ is rigorously defined as a point derivation‚Äîan operator that mimics the directional derivative by satisfying the Leibniz rule. B. Derivative of a Vector Field ($D_X V$) This derivative measures how the vector field $V$ changes as we move in the direction $X$. In $\mathbb{R}^3$, this is calculated by taking the directional derivative of each component of $V$. Using the vector fields $X$ and $V$: The $k$-th component of the resulting vector $D_X V$ is $(D_X V)^k = \sum_{i} X^i \frac{\partial V^k}{\partial x^i}$. Component 1 (i.e., $k=1$): $(D_X V)^1 = yz + 3x^2$ Component 2 (i.e., $k=2$): $(D_X V)^2 = -2xy$ Component 3 (i.e., $k=3$): $(D_X V)^3 = -2y$ Evaluating at $p=(1, 2, 0)$ gives:</p> <p>\(D_X V(p) = \langle 3, -4, -4 \rangle\)</p> <ol> <li>The General Connection: The Covariant Derivative ($\nabla_X V$) The formula for $D_X V$ fails on a curved manifold $M$ because the tangent spaces $T_p M$ and $T_q M$ at nearby points $p$ and $q$ are distinct. We cannot simply subtract the vector $V(p)$ from $V(q)$. A Connection ($\nabla$) is the rule that provides the necessary ‚Äúcorrection‚Äù to define the derivative intrinsically on $M$. The resulting derivative is called the covariant derivative $\nabla_X V$. A. Definition and Axioms The connection is an operator $\nabla: C^{\infty}(M) \times C^{\infty}(M) \to C^{\infty}(M)$ that maps two vector fields, $X$ and $V$, to a new vector field $\nabla_X V$, satisfying: Linearity over Functions in $X$: $\nabla_{fX} V = f \nabla_X V$ Linearity in $V$: $\nabla_X (aV + bW) = a \nabla_X V + b \nabla_X W$ Leibniz Rule: $\nabla_X (fV) = (Xf) V + f \nabla_X V$ (where $Xf$ is the directional derivative of $f$) B. The Coordinate Form and Christoffel Symbols In local coordinates, the covariant derivative $\nabla_X V$ is defined using the Christoffel symbols ($\Gamma^k_{ij}$), which represent the rate of change of the coordinate basis vectors $\left{ \frac{\partial}{\partial x^i} \right}$:</li> </ol> <p>\((\nabla_X V)^k = \underbrace{X^i \frac{\partial V^k}{\partial x^i}}_{\text{I. Flat-Space Derivative Term}} + \underbrace{X^i \Gamma^k_{ij} V^j}_{\text{II. Curvature Correction Term}}\) The Christoffel symbols $\Gamma^k_{ij}$ are defined by the action of the connection on the basis vectors:</p> <p>\(\nabla_{\frac{\partial}{\partial x^i}} \frac{\partial}{\partial x^j} = \sum_{k} \Gamma^k_{ij} \frac{\partial}{\partial x^k}\) Difference from Euclidean Case: In $\mathbb{R}^3$ with Cartesian coordinates, $\Gamma^k_{ij} = 0$, and the second term vanishes, resulting in $\nabla_X V = D_X V$. On a curved manifold, $\Gamma^k_{ij} \neq 0$, and the correction term is essential.</p> <ol> <li>The Levi-Civita Connection In Riemannian Geometry, a Riemannian metric $g$ is introduced to measure lengths and angles. The Levi-Civita Connection is the unique connection that respects this metric structure. It is defined by two crucial properties: Metric Compatibility: The connection must preserve the metric $g$ under parallel transport.</li> </ol> <p>\(X(g(V, W)) = g(\nabla_X V, W) + g(V, \nabla_X W)\) Zero Torsion: The connection must satisfy:</p> \[\nabla_X Y - \nabla_Y X = [X, Y]\] <p>where $[X, Y]$ is the Lie bracket. The Christoffel symbols of the Levi-Civita Connection are thus entirely determined by the components of the metric $g_{ij}$ and their first derivatives:</p> \[\Gamma^k_{ij} = \frac{1}{2} g^{k\ell} \left( \frac{\partial g_{j\ell}}{\partial x^i} + \frac{\partial g_{i\ell}}{\partial x^j} - \frac{\partial g_{ij}}{\partial x^\ell} \right)\] <hr/> <h2 id="0-introduction">0] Introduction</h2> <h2 id="1-why-do-we-need-a-connection">1] Why Do We Need a Connection?</h2> <p>In Euclidean space \(\mathbb{R}^{n}\), if we have a vector field \(Y(x) = (Y^1(x), \cdots, Y^n(x))\), and we could easily differentiate component-wise: \(\frac{\partial Y^i}{\partial x^j}\). This works because</p> <ul> <li>All tangent spaces are the same vector space.</li> <li>We can subtract vectors at different points.</li> <li>We can differentiate vector fields componentwise.</li> </ul> <p>But on a manifold:</p> <ul> <li>A vector field satisfies<br/> \(V(p) \in T_pM\) \(V(q) \in T_qM\)</li> <li>These live in <em>different</em> vector spaces.</li> <li>Subtracting them directly makes no sense.</li> </ul> <p>So we need a rule that tells us:</p> <blockquote> <p>How does a vector field \(Y\) change as we move in a given direction given by \(X\)?</p> </blockquote> <p>This rule is an <strong>affine connection</strong>. And we <strong>choose</strong> such a rule.</p> <hr/> <h2 id="2-definition-of-an-affine-connection">2] Definition of an Affine Connection</h2> <p>A connection \(\nabla\) assigns to vector fields X and Y a new vector field: \(\nabla_X Y\)</p> <p>We could think of it as:</p> <blockquote> <p>The derivative of \(Y\) in the direction \(X\).</p> </blockquote> <p>This specified choice needs to satisfy the following three rules:</p> <ul> <li> <p>Linearity in \(X\) \(\nabla_{fX + gZ} Y = f\nabla_X Y + g\nabla_Z Y\)</p> </li> <li> <p>Linearity in \(Y\) \(\nabla_X (Y + Z) = \nabla_X Y + \nabla_X Z\)</p> </li> <li> <p><strong>Leibniz Rule</strong> \(\nabla_X (fY) = X(f)Y + f\nabla_X Y\)</p> </li> </ul> <p>This last property is crucial, as it mirrors ordinary differentiation (notice that we also require the definition of derivation when in tangent vector to also satisfy the Leibniz rule).</p> <p>Also note that this definition does <strong>not</strong> require a metric.</p> <hr/> <h2 id="3-local-coordinate-formula-and-christoffel-symbols">3] Local Coordinate Formula and Christoffel Symbols</h2> <p>Choose coordinates \((x^1, \dots, x^n)\) first.</p> <p>Let<br/> \(X_i = \frac{\partial}{\partial x^i}\)</p> <p>Define:</p> \[\nabla_{X_i} X_j = \Gamma^k_{ij} X_k\] <p>(Or, just in another format, to be crystal clear)</p> \[\nabla_{\frac{\partial}{\partial x_i}}\frac{\partial}{\partial x_j} = \sum_{k = 1}^n \Gamma^k_{ij} \frac{\partial}{\partial x_k}\] <p>We use above the Einstein summation notation as convention (to simplify notations). In below, whenever summation might be obvious I will drop it. Here the $\Gamma^k_{ij}$ are called the <code class="language-plaintext highlighter-rouge">Christoffel symbols</code>, which tell us how the coordinate basis vectors change as we move.</p> <p>Now use the above notation, say for two vector fields:</p> <p>\(X = X^i \frac{\partial}{\partial x^i}\) \(Y = Y^j \frac{\partial}{\partial x^j}\)</p> <p>We could obtain:</p> \[\boxed{ \nabla_X Y = \left( X^i \frac{\partial Y^k}{\partial x^i} + X^i \Gamma^k_{ij} Y^j \right)\frac{\partial}{\partial{x^k}} }\] <p>This will be our main formula. If we stair at it for a bit, the following should be intuitive:</p> <ul> <li>First term: change in components, what we shall expect in Euclidean space. It measures how the components of \(Y\) change. If the manifold were flat and the basis vectors didn‚Äôt vary, this would be enough.</li> <li>Second term: correction for changes in basis, because on a curved manifold, the basis vectors \(\frac{\partial}{\partial x^j}\) change from point to point. So even if the components \(Y^j\) stay constant, the vector field itself might change because the basis is rotating or stretching. This change is captured by the second term including the Christoffel symbols, which measure how the coordinate frame twists and turns.</li> </ul> <p>Consequently, covariant derivative is basically the combination of component change and basis change, and it‚Äôs obvious from above the Christoffel symbols entirely dictate the properties of the chosen connection.</p> <hr/> <h2 id="4-a-short-reflection-and-discussion">4] A Short Reflection and Discussion</h2> <p>Let‚Äôs do a short summary and discussion before we continue. Based on the above definition, what is an affine connection conceptually? Well, an affine connection gives us a way to compare vectors at nearby points, which later will turn out to be our abilities to define</p> <ul> <li>1] a notion of parallel transport</li> <li>2] a notion of geodesics</li> <li>3] a notion of curvature</li> </ul> <p>It is the structure that tells us how to move vectors around on a manifold. Without it, differentiation makes no sense.</p> <p>At the same time, notice that apriori we <strong>choose</strong> this arbitrary connection. There are infinitely many affine connections on a manifold. Why? Recall the local definition of connection in coordinates \(\nabla_{X_i} X_j = \Gamma^k_{ij} X_k\). The Christoffel symbols \(\Gamma^k_{ij}\) can be any smooth functions. This already tells us that there are infinitely many possible connections. Nothing forces a specific choice at this stage. In fact, If \(\nabla\) is a connection and \(A\) is any smooth \((1,2)\)-tensor field, then \(\tilde{\nabla}_XY = \nabla_XY + A(X, Y)\) is another connection. So the space of connections is affine, not linear and thus there is enormous freedom. This is why it‚Äôs called ‚Äúaffine connection‚Äù.</p> <p>It‚Äôs only after we impose some natural geometric conditions that one becomes canonical (more to reveal below). And after introducing <code class="language-plaintext highlighter-rouge">Riemmanian metric</code>, there‚Äôre specific properties of connection that we demand which render only one unique option left (kind of miracle). That‚Äôs why we do not drown in choices.</p> <p>But let me also reinforce the idea that at this stage, no Riemannian metric at all is assumed. The definition of connection has nothing to do with any metric (we will see shortly that in some sense we do need to marry these two notions in a nice way; if this is your intuition, keep immersing yourself in it).</p> <p>Another crucial point is that, you might object that after all why can‚Äôt we just subtract tangent vectors using the ambient Euclidean structure? If you visualize a manifold as a curved surface floating in 3D, there‚Äôs clearly a nice inner product structure already defined in \(\mathbb{R}^3\) which we effortlessly borrow.</p> <p>Well, this is the deeper part, echoeing all the way back to why we even want a manifold. Recall that a general manifold has no built-in shape. A smooth manifold is just a set of points with coordinate charts which are glued together smoothly. That‚Äôs it. There‚Äôs no concept of length, angle, curvature, or embedding. Just smooth structure. We should <strong>NOT</strong> think of a general manifold as a curved surface floating in space. That is only a visualization aid.</p> <p>Shape appears only after adding extra structure. For a smooth manifold, only differentiability exists. We can talk about vector fields, Lie brackets, and differential forms. But there‚Äôs <strong>no metric</strong> yet. Now if we add an affine connection, we could then ifferentiate vector fields (and also define parallel transport and talk about geodesics, see below). However, there‚Äôs still <strong>no notion of length or angle</strong>. It‚Äôs only after we introduce a Riemannian metric that we get a notion of lengths, angles, volumes, and curvature. Only here does something resembling ‚Äúshape‚Äù emerge. And remarkably, all of this can be defined without embedding.</p> <p>But before Riemannian metric, for a generic smooth manifold, embeddings are misleading. When we picture a sphere in \(\mathbb{R}^3\) or torus in space, we are seeing extrinsic geometry. But intrinsic geometry ignores how the object sits in space. A famous example is imagine a flat sheet of paper vs a cylinder made by rolling it. They look totaly different in \(\mathbb{R}^3\), but <strong>intrinsically they are the same geometry</strong>, because distances measured along the surface are unchanged. Consequently, intrinsic geometry does not care about bending in ambient space.</p> <p>Recall that Gauss proved in his Theorema Egregium that</p> <blockquote> <p>Curvature of a surface can be computed entirely from the <strong>metric</strong>.</p> </blockquote> <p>(Here Curvature specifically refers to Gaussian Curvature). It does not depend on how the surface sits in space. This was revolutionary, only to be pushed further by Riemann later that:</p> <blockquote> <p>Manifolds need not be embedded at all.</p> </blockquote> <p>They can exist abstractly.</p> <p>Consequently, if you are still thinking that a manifold is a curved surface in space, it might be helpful if you instead interpret manifold as an abstract smooth space and geometry is an additional structure placed on it. And more importantly,</p> <blockquote> <p>The <strong>metric</strong> determines what ‚Äúcurved‚Äù even means.</p> </blockquote> <p>The manifold is just an abstract collection of points with smooth structure, not an object with a fixed geometric shape. Shape emerges only after adding a metric.</p> <p>Now, with the above discussions, suppose we do have an embedding: \(M \in \mathbb{R}^n\). We embed it. Vectors lie in tangent planes inside \(\mathbb{R}^n\). Then yes, we can subtract them as vectors in \(\mathbb{R}^n\). So what‚Äôs the problem? Well, it lies in again the insight Gauss offered back in 1827: <strong>intrinsic vs extrinsic</strong> geometry.</p> <blockquote> <p>Let \(V(t) \in T_{\alpha(t)}M\). If we compute the ordinary derivative \(\frac{dV}{dt}\) (pretend that \(M\) never has existed), it sure lives in in \(\mathbb{R}^n\) , but generally \(\frac{dV}{dt} \notin T_{\alpha(t)}M\). It usually has a normal component. That normal component measures how the surface bends in space. But that bending <strong>depends on the specific embedding</strong> we chose.</p> </blockquote> <p>For example, on the sphere, if we carry a tangent vector along a great circle, the ambient derivative will point slightly inward (the normal component). But that normal part has nothing to do with intrinsic geometry, and it only reflects how the sphere sits in space. To get intrinsic change, we would take ambient derivative and project back to the tangent plane. Spoiler alert: this projection then defines the Levi-Civita connection of the induced metric.</p> <p>To answer the question why we cannot always use this trick?</p> <blockquote> <p>Not all manifolds come with a preferred embedding. Manifolds might be embedded but the embedding is not unique. Different embeddings give different ambient derivatives and the induced extrinsic correction indeed depends on embedding.</p> </blockquote> <blockquote> <p>On the other hand, intrinsic geometry should not depend on how we sit in space. If we rely on ambient subtraction, we‚Äôre measuring extrinsic curvature. But Riemannian geometry is centered around exploring intrinsic properties. The connection should not depend on embedding.</p> </blockquote> <p>Using Euclidean subtraction measures the intrinsic change plus the extrinsic bending, whereas the covariant derivative isolates and includes intrinsic change only.</p> <p>Enough deliberation for now, let‚Äôs continue our story.</p> <hr/> <h2 id="5-covariant-derivative-along-a-curve">5] Covariant Derivative Along a Curve</h2> <p>In many scenarios, we are interested in how a vector field ‚Äúchanges‚Äù on a specific trajectory. With an affine connection defined as above, how could we calculate such change?</p> <p>Given a curve \(\alpha(t)\) and vector field along the curve: \(V(t) \in T_{\alpha(t)}M\). Again, the ordinary derivative $\frac{dV}{dt}$ makes no sense because:</p> \[\frac{dV}{dt} = \lim_{h \rightarrow 0}\frac{V(t + h) - V(t)}{h}\] <p>but \(V(t+v) \in T_{\alpha(t+h)}M\), \(V(t) \in T_{\alpha(t)}M\), and subtraction requires vectors to live in the same space.</p> <p>To resolve this conflict and apply the defined connection, we define:</p> \[\frac{DV}{dt} = \nabla_{\alpha'(t)} V\] <p>where \(\alpha'(t) = \frac{d\alpha}{dt} \in T_{\alpha(t)}M\).</p> <p>which is called the <code class="language-plaintext highlighter-rouge">covariant derivative</code>. In other words, the covariant derivative along a curve is just the affine connection applied in the direction of the velocity vector. It‚Äôs not new machinery ‚Äî it‚Äôs just the connection specialized to a curve.</p> <p>In coordinates, if we write \(V(t) = V^k(t)\frac{\partial}{\partial x^k}\):</p> \[\frac{DV^k}{dt} = (\frac{dV^k}{dt} + \Gamma^k_{ij} \frac{dx^i}{dt} V^j)\frac{\partial}{\partial x^k}\] <p>This is just the previous formula about coordinate representation of connections if we replace \(X = \alpha'(t)\). Consequently, we could carry the same interpretation: Covariant derivative = change in components+change in basis.</p> <p>We could interpret it this way: Imagine walking on a sphere. We carry an arrow that we try to keep ‚Äúpointing in the same direction.‚Äù Even if the arrow looks constant to you, from an external viewpoint it may be rotating because:</p> <blockquote> <p>The tangent plane itself rotates as you move.</p> </blockquote> <blockquote> <p>The coordinate basis changes.</p> </blockquote> <p>So \(\frac{D V}{dt}\) actually measures the true rate of change of the vector relative to/correcting for the manifold‚Äôs geometry. There are two kinds of change: 1)Intrinsic change ‚Äî the vector itself changes‚Äô 2) Fake change ‚Äî the coordinate frame changes. But \(\frac{D V}{dt}\) removes the fake change. It measures intrinsic change.</p> <hr/> <h2 id="6-parallel-vector-field-and-parallel-transport">6] Parallel Vector Field and Parallel Transport</h2> <p>Then the natural question to ask is what does it mean to have \(\frac{DV}{dt} = 0\)?</p> <p>Well, there‚Äôs a specific name for it ‚Äî the vector field \(V\) such \(\frac{DV}{dt} = 0\) for a specific curve \(c\) is called a <code class="language-plaintext highlighter-rouge">parallel vector field</code> along the \(c\). Geometrically, this means that the vector is being transported without turning according to the manifold‚Äôs geometry.</p> <p>On the other hand, the vector \(V(t)\) on \(V\) is said to <code class="language-plaintext highlighter-rouge">parallel transport</code> along \(c\).</p> <p>Parallel transport is obtained by solving the following system of ODE:</p> \[\frac{dV^k}{dt} + \Gamma^k_{ij} \frac{dx^i}{dt}V^j = 0\] <p>Notice that this is just \(n\) first order equations. Pointwise Christoffel data determines global comparison via integration.</p> <p>This offers us another way to think of $\frac{D V}{dt}$:</p> <blockquote> <p>Take \(V(t+h)\), parallel transport it back to \(T_{\alpha(t)}M\), subtract \(V(t)\), divide by $h$. Then take the limit.</p> </blockquote> <p>This is the hidden geometric definition.</p> <hr/> <h2 id="7-geodesics">7] Geodesics</h2> <p>Closely building upon the above idea of parallelism, we define a curve to be a <code class="language-plaintext highlighter-rouge">geodesic</code> if:</p> \[\frac{D\alpha'}{dt} = 0\] <p>Meaning:</p> <blockquote> <p>The velocity vector transports itself parallelly.</p> </blockquote> <p>Geometric, this means that there is no intrinsic acceleration and the curve does not ‚Äúturn‚Äù inside the manifold. This generalizes straight lines.</p> <p>Another physical way to interpret this is that a geodesic has only acceleration perpendicular to the surface.</p> <p>A quick sanity check: In Euclidean space \(\mathbb{R}\), since the basis vectors then Christoffel symbols vanish.</p> <p>Consequently, \(\frac{DV}{dt} = \frac{dV}{dt}\) ‚Äî Covariant derivative reduces to ordinary derivative, and this demonstrates that the definition of connection is truly a generalization.</p> <p>And if we solve</p> \[\frac{d^2\alpha^k}{dt^2} + \Gamma^k_{ij} \frac{dx^i}{dt}\alpha^j = 0\] <p>where we plug in \(V^j = \alpha^j\),</p> <p>we will arrive at a straight line, which is <strong>the straight line</strong> in Euclidean space.</p> <hr/> <h1 id="7-infinite-choices-of-connections">7. Infinite Choices of Connections</h1> <p>There are infinitely many connections.</p> <p>If $\nabla$ is a connection and $A$ is a $(1,2)$-tensor:</p> \[\widetilde{\nabla}_X Y = \nabla_X Y + A(X,Y)\] <p>is another connection.</p> <p>The space of connections is affine.</p> <hr/> <h1 id="8-metric-compatibility">8. Metric Compatibility</h1> <p>Condition:</p> \[X \langle Y,Z \rangle = \langle \nabla_X Y, Z \rangle + \langle Y, \nabla_X Z \rangle\] <p>Equivalent to:</p> \[\nabla g = 0\] <p>Meaning:</p> <p>Parallel transport preserves:</p> <ul> <li>Length</li> <li>Angles</li> <li>Inner products</li> </ul> <p>Interpretation:</p> <blockquote> <p>The connection moves vectors without distorting measurement.</p> </blockquote> <p>Metric compatibility controls stretching.</p> <hr/> <h1 id="9-torsion-free-symmetry">9. Torsion-Free (Symmetry)</h1> <p>Torsion tensor:</p> \[T(X,Y) = \nabla_X Y - \nabla_Y X - [X,Y]\] <p>If $T = 0$:</p> \[\nabla_X Y - \nabla_Y X = [X,Y]\] <p>Geometric meaning:</p> <ul> <li>No artificial twisting.</li> <li>Infinitesimal parallelograms close.</li> <li>Connection respects intrinsic commutation of flows.</li> </ul> <p>In coordinates:</p> \[\Gamma^k_{ij} = \Gamma^k_{ji}\] <p>Torsion-free controls twisting.</p> <hr/> <h1 id="10-levi-civita-connection">10. Levi-Civita Connection</h1> <p>Imposing:</p> <ol> <li>Metric compatibility</li> <li>Torsion-free</li> </ol> <p>There exists exactly one such connection.</p> <p>This is the <strong>Levi-Civita connection</strong>.</p> <p>It removes stretching and twisting.</p> <p>Only curvature remains.</p> <hr/> <h1 id="11-lie-brackets-of-coordinate-vector-fields">11. Lie Brackets of Coordinate Vector Fields</h1> <p>For coordinate fields:</p> \[X_i = \frac{\partial}{\partial x^i}\] <p>We always have:</p> \[[X_i, X_j] = 0\] <p>This is independent of connection.</p> <p>Torsion-free does not force Lie bracket to vanish.</p> <p>It ensures the connection respects the bracket structure:</p> \[\nabla_{X_i}X_j - \nabla_{X_j}X_i = [X_i,X_j]\] <hr/> <h1 id="12-why-christoffel-symbols-are-defined-pointwise">12. Why Christoffel Symbols Are Defined Pointwise</h1> <p>Concern:</p> <p>Connection compares vectors at different points, yet $\Gamma^k_{ij}$ are defined at one point.</p> <p>Resolution:</p> <p>A connection describes infinitesimal change.</p> <p>Just like:</p> \[f(b) - f(a) = \int_a^b f'(t)\,dt\] <p>Christoffel symbols encode infinitesimal twisting of the frame.</p> <p>Solving the parallel transport ODE gives comparison between points.</p> <p>Local data ‚Üí differential equation ‚Üí global transport.</p> <hr/> <h1 id="13-intrinsic-vs-extrinsic-derivative">13. Intrinsic vs Extrinsic Derivative</h1> <p>If $M \subset \mathbb{R}^n$:</p> <p>You can compute ambient derivative $\frac{dV}{dt}$.</p> <p>But generally:</p> \[\frac{dV}{dt} \notin T_{\alpha(t)}M\] <p>It has a normal component.</p> <p>Projecting back to tangent space gives intrinsic derivative.</p> <p>However:</p> <ul> <li>Different embeddings give different projections.</li> <li>Intrinsic geometry should not depend on embedding.</li> </ul> <p>Connections defined intrinsically avoid embedding dependence.</p> <hr/> <h1 id="14-what-is-a-manifold">14. What Is a Manifold?</h1> <p>A smooth manifold is:</p> <ul> <li>An abstract set of points</li> <li>With smooth coordinate charts</li> </ul> <p>No built-in:</p> <ul> <li>Length</li> <li>Angles</li> <li>Curvature</li> </ul> <p>Shape appears only after adding structure.</p> <hr/> <h1 id="15-structural-hierarchy">15. Structural Hierarchy</h1> <p>Smooth structure ‚Üí calculus<br/> Connection ‚Üí differentiation<br/> Metric ‚Üí measurement<br/> Curvature ‚Üí failure of parallel transport to commute</p> <p>Connections exist without metrics.</p> <p>Metrics are additional structure.</p> <hr/> <h1 id="16-why-connections-exist-without-metrics">16. Why Connections Exist Without Metrics</h1> <p>Differentiation requires:</p> <ul> <li>Smooth structure</li> <li>Rule comparing tangent spaces infinitesimally</li> </ul> <p>It does not require:</p> <ul> <li>Inner product</li> <li>Length</li> <li>Angle</li> </ul> <p>Connections are about motion of vectors.</p> <p>Metrics are about measurement.</p> <hr/> <h1 id="17-embedding-is-not-essential">17. Embedding Is Not Essential</h1> <p>Manifolds need not be embedded.</p> <p>Geometry should not depend on ambient space.</p> <p>Intrinsic geometry depends only on internal structures:</p> <ul> <li>Connection</li> <li>Metric</li> </ul> <hr/> <h1 id="18-final-synthesis">18. Final Synthesis</h1> <p>Connection:</p> <blockquote> <p>Infinitesimal rule for comparing tangent spaces.</p> </blockquote> <p>Christoffel symbols:</p> <blockquote> <p>Local coefficients describing how frames twist.</p> </blockquote> <p>Metric compatibility:</p> <blockquote> <p>No stretching under transport.</p> </blockquote> <p>Torsion-free:</p> <blockquote> <p>No artificial twisting.</p> </blockquote> <p>Levi-Civita:</p> <blockquote> <p>Unique natural connection for a Riemannian metric.</p> </blockquote> <p>Intrinsic geometry:</p> <blockquote> <p>Independent of embedding.</p> </blockquote> <hr/> <h1 id="one-sentence-summary">One-Sentence Summary</h1> <p>A connection encodes infinitesimal comparison of tangent spaces;<br/> metric compatibility preserves lengths and angles;<br/> torsion-free removes artificial twisting;<br/> and from these local rules, global geometry emerges.</p> <hr/> <p>Now your earlier question becomes clearer:</p> <p>If we define derivatives using an embedding, we are using extra structure.</p> <p>But intrinsic geometry should depend only on data defined on the manifold itself.</p> <p>That‚Äôs why the Levi-Civita connection is defined purely from the metric.</p> <p>No embedding required.</p> <p>Theoretical (sanity check) questions to ponder: 1] Why curvature is intrinsic 2] Why connections exist even without metrics 3] Or how geodesics make sense without embedding</p> <h2 id="discussions">Discussions</h2> <p>When I first learned about connection/parallel transport/geodesics, I have tons of questions and different topics seem to mingle with one another, each defying the others‚Äô validity. After months of delibration and study, I finally figured out the inner workings of these concepts, and I have to admit that I‚Äôm still deepening my understanding.</p> <p>I‚Äôll not regurgitate statements and clarifications made in the previous ‚Äúshort summary‚Äù section 4], but instead think back on a few other critical questions to ponder. I highly recommen that readers go through 4] before reading this section.</p> <blockquote> <p>1) Why do connections exist even without metrics?</p> </blockquote> <p>I had this thought which I deemed natural and hard to wrap my head aroud: If connections are about measuring changes, and metrics measure geometry, why can a connection exist without a metric?</p> <p>Well, let‚Äôs stay back a ste and think towards what a connection does and does <em>NOT</em> do. Notice that a connection gives us a way to differentiate vector fields, to compare nearby tangent spaces. From there the notion of parallel transport and geodesics. However, notice that there‚Äôs nowhere we mentioned lengths or angles. A connection is about how vectors move, not about how long they are.</p> <p>Furthermore, <em>differentiation itself does not require a metric</em>. Think about perhaps ordinary calculus. When we compute \(\frac{df}{dx}\), we do not need an inner product. Differentiation only needs smooth structure.</p> <p>Similarly, on a manifold, to differentiate vector fields, we only need a mooth structure, a rule for comparing tangent spaces which we call/define as the connection. A metric is not logically required for this.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Exploration of Connection]]></summary></entry><entry><title type="html">High Dimensional Nitty-gritty (2): Z-scoring before PCA?</title><link href="https://jasmineruixiang.github.io/blog/2026/covariance/" rel="alternate" type="text/html" title="High Dimensional Nitty-gritty (2): Z-scoring before PCA?"/><published>2026-02-09T01:22:40+00:00</published><updated>2026-02-09T01:22:40+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/covariance</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/covariance/"><![CDATA[<p>This blog originates from a daily discussion of neural signal (pre)processing with my mentors and peers. People utilize z-scoring and PCA all the time, and it‚Äôs a little shameful to admit by hindsight that I haven‚Äôt dwelled on the following question deep enough. Again, we reencounter the conundrum in high dimensional observations haunted by irreducible noise, under which lies our ambitious intent to extract robust and effective information.</p> <hr/> <h3 id="0-problem-setup">0] Problem Setup</h3> <p>Let‚Äôs say we have a collection of neural data in the format \(X \in \mathbb{R}^{n \times d}\), where \(n\) is the number of samples, and \(d\) the number of features. These are raw features each coming from a single electrode. For simplicity let‚Äôs assume that there is only 1 kind of feature, like threshold crossing. Let‚Äôs further say that we want to visualize the low dimensional structure of this collection of data, preserving its global geometry as much as possible (we‚Äôll clarify this later).</p> <p>Now we‚Äôd like to apply PCA on it for the first try, meaning to transform from \(\mathbb{R}^{n \times d}\) into \(\mathbb{R}^{n \times d'}\), where \(d'\) might be just \(3\), for example. Many methods would start by z-scoring \(X\) for each feature (so for each column of \(X\), after z-scoring would be mean 0 and standard deviation 1; for more discussions please refer to the <a href="https://jasmineruixiang.github.io/blog/2026/zscore/">previous blog</a> of this series) before applying PCA.</p> <p>I‚Äôm a little unsure about the motivation behind it. There‚Äôre of course many but I cannot pinpoint a conclusive answer.</p> <p>More importantly, I‚Äôm naively concerned that if we apply this feature-wise normalization, whether that would still preserve the covariance matrix between the original features (the diagonal will be 1, but I‚Äôm wondering how the off-diagonal terms would possibly change, or ‚Ä¶ would they).</p> <p>Let‚Äôs begin.</p> <hr/> <h3 id="1-what-pca-actually-does">1] What PCA actually does</h3> <p>The very first step: a quick review of what PCA is doing:</p> <blockquote> <p>Standard PCA:</p> <ol> <li>Center the data (subtracts column-wise/global means across samples; For the following, \(X\) will denote the centered data, if without specifications)</li> <li>Compute the <strong>covariance matrix</strong>: \(\Sigma = \frac{1}{n}X^TX\)</li> <li>Find eigenvectors of \(\Sigma\)</li> </ol> </blockquote> <p>PCA finds directions of maximal variance in the original coordinate system. We could also interpret PCA as finding minimization of reconstruction error, but that‚Äôs not explicitly helpful for deepening our interpretation here. However, I‚Äôll provide another useful perspective in section 3] from the angle of constrained optimization, but this covariance interpretation is what we will grapple with now for this section ‚Äî</p> <p>Because it already makes it obvious that</p> <blockquote> <p>PCA is sensitive to feature scale.</p> </blockquote> <p>If one electrode has variance 100 and another has variance 1, the first will dominate the principal components ‚Äî even if its structure might not be more meaningful. And that‚Äôs exactly where z-scoring makes a huge distinction.</p> <hr/> <h3 id="2-what-does-z-scoring-do">2] What does z-scoring do?</h3> <p>Column-wise z-scoring transforms</p> \[\tilde{X}_{ij} = \frac{X_{ij} - \mu_j}{\sigma_j}\] <p>(Here‚Äôs a bit of abuse of notation, as \(\tilde{X}_{ij}, X_{ij}\) are scalars while \(\mu_j, \sigma_j \in \mathbb{R}^{1\times d}\))</p> <p>If we use matrix notation, \(D = diag{(\sigma_1, \cdots, \sigma_n)}\), then the above could be simplified into (again, assume it‚Äôs already centered):</p> \[\tilde{X} = XD^{-1}\] <p>Now if we look at the new covariance matrix:</p> \[\tilde{\Sigma} = \frac{1}{n}\tilde{X}^T\tilde{X} = \frac{1}{n}D^{-1}X^TXD^{-1} = D^{-1}\Sigma D^{-1}\] <p>This is obviously <strong>not the same</strong> covariance matrix.</p> <p>In fact, it‚Äôs not hard to see that, from simple linear algebra:</p> \[\tilde{\Sigma}_{ij} = \frac{\Sigma_{ij}}{\sigma_i \sigma_j}\] <p>And this turns out to be exactly the <strong>correlation matrix</strong>.</p> <p>So:</p> <blockquote> <p>PCA on z-scored data = PCA on the <code class="language-plaintext highlighter-rouge">correlation matrix</code></p> </blockquote> <blockquote> <p>PCA on raw centered data = PCA on the <code class="language-plaintext highlighter-rouge">covariance matrix</code></p> </blockquote> <p>So back to one of our original questions: does normalization preserve <code class="language-plaintext highlighter-rouge">covariance</code> between original features?</p> <p>No. The off-diagonal terms change as:</p> \[\frac{\Sigma_{ij}}{\sigma_i \sigma_j} \leftarrow \Sigma_{ij}\] <p>So they become <code class="language-plaintext highlighter-rouge">correlations</code> (or we could say that the correlations are preserved). The important distinction is that <code class="language-plaintext highlighter-rouge">covariance</code> measures co-variation in physical units, while <code class="language-plaintext highlighter-rouge">correlation</code> measures co-variation relative to each variable‚Äôs scale. So z-scoring does not preserve the original covariance geometry: It preserves the <code class="language-plaintext highlighter-rouge">correlation</code> <em>structure</em> instead.</p> <hr/> <h3 id="3-a-geometric-way-to-think-about-this">3] A geometric way to think about this</h3> <p>The above covariance calculation is clear, yet we could reinterpret PCA in a different way by variational characterization of eigenvectors, i.e. the Rayleigh quotient formulation of PCA (depending on your views, these two migth be considered the exact same thing; but even as an explanation for why we care about eigenvectors of the covariance, let me elaborate below).</p> <h4 id="31-pca-as-a-variational-problem">3.1] PCA as a Variational Problem</h4> <p>Let \(X \in \mathbb{R}^{n \times d}\) be the centered data and the sample covariance matrix</p> \[\Sigma = \frac{1}{n} X^\top X\] <p>If we project the data onto a direction \(v \in \mathbb{R}^d\), the projected variance is:</p> \[\mathrm{Var}(Xv) = \frac{1}{n} \|Xv\|^2 = v^\top \Sigma v\] <p>Therefore, the first principal component solves:</p> \[\max_{\|v\| = 1} v^\top \Sigma v\] <p>This is the Rayleigh quotient, and the solution is the top eigenvector of \(\Sigma\) (not proved here; many other sources exist online).</p> <h4 id="32-pca-after-z-scoring">3.2] PCA After Z-Scoring</h4> <p>Suppose we z-score each feature (column-wise normalization). Again, let me reiterate from the above that</p> \[D = \mathrm{diag}(\sigma_1, \dots, \sigma_d)\] <p>and the transformed data is:</p> \[\tilde{X} = X D^{-1}\] <p>The new covariance matrix becomes:</p> \[\tilde{\Sigma} = \frac{1}{n} \tilde{X}^\top \tilde{X} = D^{-1} \Sigma D^{-1}\] <p>PCA on z-scored data solves:</p> \[\max_{\|v\| = 1} v^\top D^{-1} \Sigma D^{-1} v\] <hr/> <h4 id="33-equivalent-reformulation-changing-the-constraint">3.3] Equivalent Reformulation (Changing the Constraint)</h4> <p>Now let‚Äôs do a simple trick. Let:</p> \[w = D^{-1} v \quad \text{so that} \quad v = D w\] <p>Substitute into the objective:</p> \[v^\top D^{-1} \Sigma D^{-1} v = (Dw)^\top D^{-1} \Sigma D^{-1} (Dw) = w^\top \Sigma w\] <p>Now examine the constraint:</p> \[\|v\|^2 = 1\] \[v^\top v = (Dw)^\top (Dw) = w^\top D^2 w\] <p>So the optimization becomes:</p> \[\max_{w^\top D^2 w = 1} w^\top \Sigma w\] <hr/> <h4 id="34-geometric-interpretation">3.4] Geometric Interpretation</h4> <p>The raw PCA solves:</p> \[\max_{v^\top v = 1} v^\top \Sigma v\] <p>Now, the z-scored PCA solves:</p> \[\max_{w^\top D^2 w = 1} w^\top \Sigma w\] <p>So at a glance from 2], z-scoring rescales the covariance matrix into the correlation matrix.</p> <p>But viewed from this different perspective, it <strong>changes the metric constraint</strong>. Instead of using the standard Euclidean norm:</p> \[v^\top v\] <p>we now use a weighted norm:</p> \[w^\top D^2 w\] <p>This means:</p> <ul> <li>Raw PCA assumes the <strong>standard Euclidean</strong> inner product.</li> <li>Z-scored PCA uses a different inner product <strong>induced by \(D^2\)</strong>.</li> </ul> <hr/> <h4 id="35-multi-dimensional-pca-k-components">3.5] Multi-Dimensional PCA (k Components)</h4> <p>Up to this point, you might object that the above is just to find one single direction/one principal component. Usually we do multiple components. Well, there isn‚Äôt too much effort for an extension.</p> <p>Raw PCA solves:</p> \[\max_{V^\top V = I} \mathrm{Tr}(V^\top \Sigma V)\] <p>where \(V \in \mathbb{R}^{d \times k}\). The solution is the top-\(k\) eigenvectors of \(\Sigma\) (proof omitted, similar as 3.1]). As in 3.2], After z-scoring, we solve:</p> \[\max_{V^\top V = I} \mathrm{Tr}(V^\top D^{-1} \Sigma D^{-1} V)\] <p>Using the substitution \(V = D W\), this naturally becomes (similar to 3.3]):</p> \[\max_{W^\top D^2 W = I} \mathrm{Tr}(W^\top \Sigma W)\] <hr/> <h4 id="36-generalized-eigenvalue-interpretation">3.6] Generalized Eigenvalue Interpretation</h4> <p>What does this mean geometrically about the solution?</p> <ul> <li>For the raw PCA: <ul> <li>Orthonormal basis in standard <strong>Euclidean</strong> metric</li> <li>Maximizes variance</li> </ul> </li> <li>For Z-scored PCA: <ul> <li>Orthonormal basis under <strong>weighted</strong> metric \(D^2\)</li> <li>This is equivalent to solving a <strong>generalized eigenvalue</strong> problem:</li> </ul> </li> </ul> \[\Sigma w = \lambda D^2 w\] <p>I‚Äôll also skip the details of why the solution is equivalent to finding the generalized eigenvalues/eigenvectors. However, this fact informs us of the fundamental framework of PCA:</p> <p>the big geometric insight is that PCA always solves:</p> \[\max_{W^\top G W = I} \mathrm{Tr}(W^\top \Sigma W)\] <p>where \(G\) defines the metric.</p> <ul> <li>Raw PCA: \(G = I\)</li> <li>Z-scored PCA: \(G = D^2\)</li> </ul> <p>So z-scoring means that we are not trusting that Euclidean length in raw coordinates is meaningful. We redefine what unit length means.</p> <p>Which is consistent with the previous observation that</p> <ul> <li>Raw PCA preserves covariance geometry.</li> <li>Z-scored PCA preserves correlation geometry.</li> </ul> <hr/> <h3 id="4-what-geometry-are-we-preserving">4] What geometry are we preserving?</h3> <p>To some extent, this is the real conceptual issue.</p> <p>If we do PCA without z-scoring, it preserves the Euclidean geometry in the original feature space. Variance magnitude is meaningful because we indeed keep such information. We‚Äôd hold the underlying premise that</p> <blockquote> <p>Electrodes with larger variance are considered more important.</p> </blockquote> <p>This suggests that this methood is good if variance magnitude reflects real neural signal strength or the feature scale is physically meaningful.</p> <p>On the other hand, if we do PCA after z-scoring, then it would preserve geometry under a <strong>reweighted metric</strong> and all dimensions are treated equally. Each electrode is thus given equal prior importance.</p> <p>This should work if variance differences are arbitrary (e.g., electrode gain differences) and we care about patterns of co-variation, not absolute magnitude.</p> <p>Since neural data often has:</p> <ul> <li>Different firing rates across electrodes</li> <li>Different noise levels</li> <li>Different dynamic ranges</li> </ul> <p>If we don‚Äôt z-score:</p> <blockquote> <p>High firing-rate neurons dominate PCA.</p> </blockquote> <p>whereas if we do z-score:</p> <blockquote> <p>Each neuron contributes equally in variance units.</p> </blockquote> <hr/> <h3 id="5-which-one-preserves-global-geometry">5] Which one preserves global geometry?</h3> <p>This depends on what geometry we think is meaningful.</p> <p>If our raw space is: \(\mathbb{R}^d\) with standard Euclidean metric, then PCA without z-scoring preserves global geometry better. If we believe that true geometry should not depend on firing rate scale, then z-scoring defines a more appropriate metric (as elaborated in section 3]):</p> \[&lt;x, y&gt;_D = x^T(D^{-1})^{2}y\] <p>which means we could alternatively interpret this as keeping the underlying space unchanged but essentially altering the metric before doing PCA.</p> <p>Usually in systems neuroscience people often z-score across time and then do PCA. The reason behind is that neural manifold studies often care about relative population patterns, not which neuron fires more. If we want true population variance magnitude, then we should not z-score. If we intend to obtain population structure independent of scale, then z-score.</p> <hr/> <h3 id="6-summary">6] Summary</h3> <p>In conclusion, if we do z-scoring before PCA, then</p> <ul> <li>Z-scoring does NOT preserve the <code class="language-plaintext highlighter-rouge">covariance</code> matrix.</li> <li>It converts <code class="language-plaintext highlighter-rouge">covariance</code> to <code class="language-plaintext highlighter-rouge">correlation</code>.</li> <li>Off-diagonal terms are divided by product of standard deviations.</li> <li>Equivalently, we are changing the <strong>metric</strong> of the space.</li> <li>PCA result can change dramatically depending on scaling.</li> </ul> <p>Finally, in practice we often have more than one kind of features. For example, we could obtain both threshold crossings and spike band power from each electrode at the same time. However, these two measures have drastically different scales. In this scenario, of course we could look into them separately, but if combined, the spike power would dominate. Consequently, z-scoring also helps to re-weight the feature importance apriori.</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Brain Computer Interface"/><summary type="html"><![CDATA[Zscoring, covariance, PCA]]></summary></entry><entry><title type="html">The Dance of Space: Geom/Topo/Dynam Mumble(5)</title><link href="https://jasmineruixiang.github.io/blog/2026/geodesics/" rel="alternate" type="text/html" title="The Dance of Space: Geom/Topo/Dynam Mumble(5)"/><published>2026-02-08T14:58:02+00:00</published><updated>2026-02-08T14:58:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/geodesics</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/geodesics/"><![CDATA[<p>This weekend I was (re)thinking about the geometric connotations of geodesics, and that reminds me of a brilliant illustration with an intuitive method the eloquent mathematician Tristan Needham noted down in his remarkable and mind-numbing book <em>Visual Differential Geometry and Forms</em>. I thought just a little more, and only to recover a long-lasting misconception about Gauss Theorem Egregium which I was both shameful to admit for possessing so long and excited to have it cleared out of my mind.</p> <p>However, as I dive more into Needham‚Äôs method, I discovered a potential and simple failure mode, which naturally echoes another important statement: The Gauss-Bonnet Theorem. This deepens my grasp of the tension between <strong>local isometry</strong> (the peeling method, see below) and <strong>global topology</strong> (the area enclosed).</p> <p>Again, since this is the mumble series, I‚Äôll not give all definitions and assume you have already known some flavor of the basics. Let‚Äôs begin.</p> <hr/> <h2 id="0-the-problem-confusion-setup">0] The Problem (Confusion) Setup</h2> <p>On Page 12-13, Tristan introduced a method to easily and intuitively construct geodesics on a curved surface (see figure [1.11] below). He claimed that ‚ÄúIf a narrow strip surrounding a segment G of a geodesic is cut out of a surface and laid flat in the plane, then G becomes a segment of a straight line.‚Äù Well, he presented an intuitive proof (figure [1.12] below), but I‚Äôm immediately reminded of <strong>Gauss‚Äô Theorem Egregium</strong> and there seemed to be something inconsistent (I‚Äôll omit other information and count on you to look up the basics).</p> <p>I was thinking: the peeling is an (local) isometry without doubt, so geodesics have to be preserved. That‚Äôs why geodesics ‚Äúpeeled‚Äù off from the surface (the fruit/vegetable used in the illustration) has to be geodesics in Eucliean 2D space, which is straight according to the normal definitions. However, as I‚Äôm thinking a bit more deeply, the Gaussian curvature does change (before it‚Äôs nonzero, on 2D it‚Äôs zero, which contradicts Theorem Egregium), which leads to me to think backwards towards using this Theorem Egregium again. Immediately I realize that Gaussian theorem egregium is applied only to 2D surfaces (not to 1D curve since there‚Äôs no ‚ÄúGaussian curvature‚Äù for a curve). Ah, shame to have blundered upon conceptual confusion. But treating this as an opportunity for an upgrade overhaul of my conceptual framework, let‚Äôs sort this out step by step and see what we might also dabble into.</p> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/VDGF/VDGF_1.11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/VDGF/VDGF_1.12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig [1.11] and [1.12] in <a class="citation" href="#VDGF">(Needham, 2021)</a>. </div> <h2 id="1-the-peeling-intuition">1] The ‚ÄúPeeling‚Äù Intuition</h2> <p>Let‚Äôs make it clear: in Figure [1.11] of Needham‚Äôs text, he introduces the following powerful intuitive tool:</p> <blockquote> <p>‚ÄúIf a narrow strip surrounding a segment $G$ of a geodesic is cut out of a surface and laid flat in the plane, then $G$ becomes a segment of a straight line.‚Äù</p> </blockquote> <p>At first glance, this indeed seems to challenge Gauss‚Äôs <strong>Theorema Egregium</strong>. Since Gaussian curvature (\(K\)) is an intrinsic invariant, how can a patch of a curved surface (\(K \neq 0\)) be laid flat (\(K = 0\)) without stretching or tearing?</p> <p>Well, the resolution lies in the <strong>limit</strong>. Gaussian curvature is a property of a 2D area. By taking a ‚Äúnarrow strip,‚Äù we are effectively reducing the 2D surface to a 1D curve and its immediate neighborhood. In the limit, as the width of the strip goes to zero, the ‚Äúarea‚Äù of the surface being considered goes to zero. In other words, the Gaussian curvature of the surface does not change because we aren‚Äôt flattening the whole surface; we are only flattening an ‚Äúinfinitesimally‚Äù narrow strip.</p> <hr/> <h2 id="2-geodesic-curvature-vs-gaussian-curvature">2] Geodesic Curvature vs. Gaussian Curvature</h2> <p>The Theorema Egregium applies to <strong>isometries</strong>. While we cannot map a patch of a sphere to a plane isometrically, we can map a curve and its first-order neighborhood to a plane isometrically (often called ‚Äúdeveloping‚Äù the strip; or just developable).</p> <p>We could think of it this way:</p> <blockquote> <p>The Surface: Has intrinsic curvature \(K\). The Strip: Because it is ‚Äúinfinitesimally narrow,‚Äù the Gaussian curvature doesn‚Äôt ‚Äú<strong>trap</strong>‚Äù the strip. We aren‚Äôt forcing the 2D relationships across the strip to remain the same; we are only preserving the lengths along the geodesic \(G\).</p> </blockquote> <p>Consequently, the core of the questions (why such an intuition makes sense, beyond the simple proof shown in Fig.[1.12]) does not lie in Gauss‚Äôs Theorem, but in another concept: the geodesic curvature: The Theorema Egregium is about the surface (\(K\)), but the property of being a ‚Äústraight line‚Äù is about Geodesic Curvature (\(\kappa_g\)):</p> <blockquote> <p>Gaussian Curvature (\(K\)) reflects the property of the surface. Geodesic Curvature (\(\kappa_g\)): A property of a curve relative to the surface. It measures how much the curve bends within the surface.</p> </blockquote> <p>The ‚Äústraightness‚Äù of the peeled strip is governed by <strong>Geodesic Curvature ($\kappa_g$)</strong>, not Gaussian Curvature (\(K\)).</p> <ul> <li><strong>Gaussian Curvature (\(K\)):</strong> A property of the surface itself (\(K = \kappa_1 \kappa_2\)). It dictates whether a 2D patch can be flattened.</li> <li><strong>Geodesic Curvature (\(\kappa_g\)):</strong> A property of a curve <em>relative</em> to the surface. It measures how much the curve ‚Äúveers‚Äù to the left or right within the surface.</li> </ul> <p>Recall that a geodesic is defined as a curve where \(\kappa_g = 0\) everywhere (Will do a another blog on the computation of geodesics from the perspective of modern (Riemannian) manifold). Because \(\kappa_g\) is an intrinsic property (it can be measured by flatlanders like intelligent ants living on the surface), it must be preserved under isometry.</p> <p>With this, we could demystify the underpinning logic of the peel:</p> <ol> <li>The act of peeling the strip is a local isometry along the curve.</li> <li>The \(\kappa_g\) of the curve on the surface was 0 (by definition of a geodesic).</li> <li>Therefore, the \(\kappa_g\) of the curve on the flat plane must also be 0 .</li> <li>In the Euclidean plane, a curve with zero curvature is a straight line.</li> </ol> <p>Notice that the 3rd step above relies on the fact that isometry preserves intrinsic properties and here the geodesic curvature, but again this is NOT Gauss Theorem Egregium (same fundamental fact but applied to different objects).</p> <p>Gaussian curvature is not defined for a 1D curve. A curve only has curvature (how much it bends in space) and geodesic curvature (how much it bends relative to the surface it‚Äôs on). By narrowing the strip until the 2D ‚Äúsurface‚Äù nature of the paper effectively vanishes, we are bypassing the restriction of the Theorema Egregium regarding the 2D area, while retaining the intrinsic measurement of the curve‚Äôs straightness. We are essentially ‚Äúcheating‚Äù the theorem by reducing the 2D surface to a 1D line where the concept of Gaussian Curvature has no grip.</p> <p>The Theorem Egregium is a negative constraint (it tells us what we can‚Äôt do with a 2D patch), but the Geodesic Curvature (\(\kappa_g\)) is the positive proof. It yet emphasizes again that it‚Äôs important to separate the topological ‚Äúbudget‚Äù of the surface (the Gaussian Curvature) from the local behavior of the curve (the Geodesic Curvature).</p> <p>There‚Äôs only one last piece left to make the story rigorous.</p> <hr/> <h2 id="3-physical-approximation">3] Physical Approximation</h2> <p>Let me paraphrase again in another way. This narrow strip, in practice, since it cannot be infinitesimally narrow, by Theorem Egregium, <strong>CANNOT</strong> lie flat on the plane, but the more narrow we could get, the better aligned it is with respect to the plane. If it goes to limit, then Gaussian Theorem Egregium does not apply because this limit 1D curve is no longer constrained by this theorem.</p> <p>On the other hand, even in practice we assume that after peeling it ‚Äúis‚Äù flat, this appeal to Theorem Egregium still does not ‚Äúprove‚Äù the fact that once laid down the strip would become straight; we still need to assort to the preserved geodesic curvature, which is covered above.</p> <p>Now, back to the reality. In practice, a strip of finite width \(w\) cannot lie perfectly flat if \(K \neq 0\). The ‚Äúerror‚Äù or distortion required to flatten it is proportional to the area of the strip (\(Area \approx Length \times w\)). As \(w \to 0\):</p> <ul> <li>The area vanishes.</li> <li>The constraint of the Theorema Egregium vanishes.</li> <li>The 1D ‚Äústraightness‚Äù remains perfectly preserved.</li> </ul> <p>Another way to view Needham‚Äôs trick is that the strip isn‚Äôt just being ‚Äúflattened‚Äù; it is being identified with the unrolled <strong>tangent developable</strong> of the geodesic. If we imagine a sequence of tangent planes along the geodesic, they form a ‚Äúribbon‚Äù that has zero Gaussian curvature (because it‚Äôs a developable surface, like a cylinder or a cone). Because this ribbon has \(K=0\), it can be laid perfectly flat in a plane without any distortion at all. Needham‚Äôs ‚Äúnarrow strip‚Äù is essentially a physical approximation of this developable ribbon.</p> <hr/> <h2 id="4-extension-of-the-original-trick-connection-to-gauss-bonnet">4] Extension of the original trick: Connection to Gauss-Bonnet</h2> <h3 id="41-failure-mode-of-a-strip-when-in-closed-loop">4.1] Failure mode of a strip when in closed loop</h3> <p>Now with the above puzzle cleared out, let‚Äôs think a little deeper into the next single, perhaps most natural topic: How about peeling off not just a single strip segment, but instead a strip which loops back into itself?</p> <p>Think about the physical reality of Needham‚Äôs experiment. If we cut a strip along a great circle (a closed geodesic) of a sphere:</p> <ul> <li>1] The Segment: If we cut just a small arc (say 30¬∞), the strip is essentially a tiny rectangle. Because it‚Äôs so narrow, the ‚Äúsurface tension‚Äù of the sphere‚Äôs curvature isn‚Äôt strong enough to prevent us from pressing it flat.</li> <li>2] The Loop: If we try to cut the entire great circle, we get a ‚Äúring‚Äù or a ‚Äúhoop.‚Äù On the sphere, this hoop has a specific circumference (\(C = 2\pi R\)).</li> <li>3] The Failure: If we try to lay that hoop perfectly flat on a table without stretching it, we‚Äôll find it impossible. To lie flat in Euclidean space as a circle, the relationship between its radius and circumference must be \(C = 2\pi r\). But on the sphere, the ‚Äúradius‚Äù (the distance from the pole to the equator) is an arc length. <strong>The geometry of the 2D area inside the hoop ‚Äúlocks‚Äù the hoop‚Äôs shape</strong>.</li> </ul> <p>Or maybe another perhaps more intuitive example ‚Äî</p> <blockquote> <p>The ‚ÄúPaper Cone‚Äù Analogy:</p> </blockquote> <p>Think of a paper cone (like a party hat).</p> <ul> <li> <p>1] Local Flattening: You can cut a narrow strip from the cone running from the tip to the base. You can lay this strip flat on the table perfectly. In fact, you can lay any part of the cone flat.</p> </li> <li> <p>2] Global Failure: But if you try to flatten the <em>entire</em> cone at once, you can‚Äôt. You have to make a cut. When you flatten it, the cut edges don‚Äôt meet; there is a angular gap.</p> </li> </ul> <p>Yet perhaps another thought experiment:</p> <blockquote> <p>The ‚ÄúTrain Track‚Äù Experiment</p> </blockquote> <p>Imagine the ‚Äúnarrow strip‚Äù as a set of flexible but straight train tracks.</p> <p>On the Sphere: You lay the tracks along the equator. They go all the way around and connect perfectly at the start.</p> <p>The ‚ÄúPeeling‚Äù (Transfer to Plane): Now you transfer these tracks to a flat Euclidean floor. Because the tracks are geodesics (straight), you must lay them down as a straight line on the floor. You keep laying them down, inch by inch.</p> <p>Now the Problem: On the floor, a straight line goes on forever. It never comes back to start.</p> <p>The Contradiction: To make the tracks close a loop on the floor, you would have to bend them (add Geodesic Curvature). But we know geodesics are straight!</p> <p>So, the ‚Äúnarrow strip‚Äù of a closed geodesic loop on a sphere becomes an infinite straight line on the plane. It loses its ‚Äúloop-ness‚Äù entirely.</p> <p>Later we will make it clear that this ‚Äúangular gap‚Äù is precisely what the Gauss-Bonnet Theorem calculates (\(\iint K dA\)). The curvature \(K\) inside the loop on the sphere is responsible for ‚Äúturning‚Äù the geometry so that it closes. The flat plane (\(K=0\)) lacks this ‚Äúturning power,‚Äù so the strip simply runs away in a straight line.</p> <p>But anyway, for now, in short: We can flatten a line because a line has no ‚Äúinside.‚Äù We cannot flatten a closed loop without accounting for the gap (which is what we will show later as the holonomy, or ‚Äúequivalently‚Äù integration of the Gaussian curvature of the area it encloses.)</p> <hr/> <h3 id="42-the-formula-and-geodesic-loop">4.2] The formula and geodesic-loop</h3> <p>We resort to The Gauss-Bonnet Theorem, which almost fits in immediately, since it bridges the gap between the local straightness of the geodesic and the global curvature of the surface.</p> <p>Well, the Gauss-Bonnet Theorem is essentially a ‚Äúbudgeting‚Äù equation. It tells us exactly how much ‚Äústraightness‚Äù we have to give up to account for the curvature of the surface. Let‚Äôs see if we could intuitively see its effect from the closed loop peeling failure.</p> <p>Let me state the theorem:</p> <p>For a simply connected region \(R\) bounded by a curve \(C\), the theorem states:</p> \[\iint_R K \, dA + \oint_C \kappa_g \, ds + \sum \alpha_i = 2\pi\] <p>\(K\) is the Gaussian curvature, \(\kappa_g\) is the geodesic curvature, \(\alpha_i\) are the exterior angles at any corners.</p> <p>Moreover, if we create a closed loop (like a triangle) using only geodesic segments, then \(\kappa_g = 0\) along the edges by definition. The middle term of the equation vanishes, leaving a direct relationship between the ‚Äúarea-integral of curvature‚Äù and how much the geodesics had to ‚Äúturn‚Äù at the corners to close the loop:</p> \[\iint_R K \, dA = 2\pi - \sum \alpha_i\] <p>Furthermore, if we use a smooth closed geodesic, there is no geodesic curvature (\(\kappa_g = 0\)) and also no corners (\(\sum \alpha_i = 0\)). The equation thus becomes:</p> \[\iint_R K \, dA = 2\pi\] <p>This tells us that for a smooth closed geodesic to exist, the total Gaussian curvature of the area it encloses must equal \(2\pi\) (the ‚Äúangle‚Äù of a full circle). This is why we can have a closed geodesic on a sphere (\(K &gt; 0\)), but we can never have a simple closed geodesic on a flat plane (\(K=0\)) or a saddle (\(K&lt;0\))‚Äîthe ‚Äúcurvature budget‚Äù doesn‚Äôt add up to \(2\pi\).</p> <p>Let us dwindle here for a while, as I feel like building up intuition, a feeling of this curvature budge is of significant importance. So let‚Äôs just try again to reinterpret this equation by the following simple example.</p> <p>To walk in a simple closed loop (a circle, a square, a blob) and end up facing the same way we started, we must physically turn a total of 360¬∞ (\(2\pi\) radians).</p> <p>However, here the Gauss-Bonnet theorem says there are two ways to pay for this 360¬∞ budget:</p> <blockquote> <ol> <li>Steering (\(\kappa_g\)): we physically turn our body (like turning a steering wheel).</li> <li>Surface Curvature (\(K\)): The ground itself curves underneath us, effectively ‚Äúturning‚Äù us without we realizing it.</li> </ol> </blockquote> <p>The equation is thus:</p> <p>\(\text{Steering} + \text{Surface Curvature} = 360^\circ\) \(\oint \kappa_g ds + \iint K dA = 2\pi\)</p> <p>Note that a geodesic is defined as a path where we do not steer. Our steering wheel is locked in the straight-ahead position.</p> <p>Therefore: Steering = 0.</p> <p>Now look at our budget equation again:</p> <p>\(0 + \text{Surface Curvature} = 360^\circ\) \(\iint K dA = 2\pi\).</p> <p>This means the only way to close a loop without steering is if the surface itself does 100% of the turning for us.</p> <p>Consequently, this would explain why it fails on the plane (\(K = 0\)). The surface or the plane is flat, which contributes 0¬∞ of turning. The geodesic contributes 0¬∞ of steering. Hence the result: \(0 + 0 = 0\). We have turned 0¬∞ and we are walking in a straight line forever. We will never close the loop. Thus our conclusion: simple closed geodesics cannot exist on a plane.</p> <p>Similarly, simple closed geodesics cannot exist on a saddle, because a saddle has negative curvature. It curves ‚Äúaway‚Äù from itself and contributes negative turning. Along a geodesic there‚Äôs no contribution of steering, and the net result is still negative, not \(2\pi\). We are actually diverging away from a closed loop. The surface is actively pushing us path apart.</p> <p>In contrast, a sphere has positive curvature. It curves ‚Äúinward‚Äù toward itself and contributes positive turning. If we enclose enough area (specifically, a hemisphere), the total positive curvature adds up to exactly \(2\pi\). The surface has bent us around exactly enough to meet our own tail without us ever turning the wheel.</p> <p>In some sense, the ‚ÄúCurvature Budget‚Äù is like a tax. To close a loop, we must pay \(2\pi\). On a Geodesic, we refuse to pay (we won‚Äôt steer). Therefore, the Landlord (Surface) must pay the entire tax for us. Only a positively curved landlord (Sphere) has the cash (\(K&gt;0\)) to pay it. The plane is broke (\(K=0\)), and the saddle is in debt ($K&lt;0$).</p> <p>Well, you may wonder in the above interpretation where do the exterior angle sum go. Recall that we‚Äôre talking about smooth geodesic loops. Because the loop is smooth, there are no sharp corners (‚Äúkinks‚Äù), so there are no exterior angles to sum up. The term \(\sum \alpha_i\) becomes exactly 0. You may also wonder, then how would geodesic ‚Äúturn‚Äù be different from the exterior angle kink turn? This is a little more subtle, but also intuitive.</p> <blockquote> <p>The ‚ÄúConservation of Turning‚Äù</p> <ul> <li>Usually, when we smooth out a shape (like turning a square into a circle), we don‚Äôt lose the turning angles; we just spread them out.</li> <li>Square (Polygon): We walk straight (\(\kappa_g=0\)) and make four sharp \(90^{\circ}\) turns.</li> </ul> </blockquote> <ul> <li>\(\text{Turning} = \sum \alpha_i = 360^{\circ}\). I‚Äôd more think about these sharp turns/kinks as some colossal mechanical arm sticking from space and grabs our car to turn it around certain angles at the current position.</li> <li> <p>Circle (Smooth Curve): We never make a sharp turn (\(\alpha_i=0\)), but we are constantly steering a tiny bit to the side (\(\kappa_g = \text{constant}\)).</p> </li> <li>\(\text{Turning} = \oint \kappa_g \, ds = 360^{\circ}\).</li> </ul> <p>In both cases, we provided the turning. However, for the geodesic ‚Äúmiracle‚Äù, a smooth closed geodesic is a bizarre object because it refuses to turn in either way: there‚Äôs no sharp corners (smooth, \(\sum \alpha_i = 0\)) and no steering (geodesic, \(\kappa_g = 0\)). So where does the mandatory \(360^{\circ}\) (\(2\pi\)) turning come from to close the loop? It must come entirely from the Gaussian Curvature (\(K\)) of the area we enclosed. The surface itself has to rotate the universe under our feet by exactly \(360^{\circ}\) while we walk in a ‚Äústraight‚Äù line.</p> <hr/> <h3 id="43-how-does-the-strip-fit-in">4.3] How does the ‚Äústrip‚Äù fit in?</h3> <p>Now, imagine applying Needham‚Äôs ‚Äúpeeling‚Äù trick to each side of a geodesic triangle.</p> <blockquote> <ol> <li>On the surface: We would have three ‚Äústraight‚Äù paths (geodesics) that enclose a region of curvature \(K\). Because of that \(K\), the interior angles sum to more than \(\pi\) (on a sphere).</li> <li><strong>The ‚ÄúPeeling‚Äù Conflict</strong>: If we tried to peel a ‚Äúnarrow strip‚Äù that followed the entire boundary of the triangle and lay it flat in one go, we would encounter a physical gap or an overlap where the ends meet.</li> </ol> </blockquote> <p>The Gauss-Bonnet Theorem effectively measures this ‚Äúgap.‚Äù The amount of Gaussian curvature ‚Äútrapped‚Äù inside the triangle is exactly equal to the ‚Äúholonomy‚Äù‚Äîthe amount a vector rotates when transported around that loop:</p> <hr/> <h3 id="44-holonomy-parallel-transport-and-the-gap">4.4] Holonomy, Parallel Transport, and the ‚ÄúGap‚Äù</h3> <p>There‚Äôs a simple and intuitive relationship between holonomy, parallel transport, and exterior angles.</p> <p>Let‚Äôs start with a simple thought experiment about parallel transport (you might have seen this everywhere):</p> <p>Imagine walking along a geodesic triangle on a sphere, carrying a spear (a vector) pointing straight ahead.</p> <ul> <li>Along the edge: Because we are on a geodesic, we never turn our ‚Äústeering wheel.‚Äù The spear stays parallel to our path.</li> <li>At the corner: we stop and turn our body by an exterior angle (\(\alpha_i\)). We do not turn the spear; it still points where it was pointing.</li> <li>Back at the start: When we complete the loop, we compare the spear‚Äôs current direction to its starting direction. They won‚Äôt match. This net rotation of the vector after the full trip, the ‚Äúerror‚Äù in direction, is called the <strong>Holonomy (\(\Delta \theta\))</strong>.</li> </ul> <p>The above way of sliding a vector along a geodesic while keeping it ‚Äúparallel‚Äù is called <strong>Parallel Transport</strong>.</p> <p>At the same time, just by some simple calculation we would know that for the total turn on a flat plane, our total change in heading (sum of exterior angles \(\sum \alpha_i\)) must be \(2\pi\) <em>to close a loop</em>. In other words, the holonomy \(\Delta \theta = 0\). However, on a curved surface, the amount we actually turned, the holonomy, is \(2\pi - \sum \alpha_i \neq 0\).</p> <p>Also notice that if we do parallel transport along the ‚Äúpeeled‚Äù flat strip, the vector remains parallel in the Euclidean sense because the strip is a straight line.</p> <p>However, when we close the loop,</p> <ul> <li>On the flat plane, the vector would return to its start pointing in the original direction.</li> <li>On the curved surface, the vector returns rotated by an angle \(\Delta \theta\).</li> </ul> <p>The theorem tells us that this holonomy \(\Delta \theta\) is precisely the integral of the Gaussian curvature over the area we bypassed:</p> \[\Delta \theta = 2\pi - \sum\alpha_i = \iint_R K \, dA\] <p>Intuition Check: If you are on a flat plane, \(K=0\). Therefore, \(\iint K dA = 0\). This means \(0 = 2\pi - \sum \alpha_i\), or \(\sum \alpha_i = 2\pi\). This is just some high-school geometry rule that the exterior angles of any polygon sum to 360¬∞. On a sphere, the curvature \(K\) ‚Äúhelps‚Äù us turn, so we don‚Äôt need as much ‚Äúexterior angle‚Äù to close the loop.</p> <p>Consequently, if we apply Needham‚Äôs trick, i.e., peel off the strip of geodesic loop, we would find that</p> <blockquote> <p>While we can peel a <strong>single</strong> geodesic segment and lay it <strong>flat</strong> perfectly, we <strong>cannot</strong> peel a <strong>closed loop</strong> of geodesics and lay the resulting ‚Äúframe‚Äù flat in the plane without a gap or overlap.</p> </blockquote> <p>The ‚Äúangle‚Äù of that gap is the <strong>holonomy</strong>. The Gauss-Bonnet theorem tells us that this gap is exactly equal to the total Gaussian curvature ‚Äútrapped‚Äù inside the area we just cut out.</p> <ul> <li><strong>On a Sphere (\(K&gt;0\)):</strong> The geodesics turn ‚Äútoward‚Äù each other, and the interior angles sum to \(&gt;\pi\).</li> <li><strong>On a Saddle (\(K&lt;0\)):</strong> The geodesics flare ‚Äúaway‚Äù from each other, and the interior angles sum to \(&lt;\pi\).</li> </ul> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Needham‚Äôs trick works because a 1D line has no ‚Äúarea‚Äù to trap curvature. The ‚Äústraightness‚Äù we see on the paper is the physical manifestation of zero geodesic curvature, an intrinsic property that survives the transition from the fruit‚Äôs skin to the flat desk.</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Manifold"/><category term="Differential Geometry"/><summary type="html"><![CDATA[Geodesic and its construction, Gauss Theorem Egregium, Gauss-Bonnet and Chern]]></summary></entry><entry><title type="html">High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring</title><link href="https://jasmineruixiang.github.io/blog/2026/zscore/" rel="alternate" type="text/html" title="High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring"/><published>2026-02-06T11:16:09+00:00</published><updated>2026-02-06T11:16:09+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/zscore</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/zscore/"><![CDATA[<p>This short blog provides a detailed, self-contained computational analysis of whether two z-scoring procedures applied to block-structured neural data are equivalent, and if indeed different how.</p> <hr/> <h2 id="1-problem-setup">1] Problem setup</h2> <p>Let‚Äôs say that we have neural data organized into blocks:</p> <ul> <li>Number of blocks: \(B\)</li> <li>Each block has data matrix in shape: \((x_{i, j}^b) \in \mathbb{R}^{t \times n}, \quad b = 1, \dots, B\)</li> <li>\(t\) = number of time points (samples)</li> <li>\(n\) = number of neural features</li> </ul> <p>Z-scoring is performed <strong>feature-wise</strong>, so all derivations below consider <strong>one fixed feature</strong> (column) at a time. The argument applies independently to every feature.</p> <hr/> <h2 id="2-notation-for-a-single-feature">2] Notation for a single feature</h2> <p>For a fixed feature \(j\):</p> <ul> <li> <p>Let \(x_{i,j}^{b} \in \mathbb{R}^{1}\) denote the data at time \(i\) for the feature \(j\) in block \(b\). For the following paragraphs, I will simplify \(x_{:,j}^{b}\) into \(x_{j}^{b} \in \mathbb{R}^{t\times 1}\), and \(x_{i,:}^{b}\) into \(x_{i}^{b} \in \mathbb{R}^{1\times n}\). Naturally, \(x^b = (x_{ij}^b) \in \mathbb{R}^{t\times n}\) with the same shape for all blocks. Basically, \(i\) corresponds to the index of time \(t\), and \(j\) the index of the number of neurons \(n\).</p> </li> <li> <p>Block-wise mean: \(\mu_{j}^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i, j} \in \mathbb{R}^{1}\),</p> <p>\(\mu ^{b} = [\mu_{1}^{b}, \cdots, \mu_{n}^{b}] \in \mathbb{R}^{1 \times n}\).</p> <p>Or, we could simply write \(\mu ^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i} \in \mathbb{R}^{1 \times n}\)</p> </li> <li> <p>Block-wise variance: \((\sigma_{j}^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i, j} - \mu_{j}^b)^2 \in \mathbb{R}^{1}\),</p> <p>\((\sigma ^{b})^2 = [(\sigma_{1}^{b})^2, \cdots, (\sigma_{n}^{b})^2] \in \mathbb{R}^{1 \times n}\),</p> <p>Or, we could simplify the above as \((\sigma ^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i} - \mu^b) \odot (x^{b}_{i} - \mu^b) \in \mathbb{R}^{1\times n}\)</p> <p>where \(\odot\) is the Hadamard product between two vectors (or just elementwise multiplication), defined as \(x \odot y = diag(x)y = (x_i y_i)_i \in \mathbb{R}^{1\times n},\) where \(x,y \in \mathbb{R}^{1\times n}\)</p> </li> </ul> <hr/> <h2 id="3-method-a-block-wise-z-scoring-concatenate-and-then-global-z-scoring">3] Method A: Block-wise z-scoring, concatenate, and then global z-scoring</h2> <h3 id="step-3a-z-score-within-each-block">Step 3a]: Z-score within each block</h3> <p>Each block is normalized independently:</p> \[z^{b}_{i} = \frac{x^{b}_i - \mu^b}{\sigma^b} \in \mathbb{R}^{1\times n}\] <p>Notice that this is element-wise division (to not over-complicate the symbols, I‚Äôll use this abuse of notation for the following).</p> <p>By construction, for every block \(b\):</p> \[\frac{1}{t}\sum_{i=1}^t z^{b}_{i} = \vec{0} \in \mathbb{R}^{1\times n}, \qquad \frac{1}{t}\sum_{i=1}^t (z^{b}_{i} - \vec{0}) \odot (z^{b}_{i} - \vec{0}) = \vec{1} \in \mathbb{R}^{1\times n},\] <p>Consequently, each feature in each block has mean 0 and standard deviation 1. \(z^b\) observes the same notation rule as I described above for \(x^b\).</p> <hr/> <h3 id="step-3b-concatenate-all-normalized-blocks">Step 3b]: Concatenate all normalized blocks</h3> <p>Concatenate all \(z^{b}\) into a single vector of length \(Bt\).</p> <h4 id="global-mean">Global mean</h4> \[\frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B \vec{0} = \vec{0} \in \mathbb{R}^{1\times n}\] <h4 id="global-variance">Global variance</h4> \[\frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) \\ = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) = \frac{1}{B}\sum_{b=1}^B\vec{1} = \vec{1} \in \mathbb{R}^{1\times n}\] <hr/> <h3 id="step-3c-second-z-scoring">Step 3c]: Second z-scoring?</h3> <p>Since the concatenated data already has zero mean and unit variance, adding any other layers of z-scoring has no effect.</p> <p><strong>Final output of Method A:</strong></p> \[\boxed{z^{b} \in \mathbb{R}^{t\times n}}\] <p>for each block. So method A simply returns the block-wise standardized data.</p> <hr/> <h2 id="4-method-b-concatenate-first-then-global-z-scoring">4] Method B: Concatenate first, then global z-scoring</h2> <h3 id="step-4a-concatenate-raw-data">Step 4a]: Concatenate raw data</h3> <p>Concatenate all blocks \(x^{b}\) into a single matrix \(X \in \mathbb{R}^{Bt \times n}\).</p> <hr/> <h3 id="step-4b-compute-global-mean-and-standard-deviation">Step 4b]: Compute global mean and standard deviation</h3> \[\mu = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t x^{b}_i = \frac{1}{B}\sum_{b=1}^B \frac{1}{t}\sum_{i=1}^{t}x_i^b \\ = \frac{1}{B}\sum_{b = 1}^{B} \mu^b \in \mathbb{R}^{1\times n}\] <p>The global mean is the average of block-wise means (it‚Äôs not hard to show that if each block has different samples, this average will become <em>weighted average</em> by the ratio of the amount of each block‚Äôs data to total data amount).</p> <hr/> <h3 id="step-4c-compute-global-variance">Step 4c]: Compute global variance</h3> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (x^{b}_i - \mu) \odot (x^{b}_i - \mu) \in \mathbb{R}^{1 \times n}\] <p>Expand the product term:</p> \[(x^{b}_i - \mu) \odot (x^{b}_i - \mu) \\ = (x^{b}_i - \mu^b + \mu^b - \mu) \odot (x^{b}_i - \mu^b + \mu^b - \mu) \\ = (x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + 2(x^{b}_i - \mu^b) \odot (\mu^b - \mu) \\ + (\mu^b - \mu) \odot (\mu^b - \mu) \\\] <p>Let‚Äôs look at each of the terms more closely. Notice that when summing over \(i\) in \(\sigma^2\), the cross term vanishes:</p> \[\frac{1}{t}\sum_{i=1}^t ((x^{b}_i - \mu^b) \odot (\mu^b - \mu))\\ = (\frac{1}{t}\sum_{i=1}^t (x^{b}_i - \mu^b)) \odot (\mu^b - \mu)\\ = (\mu^b - \mu^b) \odot (\mu^b - \mu) \\ = \vec{0}\] <p>Thus,</p> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t((x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + (\mu^b - \mu) \odot (\mu^b - \mu)) \\ = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (\mu^b - \mu) \odot (\mu^b - \mu) \\ = \frac{1}{B}\sum_{1}^{B}\frac{1}{t}\sum_{i=1}^{t}(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\\ = \frac{1}{B}\sum_{1}^{B}(\sigma^b)^2 + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\] <p>This is a <strong>variance decomposition</strong> into:</p> <ul> <li>average within-block variance</li> <li>variance of block means (between-block variance)</li> </ul> <p>which meets our intuitive expectation (what else could it be anyway‚Ä¶).</p> <hr/> <h3 id="step-4d-global-z-scoring">Step 4d]: Global z-scoring</h3> <p>Each sample is normalized as:</p> \[y^{b}_i = \frac{x^{b}_i - \mu}{\sigma} \in \mathbb{R}^{1\times n}\] <p>By the nature of z-scoring, if we now calculate the global mean and standard deviation on concatenated \(y^b\), as in 3b] for each feature, we will obtain 0 and 1 respectively.</p> <p>Now if we rewrite the above using block-wise z-scores, since</p> <p>\(z_i^b = \frac{x_i^b - \mu^b}{\sigma^b} \rightarrow x_i^b = \sigma^bz_i^b + \mu^b\), then:</p> \[\boxed{ y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} }\] <p>which reinforces the idea that these all are just linear transformations: Transformations being linear, linear into one another.</p> <hr/> <h2 id="comparison-of-method-a-and-method-b">Comparison of Method A and Method B</h2> <p>Method A output:</p> \[z^{b}_i \in \mathbb{R}^{1\times n},\; \forall i, b\] <p>Method B output:</p> \[y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} \in \mathbb{R}^{1\times n}, \; \forall i, b\] <p>For the two methods to be identical \(\forall i,b\), we must have:</p> \[\sigma_b = \sigma \quad \text{and} \quad \mu_b = \mu \quad \forall b\] <p>And again, if we concatenate all \(y^b\) and \(z^b\) together separately into \(Y, Z\), they <strong>BOTH</strong> have feature-wise mean 0 and std 1.</p> <hr/> <h2 id="final-result">Final result</h2> \[\boxed{ \begin{array}{l} \text{The two procedures are NOT equivalent in general, even though} \\ \text{both yield global mean } 0 \text{ and std } 1 \text{ after transformations}. \end{array} }\] <p>They are equivalent <strong>if and only if</strong> every block already has identical feature-wise means and variances.</p> <ul> <li><strong>Method A</strong> removes all block-level mean and variance differences before concatenation.</li> <li><strong>Method B</strong> preserves block-level differences and normalizes relative to the pooled distribution.</li> </ul> <p>Block-wise z-scoring and global z-scoring <strong>do not commute</strong>. These choices encode different assumptions about whether block identity (e.g., session, subject, condition) should be preserved or discarded. Our choice should be driven by whether block-to-block variability is meaningful signal or nuisance variability in our analysis.</p> <p>Fun quesitons:</p> <ul> <li>1] What if block size \(t\) is not the same across all blocks?</li> <li>2] What are other (useful/effective) ways of normalization which also return the same mean/std (0/1, e.g.)?</li> </ul> <p>Practical question: In practice, how much do the statistics from these two methods actually differ? How should we interpret such differences?</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Brain Computer Interface"/><summary type="html"><![CDATA[Zscoring, block vs session level comparisons]]></summary></entry><entry><title type="html">The Dance of Space: Geom/Topo/Dynam Mumble(4) (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/subspace/" rel="alternate" type="text/html" title="The Dance of Space: Geom/Topo/Dynam Mumble(4) (in progress)"/><published>2026-02-03T21:49:38+00:00</published><updated>2026-02-03T21:49:38+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/subspace</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/subspace/"><![CDATA[<h2 id="setup">Setup</h2> <p>Suppose we compute PCA on two datasets (e.g., train vs.\ test), and we keep the top-$r$ principal components. Let</p> \[U_{\text{train}} \in \mathbb{R}^{d \times r}, \qquad U_{\text{test}} \in \mathbb{R}^{d \times r},\] <p>where each matrix has <strong>orthonormal columns</strong> (so each is a basis for an $r$-dimensional subspace of $\mathbb{R}^d$).</p> <p>Our first obvious intuition might be to define the <strong>subspace overlap matrix</strong> as the following:</p> \[S = U_{\text{train}}^{\top} U_{\text{test}} \in \mathbb{R}^{r \times r}.\] <p>So, each entry is a dot product between basis vectors:</p> \[S_{ij} = u_i^{(\text{train})} \cdot u_j^{(\text{test})}.\] <p>At first glance, this looks like a direct ‚Äúbasis alignment‚Äù comparison, which is exactly what we aim for. How else could we characterize subspace change other than looking at pairs-wise relationships among two sets of basis vectors? Well, there‚Äôre a few caveats‚Ä¶</p> <hr/> <h2 id="1-why-basis-by-basis-alignment-is-unstable">1] Why basis-by-basis alignment is unstable</h2> <p>Comparing PCA vectors one-by-one (e.g., ‚ÄúPC1 vs.\ PC1‚Äù) is unstable because PCA eigenvectors are <strong>not uniquely defined</strong> in two common cases. The following problems might pop up ‚Äî</p> <h3 id="i-sign-flips">(i) Sign flips</h3> <p>If \(u\) is an eigenvector, then \(-u\) is also an eigenvector.</p> <p>So dot products can flip sign even when the <em>subspace is identical</em>.</p> <h3 id="ii-degenerate--near-degenerate-eigenvalues-rotation-inside-the-subspace">(ii) Degenerate / near-degenerate eigenvalues (rotation inside the subspace)</h3> <p>If</p> \[\lambda_i \approx \lambda_{i+1},\] <p>then the corresponding principal directions inside the 2D span can rotate dramatically under tiny perturbations (noise, finite-sample effects, etc.).</p> <p>This means that even if the <em>span</em> is essentially the same, the individual vectors $u_i$ can change a lot.<br/> So comparing ‚ÄúPC1 to PC1‚Äù is not meaningful.</p> <hr/> <h2 id="2-what-singular-values-do-that-dot-products-dont">2] What singular values do that dot products don‚Äôt</h2> <p>The matrix</p> \[S = U_{\text{train}}^{\top} U_{\text{test}}\] <p>depends on the <em>chosen bases</em> inside each subspace. If we change bases within either subspace via orthogonal transformations:</p> \[U_{\text{train}} \to U_{\text{train}} R_1, \qquad U_{\text{test}} \to U_{\text{test}} R_2,\] <p>where $R_1, R_2 \in \mathbb{R}^{r \times r}$ are orthogonal (including sign flips as a special case), then</p> \[S \to (U_{\text{train}}R_1)^{\top}(U_{\text{test}}R_2) = R_1^{\top} S R_2.\] <p>So the <em>entries</em> of $S$ can change wildly.</p> <h3 id="key-fact-invariance">Key fact (invariance)</h3> <blockquote> <p>The <strong>singular values</strong> of \(S\) are invariant under left/right orthogonal rotations:</p> </blockquote> <ul> <li>Left-multiplying by an orthogonal matrix does not change singular values.</li> <li>Right-multiplying by an orthogonal matrix does not change singular values.</li> </ul> <p>Therefore, even if PCA ‚Äúrelabels,‚Äù flips signs, or rotates the basis vectors within the subspace, the <strong>singular values remain unchanged</strong>.</p> <p>This means singular values capture a property of the <strong>subspaces</strong>, not of the particular eigenvectors chosen.</p> <hr/> <h2 id="3-geometric-meaning-principal-angles">3] Geometric meaning: principal angles</h2> <p>Take the SVD:</p> \[S = Q \Sigma R^{\top},\] <p>where</p> \[\Sigma = \mathrm{diag}(\sigma_1,\dots,\sigma_r).\] <p>A fundamental result is:</p> \[\sigma_i = \cos(\theta_i),\] <p>where $\theta_i$ are the <strong>principal angles</strong> between the two $r$-dimensional subspaces.</p> <p>Interpretation:</p> <ul> <li>$\theta_i = 0 \implies$ perfectly aligned direction exists (since $\cos(\theta_i)=1$)</li> <li>$\theta_i = 90^\circ \implies$ orthogonal direction (since $\cos(\theta_i)=0$)</li> </ul> <p>So the singular values summarize <em>how much overlap</em> the two subspaces have along their best-aligned directions.</p> <hr/> <h2 id="4-why-this-is-the-stable-comparison">4] Why this is the ‚Äústable‚Äù comparison</h2> <p>Think of $U_{\text{train}}$ and $U_{\text{test}}$ as <strong>arbitrary coordinate systems</strong> inside their respective subspaces.</p> <p>A meaningful comparison should ignore that arbitrariness.</p> <p>Principal angles / singular values do exactly this: they compute the <strong>best possible matching</strong> between directions in the two subspaces.</p> <p>Instead of comparing ‚ÄúPC1 $\leftrightarrow$ PC1,‚Äù we solve an optimal alignment problem:</p> \[\max_{\|a\|=\|b\|=1} a^{\top}\bigl(U_{\text{train}}^{\top}U_{\text{test}}\bigr)b,\] <p>and the sequence of best matches yields</p> \[\sigma_1,\sigma_2,\dots\] <p>as the strengths of alignment along the best-aligned directions.</p> <hr/> <h2 id="5-tiny-example-intuition-2d-case">5] Tiny example intuition (2D case)</h2> <p>Suppose both subspaces are actually the same 2D plane in $\mathbb{R}^d$.</p> <p>You could pick:</p> <ul> <li>$U_{\text{train}}$ = standard basis in that plane</li> <li>$U_{\text{test}}$ = same plane but rotated by $45^\circ$ inside it</li> </ul> <p>Then $S$ might look like a rotation matrix:</p> \[S= \begin{pmatrix} \cos 45^\circ &amp; -\sin 45^\circ \\ \sin 45^\circ &amp; \cos 45^\circ \end{pmatrix}.\] <p>The entries are not the identity, so basis-by-basis dot products look ‚Äúnot aligned.‚Äù</p> <p>But the singular values of a rotation matrix are both $1$.</p> <p>So singular values correctly say: <strong>the subspaces are identical</strong>.</p> <p>That‚Äôs the whole point.</p> <hr/> <h2 id="bottom-line">Bottom line</h2> <p>We use singular values of</p> \[U_{\text{train}}^{\top}U_{\text{test}}\] <p>because:</p> <ul> <li>‚úÖ they are invariant to sign flips / rotations / re-ordering of PCA vectors inside the subspace</li> <li>‚úÖ they define principal angles, which are a true subspace-to-subspace comparison</li> <li>‚úÖ they give a stable measure of drift even when eigenvectors are not uniquely defined</li> </ul> <hr/> <h2 id="a-sidenote-connection-to-projection-distance">A sidenote: Connection to projection distance</h2> <p>Let $P_{\text{train}}$ and $P_{\text{test}}$ be the orthogonal projection matrices onto the two subspaces:</p> \[P_{\text{train}} = U_{\text{train}}U_{\text{train}}^{\top}, \qquad P_{\text{test}} = U_{\text{test}}U_{\text{test}}^{\top}.\] <p>Then one can show the Frobenius-distance relationship:</p> \[\|P_{\text{train}} - P_{\text{test}}\|_F^2 = 2r - 2\|U_{\text{train}}^{\top}U_{\text{test}}\|_F^2 = 2\sum_{i=1}^r \sin^2(\theta_i).\]]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Subspace Geometry and Computation]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)"/><published>2026-01-14T20:22:25+00:00</published><updated>2026-01-14T20:22:25+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(3)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/"><![CDATA[<p>This is episode 3 on the smooth manifold series. Today we will be diving into the properties of maps between manifolds. I will first summarize how to understand and compute the differential of a smooth map between manifolds, both abstractly and concretely, culminating in the matrix-valued example<br/> \(F(A) = A^\top A.\)</p> <hr/> <h2 id="1-differential-canonical-definition">1] Differential: canonical definition</h2> <h3 id="11-differential-of-a-smooth-map-intrinsic-definition">1.1 Differential of a Smooth Map (Intrinsic Definition)</h3> <p>Let \(F : M \to N\) be a smooth map between smooth manifolds.</p> <p>For any point $ p \in M $, the <strong>differential</strong> is a linear map which is a <strong>pushforward of derivations</strong>. \(dF_p : T_p M \longrightarrow T_{F(p)} N.\)</p> <h3 id="derivation-based-definition">Derivation-based definition</h3> <p>If $ v \in T_p M $ is a tangent vector viewed as a derivation, then \((dF_p v)(g) := v(g \circ F), \qquad g \in C^\infty(N).\)</p> <p>This definition is <strong>coordinate-free</strong>.</p> <p>It might appear at first both unnecessarily abstract and underestimated as to its computation. We might claim that it is essentially just a Jacobian matrix in local coordinates. However, the essence of this concept resides on its definition to be conceptually a coordinate-independent linear map between tangent spaces.</p> <hr/> <h3 id="12-coordinate-representation-and-the-jacobian">1.2 Coordinate Representation and the Jacobian</h3> <p>To compute $ dF_p $ in practice:</p> <ol> <li>Choose a chart $ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>Choose a chart $ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ol> <p>Define the coordinate expression: \(\tilde F = \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Then: \(\boxed{ dF_p \;\text{is represented by}\; D\tilde F(\varphi(p)) }\)</p> <p>That is, <strong>the Jacobian matrix is the coordinate representation of the differential</strong>.</p> <blockquote> <p>The Jacobian depends on coordinates; the linear map $ dF_p $ does not.</p> </blockquote> <hr/> <h3 id="13-why-this-is-not-just-the-jacobian">1.3 Why this is not ‚Äújust‚Äù the Jacobian</h3> <p>The Jacobian depends on coordinates;<br/> $ dF_p $ does not.</p> <p>More precisely:</p> <ul> <li>$ dF_p $ is a <strong>geometric linear map</strong></li> <li>The Jacobian is a <strong>matrix representation</strong> of that map in chosen bases: \(\frac{\partial \bigl(\psi^1 \circ F,\;\dots,\;\psi^n \circ F\bigr)} {\partial \bigl(x^1,\;\dots,\;x^m\bigr)}\)</li> </ul> <p>If you change charts, the matrix changes by:</p> \[\boxed{ J_{\text{new}} = D\psi\,\cdot\, J_{\text{old}} \,\cdot\, (D\varphi^{-1}) }\] <p>but the underlying linear map $ dF_p $ stays the same.</p> <hr/> <h2 id="2-differential-alternative-interpretation">2] Differential: alternative interpretation</h2> <h3 id="21-curve-based-definition">2.1 Curve-based definition</h3> <p>This is also coordinate-free:</p> <p>If<br/> \(\gamma : (-\varepsilon,\varepsilon) \to M\) is a smooth curve with \(\gamma(0) = p \quad \text{and} \quad \gamma'(0) = v \in T_p M,\) then \(\boxed{ dF_p(v) = (F \circ \gamma)'(0) \in T_{F(p)} N. }\)</p> <p>No coordinates anywhere. This viewpoint is often the most intuitive and is fully equivalent to the derivation definition.</p> <hr/> <h3 id="22-curves-in-local-coordinates">2.2 Curves in local coordinates</h3> <p>Choose charts:</p> <ul> <li>$ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>$ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ul> <p>Define the coordinate representation: \(\tilde F := \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Now define the coordinate curve: \(\tilde\gamma := \varphi \circ \gamma : (-\varepsilon,\varepsilon) \to \mathbb{R}^m.\)</p> <p>Consequently, in coordinates the statement is the following:</p> \[\boxed{ D\tilde F(\varphi(p)) \cdot \tilde\gamma'(0) = D(\psi \circ F \circ \varphi^{-1})(\varphi(p)) \cdot (\varphi \circ \gamma)'(0). }\] <p>In the coordinate formula, \(\gamma'(0) \quad \text{really means} \quad \tilde\gamma'(0) = (\varphi \circ \gamma)'(0).\)</p> <p>Here‚Äôs a diagram that makes everything explicit</p> \[\begin{array}{ccc} T_p M &amp; \xrightarrow{dF_p} &amp; T_{F(p)} N \\ \downarrow d\varphi_p &amp; &amp; \uparrow d\psi^{-1}_{\psi(F(p))} \\ \mathbb{R}^m &amp; \xrightarrow{D\tilde F(\varphi(p))} &amp; \mathbb{R}^n \end{array}\] <p>Thus:</p> <ul> <li>$ \gamma‚Äô(0) $ lives in $ T_p M $</li> <li>$ (\varphi \circ \gamma)‚Äô(0) = d\varphi_p(\gamma‚Äô(0)) \in \mathbb{R}^m $</li> <li>$ D\tilde F(\varphi(p)) $ acts on that coordinate vector</li> </ul> <hr/> <h3 id="23-sidenote-graph-differential">2.3 Sidenote: Graph Differential</h3> <p>The <strong>graph</strong> of $ F $ is \(\Gamma_F = \{ (p, F(p)) \mid p \in M \} \subset M \times N.\)</p> <p>Define the graph map: \(\Phi : M \to M \times N, \quad \Phi(p) = (p, F(p)).\)</p> <p>Its differential is: \(\boxed{ d\Phi_p(v) = (v, dF_p(v)). }\)</p> <p>This is what is often called the <strong>graph differential</strong>.</p> <hr/> <h2 id="3-special-case-maps-between-vector-spaces">3] Special Case: Maps Between Vector Spaces</h2> <h3 id="31-a-great-simplification">3.1 A great simplification</h3> <p>If $ M = \mathbb{R}^m $, $ N = \mathbb{R}^n $, then: \(T_p M \cong \mathbb{R}^m, \quad T_{F(p)} N \cong \mathbb{R}^n.\)</p> <p>In this case:</p> <ul> <li>$ dF_p $ is a linear map $ \mathbb{R}^m \to \mathbb{R}^n $</li> <li>Its matrix is exactly the <strong>Jacobian matrix</strong></li> <li>$ dF_p(H) $ coincides with the <strong>Fr√©chet / directional derivative</strong></li> </ul> <hr/> <h3 id="32-worked-example--fa--atop-a-">3.2 Worked Example: $ F(A) = A^\top A $</h3> <p>Let \(F : \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}, \quad F(A) = A^\top A.\)</p> <p>Since $ \mathbb{R}^{n \times n} $ is a vector space, \(T_A(\mathbb{R}^{n \times n}) \cong \mathbb{R}^{n \times n}.\)</p> <h4 id="321-direct-computation">3.2.1 Direct computation</h4> <p>There are several ways to compute the differential. The most straight-forward method is to do the following (as you might imagine):</p> <p>For \(H \in T_A(\mathbb{R}^{n \times n})\), \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} (A+tH)^\top(A+tH).\)</p> <p>Expanding: \((A+tH)^\top(A+tH) = A^\top A + t(H^\top A + A^\top H) + t^2 H^\top H.\)</p> <p>Thus: \(\boxed{ dF_A(H) = H^\top A + A^\top H. }\)</p> <h4 id="322-curve-based-computation">3.2.2 Curve-based computation</h4> <p>Let \(A(t)\) be a smooth curve with: \(A(0)=A, \quad A'(0)=H.\)</p> <p>Then: \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} A(t)^\top A(t) = H^\top A + A^\top H.\)</p> <p>This makes it clear that the definition of \(dF_A(H)\) is <strong>coordinate-free</strong>.</p> <p>Sidenote: if we restrict \(A\) to symmetric or SPD matrices, what will we see? Or what if we connect this to Riemannian geometry on \(GL(n)\) or \(SPD(n)\)? We‚Äôll come back to this later when we discuss Lie group and Lie algebra.</p> <hr/> <h2 id="4-back-to-classical-regular-surfaces-parametrizations-immersions">4] Back to classical regular surfaces (parametrizations, immersions)</h2> <h3 id="the-question-in-do-carmo-diffgeom">The Question in do Carmo DiffGeom</h3> <p>In do Carmo‚Äôs definition of a <strong>regular surface</strong> in \(\mathbb{R}^3\), a coordinate map \(X : U \subset \mathbb{R}^2 \to \mathbb{R}^3\) is required to satisfy two conditions:</p> <ol> <li>\(X\) is a <strong>differentiable homeomorphism</strong> onto its image.</li> <li>The differential \(dX_p\) is <strong>injective</strong> at every point $p \in U$.</li> </ol> <p>Since \(X\) maps from \(\mathbb{R}^2\) to \(\mathbb{R}^3\), its differential can never be surjective, so injectivity (rank 2) is the meaningful requirement.</p> <p>A natural question arises:</p> <blockquote> <p>If \(X\) is already a differentiable homeomorphism, isn‚Äôt its differential automatically injective?</p> </blockquote> <p>The answer is <strong>no</strong>.</p> <p>Well, to make it explicit, let‚Äôs figure out first what a differentiable homeomorphism actually gives us: If \(X : U \to \mathbb{R}^3\) is a differentiable homeomorphism onto its image, then:</p> <ul> <li>\(X\) is <strong>continuous and injective</strong></li> <li>\(X^{-1}\) is <strong>continuous</strong> (but <em>not</em> necessarily differentiable)</li> <li>Topologically, \(X(U)\) looks like a 2‚Äëdimensional surface</li> </ul> <p>This is a <strong>topological</strong> statement plus differentiability of $X$. It controls <em>points</em>, but says nothing about what happens to <em>directions</em>. Crucially, differentiability of the inverse is <em>not</em> assumed.</p> <p>Fine, but then why injectivity of the differential is not automatically assured? Notice that the differential</p> \[dX_p : \mathbb{R}^2 \to \mathbb{R}^3\] <p>being injective means it has <strong>rank 2</strong> meaning no tangent direction is collapsed. A map can be:</p> <ul> <li>injective,</li> <li>continuous with continuous inverse,</li> <li>differentiable,</li> </ul> <p>and <em>still</em> have rank drop somewhere. Let me give a concrete example:</p> <p>Consider \(X(u,v) = (u^3, v, 0).\)</p> <p>Properties of this map:</p> <ul> <li>It is <strong>injective</strong></li> <li>It is a <strong>homeomorphism onto its image</strong></li> <li>It is differentiable everywhere</li> </ul> <p>However, its differential is \(dX_{(u,v)} = \begin{pmatrix} 3u^2 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix}\)</p> <p>At \($u = 0\), this matrix has <strong>rank 1</strong>, not 2. One tangent direction is squashed. Therefore, this map is <strong>not an immersion</strong>, and it does <strong>not</strong> define a regular surface in do Carmo‚Äôs sense.</p> <hr/> <p>However, you might observe here a seemingly apparent paradox: ‚ÄúBut the image is <strong>Just</strong> a plane!‚Äù</p> <p>Well, the image of \(X(u,v) = (u^3, v, 0)\) is \({(x,y,0) : x,y \in \mathbb{R}},\) which is the entire (xy)-plane indeed. As a <strong>subset</strong> of \(\mathbb{R}^3\), this plane is perfectly flat, so one might expect its tangent plane at every point to be the whole plane. So then why does the tangent collapse under this map?</p> <hr/> <p>The key point here is that here are <strong>two distinct notions</strong> at play:</p> <ol> <li>The tangent plane of a <strong>subset</strong> of \(\mathbb{R}^3\)</li> <li>The tangent plane <strong>defined by a parametrization</strong></li> </ol> <p>In do Carmo‚Äôs approach, tangent planes are defined <em>via parametrizations</em>. The tangent plane at a point is \(T_pS = \operatorname{span}{X_u(p), X_v(p)}.\)</p> <p>For the map above: \(X_u = (3u^2, 0, 0), \quad X_v = (0,1,0).\)</p> <p>At (u=0): \(X_u(0,v) = (0,0,0), \quad X_v(0,v) = (0,1,0),\) so the span is <strong>1‚Äëdimensional</strong>.</p> <p>This means:</p> <blockquote> <p>The parametrization fails to distinguish two independent directions in the parameter domain.</p> </blockquote> <p>Geometrically, the $u$-direction has been crushed.</p> <hr/> <p>But does the plane still have a 2D tangent plane?</p> <p>Yes ‚Äî but <strong>not via this parametrization</strong>.</p> <p>If instead we parametrize the same plane by \(Y(s,t) = (s,t,0),\) then \(dY = \begin{pmatrix} 1 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix},\) which has rank 2 everywhere.</p> <p>So the <em>same subset</em> becomes a <strong>regular surface</strong> under a different map.</p> <hr/> <p>Finally, what does this meana conceptually? The key lesson is:</p> <blockquote> <p><strong>Regularity is not a property of the subset alone ‚Äî it is a property of the subset together with its smooth structure.</strong></p> </blockquote> <p>Different parametrizations can induce:</p> <ul> <li>a <strong>good</strong> smooth structure (immersion)</li> <li>or a <strong>bad</strong> one (rank collapse)</li> </ul> <p>This is why do Carmo requires that <strong>there exists</strong> a local parametrization with injective differential.</p> <hr/> <p>In other words, if you ask ‚ÄúDoes that mean there might exist other maps which make the plane a regular surface?‚Äù, then the answer is Yes ‚Äî absolutely. The plane <em>is</em> a regular surface because such maps exist.</p> <p>Behind that is another question: ‚ÄúDoes immersion entirely depend on which maps we pick?‚Äù Yes!</p> <ul> <li><strong>Immersion is a property of the map</strong>, not of the set.</li> <li>A <strong>regular surface</strong> is a set for which <em>good immersions exist everywhere</em>.</li> </ul> <p>In this seense, do Carmo does separate the conditions, being deliberately modular:</p> <ol> <li><strong>Homeomorphism</strong> ‚Üí good topology (no self‚Äëintersections)</li> <li><strong>Injective differential</strong> ‚Üí good differential geometry</li> </ol> <p>whether neither condition implies the other.</p> <p>Finally, some of key points:</p> <ul> <li>The same subset of \(\mathbb{R}^3\) can support <strong>many different smooth structures</strong></li> <li>Differential geometry only works once a smooth structure is fixed</li> <li>Parametrizations are how do Carmo <em>builds</em> that structure</li> </ul> <p>This is why modern texts often say:</p> <blockquote> <p>‚ÄúA surface is a 2‚Äëdimensional smooth manifold embedded in $$\mathbb{R}^3$.‚Äù</p> </blockquote> <p>Do Carmo reaches this notion <strong>from parametrizations upward</strong>, rather than assuming it at the start.</p> <hr/> <blockquote> <p><strong>Topology sees points.</strong> <strong>Differential geometry sees directions.</strong></p> </blockquote> <p>A homeomorphism controls points. Injectivity of the differential controls directions.</p> <p>You need <strong>both</strong> to get a regular surface.</p> <h2 id="5-association-with-inverse-function-theorem">5] Association with inverse function theorem</h2> <h2 id="6-immersion-submersion">6] Immersion, submersion</h2> <p>Immersion basics</p> <p>Here I want to emphasize one key fact that:</p> <blockquote> <p>immersion imply a nonzero determinant in coordinates</p> </blockquote> <p>Why? Again, let‚Äôs recall that immersion means the differential is injective Let<br/> \(\phi:M^m\to N^n,\qquad m\le n\) and let $p\in M$.<br/> Saying <strong>(\phi) is an immersion at (p)</strong> means the differential \(d\phi_p:T_pM\to T_{\phi(p)}N\) is <strong>injective</strong>.</p> <p>Equivalently (in linear algebra language):<br/> \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Now, if write it in local coordinates ‚Üí Jacobian matrix has rank \(m\) Pick coordinate charts:</p> <ul> <li>on \(M\): \(x=(x^1,\dots,x^m)\) around \(p\)</li> <li>on \(N\): \(y=(y^1,\dots,y^n)\) around \(\phi(p)\)</li> </ul> <p>Then locally \(\phi\) looks like a smooth map between Euclidean spaces: \(y^a = \phi^a(x^1,\dots,x^m),\qquad a=1,\dots,n.\)</p> <p>Its differential in these coordinates is represented by the <strong>Jacobian matrix</strong> \(J(p)=\left(\frac{\partial \phi^a}{\partial x^i}(p)\right)\) which is an (n\times m) matrix.</p> <p>The immersion condition says: \(\operatorname{rank}(J(p))=m.\)</p> <p>So the columns of \(J(p)\) are linearly independent.</p> <p>Now we use a standard linear algebra fact:</p> <blockquote> <p>An \(n\times m\) matrix has rank \(m\) <strong>iff</strong> there exists an \(m\times m\) submatrix (choose \(m\) rows) whose determinant is nonzero.</p> </blockquote> <p>Why?</p> <ul> <li>If <strong>every</strong> \(m\times m\) minor determinant were zero, then <strong>every</strong> set of $m$ rows would be linearly dependent, so the rank would be \(&lt;m\).</li> <li>Since the rank is \(m\), at least one choice of $m$ rows gives an invertible \(m\times m\) matrix.</li> </ul> <p>Concretely: there exist indices \(1\le a_1&lt;\cdots&lt;a_m\le n\) such that the matrix \(\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)_{\alpha,i}\) has \(\det\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)\neq 0.\)</p> <p>If we now <strong>rename/reorder the target coordinates</strong> so that those special indices become \(1,\dots,m\), then we can assume:</p> \[\det\left(\frac{\partial (\phi^1,\dots,\phi^m)}{\partial (x^1,\dots,x^m)}(p)\right)\neq 0.\] <p>That‚Äôs exactly the statement: in coordinates (after renumbering if needed), an immersion gives a nonzero determinant of an \(m\times m\) Jacobian block.</p> <p>In one line summary: an immersion means \(\phi\) ‚Äúdoesn‚Äôt collapse any tangent directions,‚Äù so locally you can find $m$ coordinate functions of \(\phi\) that vary independently ‚Äî and ‚Äúvary independently‚Äù is exactly ‚ÄúJacobian block has nonzero determinant.‚Äù</p> <hr/> <h4 id="side-note-connection-to-the-constant-rank-theorem--local-normal-form-of-an-immersion">Side note: Connection to the Constant Rank Theorem / local normal form of an immersion?</h4> <p>Remember that Constant Rank Theorem (specialized to immersions) says: Let \(\phi:M^m\to N^n\) be smooth, and suppose \(\phi\) is an <strong>immersion at \(p\)</strong>.<br/> That means \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Then the constant rank theorem says:</p> <blockquote> <p>There exist coordinate charts<br/> \((U,x)\ \text{around }p,\qquad (V,y)\ \text{around }\phi(p)\) such that in these coordinates the map becomes \(y\circ \phi\circ x^{-1}(u^1,\dots,u^m) \;=\; (u^1,\dots,u^m,0,\dots,0).\)</p> </blockquote> <p>So locally, \(\phi\) looks like the <strong>standard inclusion</strong> \(\mathbb{R}^m \hookrightarrow \mathbb{R}^n,\qquad u\mapsto (u,0).\)</p> <p>That is the precise geometric meaning of ‚Äúimmersion.‚Äù</p> <p>In these special coordinates, \(\phi^1(u)=u^1,\;\dots,\;\phi^m(u)=u^m,\qquad \phi^{m+1}(u)=0,\dots,\phi^n(u)=0.\)</p> <p>So the Jacobian matrix is literally \(J= \begin{pmatrix} I_m\\ 0 \end{pmatrix}\) (an \(n\times m\) matrix).</p> <p>Now look at the top \(m\times m\) block: \(\frac{\partial(\phi^1,\dots,\phi^m)}{\partial(u^1,\dots,u^m)} = I_m,\) so \(\det(I_m)=1\neq 0.\)</p> <p>That‚Äôs exactly the coordinate statement.</p> <p>How this matches the ‚Äúminor is nonzero‚Äù argument? Before using the constant rank theorem, we only know:</p> <ul> <li>\(J(p)\) has rank \(m\)</li> <li>therefore some \(m\times m\) minor determinant is nonzero</li> </ul> <p>Then the constant rank theorem tells us that we can actually <strong>choose coordinates</strong> so that the ‚Äúgood minor‚Äù becomes the <em>first</em> \(m\) coordinates, and the map becomes \($(u,0)\).</p> <p>So:</p> <ul> <li><strong>Linear algebra fact:</strong> full rank \(\Rightarrow\) some minor \(\neq 0\)</li> <li><strong>Constant rank theorem:</strong> we can change coordinates to make that minor the obvious identity matrix.</li> </ul> <p>The geometric picture (why \((u,0)\) is the right normal form) is that an immersion means \(\phi\) ‚Äúinjects tangent vectors,‚Äù so locally \(\phi(U)\subset N\) is an $m-dimensional ‚Äúsheet‚Äù sitting inside an n-dimensional space. In good coordinates on N, that sheet looks like:</p> \[\{(y^1,\dots,y^n): y^{m+1}=\cdots=y^n=0\},\] <p>i.e. an embedded copy of \(\mathbb{R}^m\\).</p> <p>So locally, \(\phi\) is just a parametrization of that sheet.</p> <h2 id="7-embedding">7] Embedding</h2> <h3 id="72-a-key-difference-between-embedding-and-immersion">7.2 A key difference between embedding and immersion</h3> <p><strong>‚Äúno self-intersections‚Äù is one of the key geometric consequences of being <em>embedded</em></strong> (as opposed to merely <em>immersed</em>). As stated above, if a manifold \(M\) is <strong>embedded</strong> in \(\mathbb{R}^k\), it sits inside \(\mathbb{R}^k\) as a ‚Äúnice subset,‚Äù like a surface you could physically draw without crossing itself. More formally, an <strong>embedding</strong> \(F: M \to \mathbb{R}^k\) means:</p> <ol> <li>\(F\) is a <strong>smooth immersion</strong> (its differential is injective everywhere), and</li> <li>\(F\) is a <strong>homeomorphism onto its image</strong> \(F(M)\) (with the subspace topology).</li> </ol> <p>That second condition is exactly what rules out the classic ‚Äúself-crossing‚Äù pathology. If the image ‚Äúintersects itself‚Äù in the sense that two different points \(p\neq q\in M\) map to the same point in \(\mathbb{R}^k\), i.e. \(F(p)=F(q),\) then \(F\) is <strong>not injective</strong>, so it can‚Äôt be an embedding.</p> <p>So: <strong>an embedded submanifold cannot cross itself as a set in \(\mathbb{R}^k\)</strong>.</p> <p>On the other hand, an <strong>immersion</strong> can look like a manifold with self-crossings in \(\mathbb{R}^k\). An example will be the ‚Äúfigure-eight curve‚Äù in \(\mathbb{R}^2\) can be parametrized smoothly with nonzero derivative everywhere, so it‚Äôs an immersion, but it‚Äôs <strong>not embedded</strong> because it fails injectivity / fails to be a homeomorphism onto its image.</p> <p>In shoft,</p> <ul> <li><strong>Embedded \(\Rightarrow\)</strong> injective + ‚Äútopologically correct‚Äù inclusion<br/> \(\Rightarrow\) <strong>no self-intersections</strong>.</li> <li><strong>Immersed \(\Rightarrow\)</strong> locally nice but can globally overlap<br/> \(\Rightarrow\) <strong>self-intersections possible</strong>.</li> </ul> <h2 id="8-submanifolds">8] Submanifolds</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Explore relations of maps between manifolds]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)"/><published>2025-09-17T23:30:02+00:00</published><updated>2025-09-17T23:30:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(2)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/"><![CDATA[<p>This is episode 2 on the smooth manifold series. Today we will be diving into concepts that appear initially very intuitive at first glance, but then the extended version of which is indeed quite abstract.</p> <h2 id="0-familiar-examples">0] Familiar examples</h2> <p>Let me start with two simple concrete examples to illustrate what tangent vectors and tangent space are. Indeed they match up to our intuition!</p> <p>Let‚Äôs first say that we have a unit circle in \(\mathbb{R}^2\), or basically let‚Äôs denote it as \(S^1\) (this is a standard notation as</p> <p>\(S^{n} = \{ x \in \mathbb{R}^{n+1} \mid |x| = 1 \}\),</p> <p>representing the surface of an \((n+1)\)-dimensional ball). Pick \(p = (1, 0)\). If I ask you what the tangent vector is starting at \(p\) and tangnet to \(S^1\)? Your answer is probably a vector pointing upward or downward with its tail at \(p\). Indeed,</p> <h2 id="1-three-equivalent-definitions-of-tangent-space">1] Three equivalent definitions of Tangent Space</h2> <p>We‚Äôll cover three equivalent definitions tangent space.</p> <h3 id="11-tangent-vectors-are-equivalence-classes-of-curves">1.1] Tangent vectors are equivalence classes of curves</h3> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>tangent vectors are equivalence classes of smooth curves through \(p\):</p> \[T_pM = \{\frac{d}{dt}\Big|_{t = 0} \gamma(t) \Big| \gamma: (\epsilon, \epsilon) \rightarrow M, \; \gamma(0) = p \}\] <p>where \(\gamma_1 \sim \gamma_2\) if in some (equivalently, any) coordinate chart \(\phi: U \subset M \rightarrow \mathbb{R}^n\):</p> \[\frac{d}{dt}\Big|_{t = 0} \phi(\gamma_1(t)) = \frac{d}{dt}\Big|_{t = 0} \phi(\gamma_2(t))\] <p>Consequently, each tangent vector is represented by the velocity of a curve through \(p\).</p> <h3 id="12-tangent-vectors-are-derivations-at-p">1.2] Tangent vectors are derivations at \(p\)</h3> <p>Let \(M\) be a manifold, the tangent space at \(p \in M\), denoted as \(T_pM\), is the the vector space of all derivations at \(p\). Notice that a derivation at \(p\) is a <strong>linear</strong> operator (at \(p\)):</p> \[D: C^{\infty}_{p}(M) \rightarrow \mathbb{R}\] <p>satisfying the Leibniz rule:</p> \[D(fg) = f(p)D(g) + g(p)D(f), \; \forall f, g \in C^{\infty}(M)\] <p>where \(C^{\infty}_{p}\) is the equivalence class of \((f, U)\), where \(f \in C^{\infty}\) and \(U\) is a neighbourhood of \(p\). Two functions \((f, U)\) and \((g, V)\) are equivalent iff \(\exist \; W \subset U \cap V\) a neighbourhood of \(p\) such that \(f(x) = g(x), \forall x \in W\), where \(g \in C^{\infty}\) and \(V\) is a neighbourhood of \(p\).</p> <p>Intuitively, \(D\) is like a directional derivative operator acting on smooth functions near \(p\). Consequently,</p> \[T_p{M} = \{ D \mid D \; \mathrm{is\ a\ derivation\ at} p \}\] <p>and a tangent vector \(v \in T_p{M}\) is a derivation \(D\). In fact, Tu (Theorem 2.2, An introduction to manifolds) showed that there‚Äôs a bijection between derivations at \(p\) and directional derivaties.</p> <h3 id="13-tangent-vectors-as-equivalences-on-function-germs">1.3] Tangent vectors as equivalences on function germs</h3> <h2 id="2-local-coordinate-description">2] Local coordinate description</h2> <p>Specifically if we adopt interpretation [2], then with \((U, \phi)\) a chart with coordinates \((x^1, \dots, x^n)\) near \(p\)</p> \[\{ \frac{\partial}{\partial x^1}\Big|_p, \dots, \frac{\partial}{\partial x^n}\Big|_p \}\] <p>form a basis of \(T_pM\).</p> <p>Thus any tangent vector \(v\) can be written uniquely as</p> \[v = \sum_{i = 1}^{n} v^i \frac{\partial}{\partial x^i}\Big|_p\] <p>where \((v^1, \dots, v^n)\) are the components of the vector in this coordinate system.</p> <h2 id="3-equivalence-between-derivations-and-curves">3] Equivalence between derivations and curves</h2> <p>Perhaps not so surprisingly, the above two definitions are compatible and even equivalent to one another.</p> <p>From curves to derivations:</p> <h2 id="4-a-concrete-computational-example">4] A concrete computational example</h2> <p>A coherent walkthrough using the sphere \(S^2\)</p> <p>Let‚Äôs re-emphasize again that a vector field on a Manifold is defined as the following:</p> <p>Let \(M\) be a smooth manifold. A <strong>vector field</strong> on \(M\) is a smooth section/assignment \(p \mapsto X_p \in T_p M\) where \(T_p M\) is the tangent space at \(p\).</p> <p><strong>Fundamental viewpoint</strong>:</p> <blockquote> <p>A tangent vector is a <strong>derivation</strong>: a linear map \(X_p : C^\infty(M) \to \mathbb{R}\) satisfying the Leibniz rule.</p> </blockquote> <p>We‚Äôll also give the local coordinates and coordinate vector fields as the following:</p> <p>Let \((U, \varphi), \quad \varphi : U \subset M \to V \subset \mathbb{R}^n\) be a coordinate chart, with coordinates \((x^1, \dots, x^n).\)</p> <p>Each coordinate \(x^i\) is itself a <strong>function on the manifold</strong>: \(x^i : U \to \mathbb{R}.\)</p> <p>Definition of the ‚ÄúCoordinate Vector Field‚Äù:</p> <p>The coordinate vector field \(\left.\frac{\partial}{\partial x^i}\right|_p\) is defined <strong>intrinsically</strong> by its action on smooth functions: \(\boxed{ \left.\frac{\partial}{\partial x^i}\right|_p(f) := \frac{\partial}{\partial x^i} \big(f \circ \varphi^{-1}\big) \Big(\varphi(p)\Big) }\) where \(x^i\) is the \(i\)-th coordinate on \(\mathbb{R}^n\).</p> <p>Notice that there‚Äôs an abuse of notation (the left ‚Äúpartial‚Äù is what we define, whereas the right partial is the usual partial differentiation). Hopefully, it‚Äôs obvious that the above definition stems from the following basic definition of graph differential (which I will talk more into next time):</p> \[\boxed{ (dF_p(v))(f) := v_p(f \circ F) }\] <p>More importantly, this definition:</p> <ul> <li>uses <strong>only</strong> the chart,</li> <li>does <strong>not</strong> require an embedding and thus does $NOT$ give us components in $\mathbb{R}^n$</li> <li>explains what ‚Äú\(\partial f / \partial x^i\)‚Äù actually means: It means the ordinary partial derivative of the coordinate expression of $f$ after pulling $f$ back to a coordinate chart.Note that nothing is being differentiated on $\mathbb{R}^n$ unless we explicitly choose an embedding (see below).</li> </ul> <hr/> <p>Now, let‚Äôs take a look at the simple example: Sphere \(S^2\) and spherical coordinates. Given the manifold \(S^2 = \{(x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}\)</p> <p>Coordinate chart (away from poles)</p> <p>\((\theta, \varphi)\) with embedding map \(F(\theta,\varphi) = (\sin\varphi\cos\theta,\; \sin\varphi\sin\theta,\; \cos\varphi).\)</p> <p>Here:</p> <ul> <li>\(\theta, \varphi\) are <strong>functions on \(S^2\)</strong>,</li> <li>not abstract variables.</li> </ul> <p>According to the above discussions, the <strong>intrinsic</strong> meaning of \(\partial / \partial \theta\) and \(\partial / \partial \varphi\) are the following:</p> <p>For any \(f : S^2 \to \mathbb{R}\), \(\frac{\partial}{\partial \theta}(f) = \frac{\partial}{\partial \theta} \big(f \circ \varphi^{-1}\big)(\theta,\varphi), \quad \frac{\partial}{\partial \varphi}(f) = \frac{\partial}{\partial \varphi} \big(f \circ \varphi^{-1}\big).\)</p> <p>Again, this is the <strong>definition</strong>, not an interpretation. This offers us one way to calculate the vector field/tangent vectors at $\forall p \in M$.</p> <p>If we set</p> <p>\(f(x, y, z) = z\), then we could pull it back and obtain \((f \circ \phi^{-1})(\theta, \phi) = \cos(\phi)\). Consequently,</p> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X_p(f)= \frac{\partial \cos(\phi)}{\partial \theta} = 0\] <p>at $p = (\theta, \phi)$.</p> <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y_p(f)= \frac{\partial \cos(\phi)}{\partial \phi} = -\sin(\phi)\] <p>at $p = (\theta, \phi)$.</p> <hr/> <p>However, because \(S^2 \subset \mathbb{R}^3\), we may compute concrete representatives by viewing it as an embedded in $\mathbb{R}^3$ and to extrinsically compute the vector fields by pushforward of coordinate basis vectors via the embedding map of the manifold. Consequently, such computation lives in the embedded picture, not the intrinsic definition.</p> \[\frac{\partial}{\partial \theta} = \frac{\partial F}{\partial \theta} = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0)\] \[\frac{\partial}{\partial \varphi} = \frac{\partial F}{\partial \varphi}= (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi)\] <p>These are <strong>actual vectors in \(\mathbb{R}^3\)</strong> tangent to \(S^2\).</p> <blockquote> <p>Important:<br/> This is <strong>not the definition</strong> of coordinate vector fields ‚Äî<br/> it is a <strong>representation</strong> using the embedding.</p> </blockquote> <p>Consequently, we could act on functions without pulling back to coordinate space:</p> <p>Let \(f(p) = z(p)\) be a function on \(S^2\).</p> <p>Choose an extension \(\tilde f(x,y,z) = z \quad\Rightarrow\quad \nabla \tilde f = (0,0,1).\)</p> <p>Here‚Äôs the key observation:</p> <blockquote> <p>Once a tangent vector is represented in \(\mathbb{R}^3\), its action on \(f\) is given by the directional derivative of an extension of \(f\).</p> </blockquote> <p>Formally, \(\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\) where</p> <ul> <li> \[\tilde{f}$ is any smooth extension of $f$ to $\mathbb{R}^3\] </li> <li>\(X_p \in T_pS^2 \subset \mathbb{R}^3\).</li> </ul> <p>This works because tangent vectors annihilate normal components.</p> <hr/> <blockquote> <p>Example 1: \(X = \partial/\partial\theta\)</p> </blockquote> \[X(f)= (0,0,1) \cdot (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0) = 0\] <hr/> <blockquote> <p>Example 2: \(Y = \partial/\partial\varphi\)</p> </blockquote> \[Y(f)= (0,0,1) \cdot (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi) = -\sin\varphi\] <p>Same results as the intrinsic definition.</p> <p>Note that to be very concrete, in the above examples, at $p = (\theta, \phi)$, the tangent vector is:</p> <p>\(X_p = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\;0)\) (we could interpret this as horizontal circles of latitude) and \(Y_p = (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\ -\sin\varphi)\). This should make it very clear that we are treating the tangent vectors as actually ‚Äúvectors‚Äù living in $\mathbb{R}^3$ like how we usually refer them to be. With this extrinsic embedding, the general form of a vector field coincides with the intrinsic notion, but more tangible:</p> <p>\(X = a(\theta, \phi)\frac{\partial}{\partial \theta} + b(\theta, \phi)\frac{\partial}{\partial \phi}\),</p> <p>then as a vector in \(\mathbb{R}^3\): \(X_p = a \frac{\partial F}{\partial \theta} + b\frac{\partial F}{\partial \phi}\), and</p> \[X_p(f) = \nabla \tilde{f}(p) \cdot X_p\] <p>Notice that for this computation no coordinate pullback is required. But to be crystal clear, we do <strong>not</strong> compute the vector field ‚Äúwithout coordinates‚Äù. We simply You replaced intrinsic coordinates on $\mathbb{S}^2$ by ambient Cartesian coordinates in $\mathbb{R}^3$. So coordinates are still there ‚Äî just in a different space.</p> <hr/> <p>Before we continue, let‚Äôs dwell on this formula for a while:</p> \[\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\] <p>This identity guarantees consistency between 1] the derivation definition and 2] the embedded vector representation (Question: is this just chain rule?s). It explains why the extrinsic calculation agrees with the intrinsic definition. If we have a nice embedding, two views are nice. However, if we go to the general abstract case without the ambient space, we could only assort to the intrinsic definition.</p> <hr/> <p>To be rigorous, you may be concerned about the specific use of the function extension here. However, we could show that extensions do not matter, because <strong>‚Äútangent vectors annihilate normal components‚Äù</strong>. Let me explain below:</p> <p>If \(g : \mathbb{R}^3 \to \mathbb{R}\) satisfies \(g|_{S^2} = 0,\) then for all \(v \in T_p S^2\), \(v(g) = 0.\) Equivalently, \(\nabla g(p) \perp T_p\mathbb{S}^2\)</p> <hr/> <h3 id="proof-via-curves">Proof (via curves):</h3> <p>Let \(\gamma(t) \subset S^2\) with \(\gamma(0)=p, \quad \gamma'(0)=v.\)</p> <p>Since \(g(\gamma(t)) = 0\) for all \(t\), \(v(g) = \frac{d}{dt} g(\gamma(t))\big|_{t=0} = 0.\)</p> <hr/> <h3 id="concrete-example">Concrete example</h3> <p>Let \(g(x,y,z) = x^2 + y^2 + z^2 - 1.\)</p> <p>Then: \(\nabla g = (2x,2y,2z)\) which is normal to \(S^2\).</p> <p>For any tangent vector \(v\), \(v(g) = \nabla g \cdot v = 0.\)</p> <hr/> <h3 id="consequence-crucial">Consequence (crucial)</h3> <p>If \(\tilde f_1\) and \(\tilde f_2\) are two extensions of \(f\), then \(g = \tilde{f_1} - \tilde{f_2}\) vanishes on $\mathbb{S}^2$. For any tangent vector $v$:</p> \[v(\tilde f_1) - v(\tilde f_2) = v(\tilde g) = 0\] <p>thus:</p> \[v(\tilde f_1) = v(\tilde f_2) \quad\forall v \in T_p S^2.\] <p>Hence directional derivatives along tangent vectors are <strong>well-defined</strong>: this is why directional derivatives along tangent vectors do not depend on how f extends off the manifold.</p> <p>View from another perspective, this indicates that a function that is identically zero on the manifold cannot change when we move tangentially: so tangent vectors ‚Äúdon‚Äôt see‚Äù normal variations. What I mean by the former statement ‚ÄúTangent vectors annihilate normal components‚Äù is that derivatives along tangent directions ignore any part of a function that only varies off the manifold, because those variations are invisible to curves that stay on the manifold.</p> <hr/> <h3 id="intrinsic-vs-extrinsic-viewpoints-summary">Intrinsic vs Extrinsic Viewpoints (Summary)</h3> <table> <thead> <tr> <th>Aspect</th> <th>Intrinsic</th> <th>Extrinsic</th> </tr> </thead> <tbody> <tr> <td>Tangent vector</td> <td>Derivation</td> <td>Vector in \(\mathbb{R}^3\)</td> </tr> <tr> <td>Definition</td> <td>Via charts</td> <td>Via embedding</td> </tr> <tr> <td>Computation</td> <td>Pullback</td> <td>Directional derivative</td> </tr> <tr> <td>Dependence</td> <td>Coordinate-dependent</td> <td>Embedding-dependent</td> </tr> </tbody> </table> <p>Both viewpoints are <strong>equivalent</strong> when an embedding exists.</p> <hr/> <p>Something to note from the above example:</p> <blockquote> <p>Coordinate vector fields are <strong>defined intrinsically as derivations</strong>,<br/> may be <strong>represented extrinsically</strong> using embeddings,<br/> and act on functions in a way that is <strong>independent of extensions</strong> because tangent vectors annihilate normal components.</p> </blockquote> <p>Through this simple example we already develop a taste of the difference between intrinsic and extrinsic geometry, the abstract definitions and concrete calculations. Such theme will be recurring for our journey in studying differential geometry.</p> <p>One of the advantages of the intrinsic view is that it is independent of coordinate charts. It paves the ground for explaining why</p> <blockquote> <p><strong>Gradients, geodesics, and Lie brackets make sense without embedding (or ever mentioning) the abstract manifold into $\mathbb{R}^n$</strong>.</p> </blockquote> <h2 id="4-discussions">4] Discussions</h2> <p>Firstly, note that tangent/cotangent space is an intrinsic? property. A usual image of interpreting tangent space is \(S^2\) embedded in \(\mathbb{R}^3\). Our definition above does not require so, and in fact the shift of perspective from extrinsic into intrinsic properties of geometric objects is a grand evolution starting from Gauss and Rieman.</p> <p>[TODO: An illustrative figure of a sphere embedded in Euclidean space]</p> <p>From derivations to curves:</p> <h3 id="41-abuse-of-notation">4.1] Abuse of notation</h3> <p>Many textbooks will carry abuse of notation throughout their content, most often conflating functions/derivations defined on the manifold with them composed with local coordinates. I find it really confusing and hard to interpret many times, so I decide to, once again, present one simple example where no abuse of notation whatsoever happens, so we could get it crystal clear. Let‚Äôs begin.</p> <hr/> <h4 id="411-clear-statement">4.1.1] Clear Statement</h4> <p>Let:</p> <ul> <li>\(M\) be a smooth manifold.</li> <li>\(c : I \subset \mathbb{R} \to M\) be a smooth curve.</li> <li>\((U,\varphi)\) be a coordinate chart with<br/> \(\varphi : U \to \mathbb{R}^n\)</li> <li>Assume \(c(t_0) \in U\).</li> </ul> <p>Then the tangent vector \(c'(t_0) \in T_{c(t_0)}M\) satisfies:</p> \[c'(t_0) = \sum_{i=1}^n \left. \frac{d}{dt}(x^i \circ c)(t) \right|_{t_0} \; \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)}\] <p>where all objects are defined precisely below.</p> <hr/> <h4 id="412-definition-of-the-tangent-vector-to-a-curve">4.1.2] Definition of the Tangent Vector to a Curve</h4> <p>Fix \(t_0 \in I\).</p> <p>The tangent vector \(c'(t_0) \in T_{c(t_0)}M\) is defined as the derivation:</p> \[c'(t_0)(f) := \left. \frac{d}{dt}(f \circ c)(t) \right|_{t=t_0}\] <p>for every smooth function \(f \in C^\infty(M)\). Notice that this is the <strong>definition</strong> of the velocity vector, and there‚Äôs no coordinates are used here.</p> <hr/> <h4 id="413-introduce-a-coordinate-chart">4.1.3] Introduce a Coordinate Chart</h4> <p>Let \((U,\varphi)\) be a coordinate chart with:</p> <ul> <li> \[c(t_0) \in U\] </li> <li> \[\varphi: U \to \mathbb{R}^n\] </li> </ul> <p>Write:</p> \[\varphi(p) = (x^1(p),\dots,x^n(p))\] <p>where</p> \[x^i := \pi^i \circ \varphi\] <p>and \(\pi^i : \mathbb{R}^n \to \mathbb{R}\) is the standard projection.</p> <p>Thus each coordinate function</p> \[x^i : U \to \mathbb{R}\] <p>is smooth.</p> <hr/> <h4 id="414-coordinate-basis-of-the-tangent-space">4.1.4] Coordinate Basis of the Tangent Space</h4> <p>For each \(p \in U\), define the coordinate vector:</p> \[\left.\frac{\partial}{\partial x^i}\right|_p\] <p>as the derivation:</p> \[\left.\frac{\partial}{\partial x^i}\right|_p (f) := \left. \frac{\partial}{\partial u^i} \Big( f \circ \varphi^{-1} \Big) (u) \right|_{u=\varphi(p)}\] <p>for all smooth functions \(f\) defined near \(p\).</p> <p>This definition proceeds in three steps:</p> <ol> <li>Pull \(f\) back to \(\mathbb{R}^n\) via \(\varphi^{-1}\)</li> <li>Take the ordinary partial derivative</li> <li>Evaluate at the coordinate point \(\varphi(p)\)</li> </ol> <p>These vectors form a basis of \(T_pM\), as we know from above.</p> <hr/> <h4 id="415-express-the-curve-in-coordinates">4.1.5] Express the Curve in Coordinates</h4> <p>Define the coordinate representation of the curve:</p> \[\gamma := \varphi \circ c\] <p>Thus:</p> \[\gamma : I \to \mathbb{R}^n\] <p>Write:</p> \[\gamma(t) = (\gamma^1(t),\dots,\gamma^n(t))\] <p>where</p> \[\gamma^i(t) = \pi^i(\gamma(t)) = \pi^i(\varphi(c(t))) = (x^i \circ c)(t)\] <p>So:</p> \[\gamma^i = x^i \circ c\] <p>These are ordinary real-valued functions.</p> <hr/> <h4 id="416-compute-the-action-of-ct_0">4.1.6] Compute the Action of \(c'(t_0)\)</h4> <p>Let \(f \in C^\infty(M)\).</p> <p>By definition:</p> \[c'(t_0)(f) = \left. \frac{d}{dt}(f \circ c)(t) \right|_{t_0}\] <p>Insert identity:</p> \[f \circ c = (f \circ \varphi^{-1}) \circ (\varphi \circ c)\] <p>Define:</p> \[F := f \circ \varphi^{-1}\] <p>Then:</p> \[c'(t_0)(f) = \left. \frac{d}{dt} F(\gamma(t)) \right|_{t_0}\] <hr/> <h4 id="417-apply-the-multivariable-chain-rule-in-mathbbrn">4.1.7] Apply the Multivariable Chain Rule in \(\mathbb{R}^n\)</h4> <p>Since everything is now in \(\mathbb{R}^n\), apply the usual chain rule:</p> \[\frac{d}{dt}F(\gamma(t)) = \sum_{i=1}^n \frac{\partial F}{\partial u^i}(\gamma(t)) \, \frac{d\gamma^i}{dt}(t)\] <p>Evaluating at \(t_0\) gives:</p> \[c'(t_0)(f) = \sum_{i=1}^n \frac{d\gamma^i}{dt}(t_0) \, \frac{\partial F}{\partial u^i} (\gamma(t_0))\] <hr/> <h4 id="418-return-to-manifold-language">4.1.8] Return to Manifold Language</h4> <p>Recall:</p> <ul> <li> \[\gamma(t_0)=\varphi(c(t_0))\] </li> <li> \[F = f \circ \varphi^{-1}\] </li> </ul> <p>Thus:</p> \[\frac{\partial F}{\partial u^i}(\gamma(t_0)) = \left. \frac{\partial}{\partial u^i} (f \circ \varphi^{-1}) \right|_{u=\varphi(c(t_0))}\] <p>By definition of the coordinate vector fields:</p> \[= \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)}(f)\] <p>Therefore:</p> \[c'(t_0)(f) = \sum_{i=1}^n \frac{d\gamma^i}{dt}(t_0) \, \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)}(f)\] <p>Since this equality holds for every smooth function \(f\), we conclude:</p> \[c'(t_0) = \sum_{i=1}^n \frac{d\gamma^i}{dt}(t_0) \, \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)}\] <p>Finally substitute:</p> \[\gamma^i = x^i \circ c\] <p>so:</p> \[\frac{d\gamma^i}{dt}(t_0) = \frac{d}{dt}(x^i \circ c)(t_0)\] <hr/> <h4 id="419-final-fully-precise-formula-and-summary">4.1.9] Final Fully Precise Formula and Summary</h4> \[\boxed{ c'(t_0) = \sum_{i=1}^n \left. \frac{d}{dt}(x^i \circ c)(t) \right|_{t_0} \; \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)} }\] <p>In summary, the above elaboration consists of:</p> <ol> <li>Pulling everything to \(\mathbb{R}^n\)</li> <li>Applying the ordinary multivariable chain rule</li> <li>Translating back via the definition of coordinate vector fields</li> </ol> <p>Thus, the coordinate representation of the velocity vector is precisely the multivariable chain rule expressed in intrinsic manifold language.</p> <hr/> <h3 id="42-orthonormal-basis">4.2] Orthonormal basis?</h3> <p>Another subtle point that haunted me for while is the questions of whether the basis \(\left\{\frac{\partial}{\partial x_i}\Big|_p\right\}_{i=1,2,...,n}\) are orthonormal or not.</p> <p>The short answer is No. The coordinate basis is <strong>not necessarily orthonormal</strong>. It‚Äôs true that these vectors:</p> <ul> <li>Form a basis of \(T_pM\).</li> <li>Depend on the coordinate choice.</li> <li>Are defined geometrically as directional derivatives.</li> </ul> <p>However, nothing in their definition guarantees that they are orthogonal or normalized. In fact, even more basic, orthonormality requires a <strong>metric</strong>! To even ask whether a basis is orthonormal, we must have a <strong>Riemannian metric</strong> \(g\) (will talk about this in details later; readers not familiar with metric/Riemannian metric feel free to please come back later; I include this short discussion for completeness and I apologize for grabbing future yet-to-be-learned knowledge as part of this section‚Äôs story). Without a metric, we cannot measure angles and lengths, and thus ‚Äúorthonormal‚Äù is meaningless.</p> <p>But even given a Riemannian metric \(g\), usually orthonormality is not guaranteed. We can easily compute</p> \[g_{ij}(p) = g\!\left(\frac{\partial}{\partial x_i}, \frac{\partial}{\partial x_j}\right)_p.\] <p>In general,</p> \[g_{ij}(p) \neq \delta_{ij}.\] <p>So the coordinate basis is usually <strong>not orthonormal</strong>.</p> <p>Let‚Äôs do a simlpe example: the sphere. Consider the 2-sphere \(S^2\) with spherical coordinates \((\theta, \phi)\).</p> <p>Given a metric:</p> \[ds^2 = d\theta^2 + \sin^2\theta \, d\phi^2.\] <p>Then:</p> \[g\left(\frac{\partial}{\partial \theta}, \frac{\partial}{\partial \theta}\right) = 1,\] \[g\left(\frac{\partial}{\partial \phi}, \frac{\partial}{\partial \phi}\right) = \sin^2\theta.\] <p>These coordinate vectors are indeed:</p> <ul> <li>Orthogonal.</li> <li>But <strong>not normalized</strong> unless \(\sin\theta = 1\).</li> </ul> <p>Even in nice coordinates, the basis is typically not orthonormal.</p> <p>So then when are they orthonormal? As said above, they are orthonormal only if</p> \[g_{ij}(p) = \delta_{ij}.\] <p>This happens in special cases, like in</p> <ul> <li>Case 1: Euclidean Space. In \(\mathbb{R}^n\) with the standard metric,</li> </ul> \[g_{ij} = \delta_{ij},\] <p>so the coordinate basis is orthonormal everywhere.</p> <ul> <li>Case 2: Riemannian Normal Coordinates: On any Riemannian manifold, we can construct <strong>normal coordinates</strong> around \(p\) such that</li> </ul> \[g_{ij}(p) = \delta_{ij}.\] <p>However, this holds only at the single point \(p\). Away from \(p\), the metric is no longer Euclidean. On a side note, this reflects a deep geometric fact:</p> <blockquote> <p>Every Riemannian manifold looks Euclidean to first order at a point.</p> </blockquote> <p>To do a short summary and comparison:</p> <p>Coordinate basis vectors:</p> <ul> <li>Depend on how we label the manifold.</li> <li>Reflect the coordinate grid.</li> <li>May stretch or skew according to the geometry.</li> </ul> <p>An orthonormal basis instead:</p> <ul> <li>Is chosen using the <strong>metric</strong>.</li> <li>Is independent of coordinates.</li> <li>Can be obtained via Gram‚ÄìSchmidt if needed.</li> </ul>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Intuition and definition of tangent/cotangent space]]></summary></entry><entry><title type="html">The Dance of Space: Geom/Topo/Dynam Mumble(2) (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/" rel="alternate" type="text/html" title="The Dance of Space: Geom/Topo/Dynam Mumble(2) (in progress)"/><published>2025-09-17T17:49:33+00:00</published><updated>2025-09-17T17:49:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Today I will cover a beautiful subject in differenetial geometry: Ricci flows and Ricci curvature.</p> <p>Generally, in Riemannian geometry, curvature measures how space bends. For instance, on a sphere, geodesics (shortest paths) come closer together compared to flat space; on a hyperbolic surface, they diverge.</p> <p>Ricci curvature is a particular way of summarizing curvature: Instead of describing how all directions bend (that‚Äôs what the full <strong>Riemann curvature tensor</strong> does), Ricci curvature focuses on <strong>volume distortion</strong>. More concretely, it tells us how the volume of a small geodesic ball deviates from the volume we‚Äôd expect in flat Euclidean space.</p> <p>Intuitively, we could summarize into the following:</p> <blockquote> <p>Positive Ricci curvature (like on a sphere) means geodesics tend to converge, and small balls have less volume than in flat space.</p> <p>Zero Ricci curvature (like in Euclidean space) means geodesics neither converge nor diverge, so volumes match Euclidean.</p> <p>Negative Ricci curvature (like on a hyperbolic space) means geodesics diverge, so small balls have more volume than Euclidean.</p> </blockquote> <p>Mathematically, Ricci curvature is obtained by ‚Äútracing‚Äù the <strong>Riemann curvature tensor</strong>. It compresses information about how different directions curve into a symmetric 2-tensor <code class="language-plaintext highlighter-rouge">Ric</code>.</p> <h3 id="riemannian-geometry-and-tensor">Riemannian Geometry and Tensor</h3> <p>The Riemann tensor is written in the following way:</p> \[R(X, Y)Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X, Y]} Z\] <p>This tensor captures all information about curvature. Succinctly, this is a 4-tensor: \(R_{ijkl}\).</p> <p>The above is an unfair treatment of Riemannian geometry. I‚Äôll have a separate blog on that subject soon.</p> <p>How to understand: Riemannian metric tensor informs the manifold where to expand, shrink, and curve. How does Riemannian metric tensor relate with curvature?</p> <h3 id="ricci-curvature">Ricci Curvature</h3> <p>Based on the Riemann tensor, what is the curvature?</p> <p>To get Ricci curvature, we take a <strong>trace</strong> of the Riemann tensor:</p> \[Ric_{ij} = R^{k}_{ikj} = g^{kl}R_{kilj}\] <p>This reduces the information down to a 2-tensor (like the metric itself). Geometrically, this represents the volume distortion of geodesic balls.</p> <h3 id="ricci-flow">Ricci Flow</h3> <p>Introduced by <code class="language-plaintext highlighter-rouge">Richard Hamilton (1982)</code>, Ricci flow is a process that evolves a Riemannian metric \(g(t)\) over time:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>The factor -2 is just convention (to simplify later computations). We could think of this as a heat equation for geometry: Just as heat diffuses to smooth out temperature differences, Ricci flow smooths out <strong>irregularities</strong> in curvature. As illustrated in the above section of Ricci curvature, we could naturally arrive at the result that positive curvature regions tend to shrink and negative curvature regions expand. Over time, the underlying geometry becomes more ‚Äúregular‚Äù, like ironing out wrinkles.</p> <p>The effect of Ricci flow on curvature could be expressed in the following way. The derivative of scalar curvature \(R\) under this flow is:</p> \[\frac{\partial R}{\partial t} = \Delta R + 2 |Ric|^2\] <p>This resembles a heat equation $(\Delta R)$ plus a positive correction. Consequently, curvature tends to diffuse out but also grows in positive-curvature regions.</p> <h4 id="examples-of-ricci-flow">Examples of Ricci Flow</h4> <p>Perhaps the simplest example is to imagine how a sphere evolves under Ricci flow. We all know that a round sphere has positive Ricci curvature. The Ricci flow equation says:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>Since Ricci is positive, the metric shrinks, and thus makes the sphere to contract uniformly, eventually collapsing to a point. This closely mirrors the idea that positive curvature makes geodesics converge. Under the Ricci flow, it tightens further.</p> <p>Another simple example is the flat torus. Since the Ricci curvature is 0 everywhere,</p> \[\frac{\partial g_{ij}}{\partial t} = 0\] <p>the torus will stay unchanged after the flow forever. This is analogous to heat diffusion on a perfectly uniform temperature field where nothing would effectively changes.</p> <p>Likewise, a hyperbolic surface has negative Ricci curvature. Consequently, under the Ricci flow, the metric would expand and the hyperbolic surface would grow larger and more uniform in curvature.</p> <p>In a nutshell, irregular geometries with bumps or folds (different curvature in different regions) get ‚Äúsmoothed‚Äù over time. All the high-curvature ‚Äúwrinkles‚Äù would get flatten out, like how heat equalizes temperature.</p> <h3 id="application-of-ricci-flow">Application of Ricci Flow</h3> <p>Poincare conjecture, Ricci flow, surgery theory, what Terrence Tao called ‚Äúone of the most impressive recent achievements of modern mathematics‚Äù</p> <p>Poincare conjecture:</p> <blockquote> <p>Any closed 3-manifold that is simply-connected, compact, and boundless is homeomorphic to a 3-sphere.</p> </blockquote> <p>Specifically, Poincare conjecture in higher dimensions has been solved around 1961, and dimension 4 case has been proved by Michael Freedman who by which won Fields medal in 1986. The \(n = 3\) case seemed really difficult to crack and it was only at 2002 that Grisha Perelman proved it using Ricci flow.</p> <p>Very briefly, since a sphere has positive curvature, by applying Ricci flow through time such sphere will contract and eventually vanish. Perelman proved the opposite also holds: if metric goes to 0, it must have been a sphere. To prove Poincare‚Äôs conjecture using Ricci flow,</p> <p>One of the most triumphant use of Ricci flow happens when Grigori Perelman (2002‚Äì2003) to prove <strong>the Poincar√© conjecture</strong> and the more general <strong>Thurston geometrization conjecture</strong>. He showed how Ricci flow with ‚Äúsurgery‚Äù (cutting and patching when singularities form) classifies 3-manifolds.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Introduction of Ricci flows and Ricci curvatures]]></summary></entry><entry><title type="html">The Dance of Space: Geom/Topo/Dynam Mumble(3) (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/" rel="alternate" type="text/html" title="The Dance of Space: Geom/Topo/Dynam Mumble(3) (in progress)"/><published>2025-09-16T16:34:22+00:00</published><updated>2025-09-16T16:34:22+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Differential_Forms</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/"><![CDATA[<p>The second episode to appreciate the inner workings of space is through the differential forms. Differential forms is not an ancient subject, ‚Ä¶</p> <h3 id="vector-outer-product">Vector Outer Product</h3> <h3 id="wedge-productexterior-derivative">Wedge Product/Exterior Derivative</h3> <h3 id="three-in-one">Three in One</h3> <p>Green‚Äôs theorem, Gauss‚Äô theorem, Stoke‚Äôs theorem.</p> <p>Green‚Äôs theorem:</p> \[\int_{L} Pdx + Qdy = \iint_{D}(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y})dxdy\] <p>Generalized Stoke‚Äôs theorem.</p> <p>A k-form is supposed to be integrated over an oriented k-dimensional manifold</p> <h3 id="fundamental-theorem-of-calculus-ftoc">Fundamental Theorem of Calculus (FTOC)</h3> <p>High dimensional Stoke‚Äôs theorem is exactly the fundamental theorem of calculus (FTOC) in high-dimensional space.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Intuite and define differential forms]]></summary></entry><entry><title type="html">Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Equivalence/" rel="alternate" type="text/html" title="Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)"/><published>2025-09-07T00:23:16+00:00</published><updated>2025-09-07T00:23:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Equivalence</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Equivalence/"><![CDATA[<p>Explain and compare multiple equivalences from differential geometry and topology, including homeomorphism, diffeomorphism, homotopy equivalence, homomorphism, isomorphism, etc.</p> <p>Invariant and equivariant functions (CNN is equivariant).</p> <p>Also discussions on cardinality among sets (including finite and infinite (countably infinite \(N0\)? and uncountably infinite), Hillbert Hotel problem, equipotent sets)</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Topology"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Compare various equivalences in geometry/topology/group theory]]></summary></entry></feed>