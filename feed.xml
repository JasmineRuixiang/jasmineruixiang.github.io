<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jasmineruixiang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasmineruixiang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-14T03:42:23+00:00</updated><id>https://jasmineruixiang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Manifold and Riemannian Geometry (X): Connections, Parallel Transport, and Geodesics (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(X)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (X): Connections, Parallel Transport, and Geodesics (in progress)"/><published>2026-02-13T16:36:23+00:00</published><updated>2026-02-13T16:36:23+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(X)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(X)/"><![CDATA[<p>This is episode X on the Manifold and Riemannian Geometry series. Today we will be exploring more on tangent vectors, vector fields, another key concept about how to relate tangent spaces together, and many other derivative notions crucial for the development of Riemannian Geometry.</p> <h2 id="0-intuition-and-introduction-directional-derivatives">0] Intuition and Introduction: Directional Derivatives</h2> <p>Connection: Bridging derivatives from \(\mathbb{R}^n\) to curved manifolds. The concept of a connection is the necessary tool that allows us to perform differential calculus on curved spaces (manifolds), such as the surface of a sphere. It generalizes the familiar idea of the directional derivative from flat Euclidean space (\(\mathbb{R}^n\)).</p> <p>Directional Derivatives in Euclidean Space (\(\mathbb{R}^3\)) In \(\mathbb{R}^3\) with Cartesian coordinates \((x, y, z)\), the directional derivative provides a simple way to measure change. The basis vectors \(\left\{ \mathbf{i}, \mathbf{j}, \mathbf{k} \right\}\) (or the equivalent operators \(\left\{ \frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z} \right\}\)) are constant, allowing us to define derivatives simply as component-wise partial derivatives. Let \(p=(1, 2, 0)\) be a point, \(X=(y, -x, 3x)\) be the direction vector field, and \(V=(xz, y^2, -2x)\) be a vector field.</p> <p>There are two things we could implement:</p> <h3 id="1-derivative-of-a-scalar-function-d_x-f">1) Derivative of a Scalar Function (\(D_X f\)).</h3> <p>This is the directional derivative of a smooth scalar function \(f\) in the direction \(X\).</p> <p>For example, for \(f(x,y,z)=xy^2+z\) at \(p\): By definition, \(D_X f = \nabla f \cdot X\), so \(D_X f(p) = \langle 4, 4, 1 \rangle \cdot \langle 2, -1, 3 \rangle = \mathbf{7}\)</p> <p>Point Derivation (Geometry) \(X[f] = X^i \frac{\partial f}{\partial x^i}\) \(Xf = (y^3 - 2x^2y + 3x)\)</p> <p>This confirms that in differential geometry, a tangent vector \(X\) is rigorously defined as a point derivation: an operator that mimics the directional derivative by satisfying the Leibniz rule (please see my <a href="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/">previous blog</a> on the definition and interpretation of tangent vectors).</p> <h3 id="2-derivative-of-a-vector-field-d_x-v">2) Derivative of a Vector Field (\(D_X V\))</h3> <p>This derivative measures how the vector field \(V\) changes as we move in the direction \(X\). In \(\mathbb{R}^3\), this is calculated by taking the directional derivative of each component of \(V\).</p> <p>Using the vector fields \(X\) and \(V\), we notice that the \(k\)-th component of the resulting vector \(D_X V\) is \((D_X V)^k = \sum_{i} X^i \frac{\partial V^k}{\partial x^i}\).</p> <p>Component 1 (i.e., \(k=1\)): \((D_X V)^1 = yz + 3x^2\) Component 2 (i.e., \(k=2\)): \((D_X V)^2 = -2xy\) Component 3 (i.e., \(k=3\)): \((D_X V)^3 = -2y\)</p> <p>Evaluating at \(p=(1, 2, 0)\) gives:</p> \[D_X V(p) = \langle 3, -4, -4 \rangle\] <hr/> <h2 id="1-why-do-we-need-a-connection">1] Why Do We Need a Connection?</h2> <p>In Euclidean space \(\mathbb{R}^{n}\), if we have a vector field \(Y(x) = (Y^1(x), \cdots, Y^n(x))\), and we could easily differentiate component-wise: \(\frac{\partial Y^i}{\partial x^j}\). This works because</p> <ul> <li>All tangent spaces are the same vector space.</li> <li>We can subtract vectors at different points.</li> <li>We can differentiate vector fields componentwise.</li> </ul> <p>But on a manifold:</p> <ul> <li>A vector field satisfies<br/> \(V(p) \in T_pM\) \(V(q) \in T_qM\)</li> <li>These live in <em>different</em> vector spaces.</li> <li>Subtracting them directly makes no sense.</li> </ul> <p>So we need a rule that tells us:</p> <blockquote> <p>How does a vector field \(Y\) change as we move in a given direction given by \(X\)?</p> </blockquote> <p>This rule is an <strong>affine connection</strong>. And we <strong>choose</strong> such a rule.</p> <hr/> <h2 id="2-definition-of-an-affine-connection">2] Definition of an Affine Connection</h2> <p>A connection is an operator operator \(\nabla: C^{\infty}(M) \times C^{\infty}(M) \to C^{\infty}(M)\) that maps two vector fields, \(X\) and \(Y\), to a new vector field \(\nabla_X Y\).</p> <p>We could think of it as:</p> <blockquote> <p>The derivative of \(Y\) in the direction \(X\).</p> </blockquote> <p>This specified choice needs to satisfy the following three rules:</p> <ul> <li> <p>Linearity in \(X\) \(\nabla_{fX + gZ} Y = f\nabla_X Y + g\nabla_Z Y\)</p> </li> <li> <p>Linearity in \(Y\) \(\nabla_X (Y + Z) = \nabla_X Y + \nabla_X Z\)</p> </li> <li> <p><strong>Leibniz Rule</strong> \(\nabla_X (fY) = X(f)Y + f\nabla_X Y\)</p> </li> </ul> <p>where $X(f) = Xf$ is the directional derivative of \(f\). This last property is crucial, as it mirrors ordinary differentiation (notice that we also require the definition of derivation when in tangent vector to also satisfy the Leibniz rule).</p> <p>Also note that this definition does <strong>not</strong> require a metric.</p> <hr/> <h2 id="3-local-coordinate-formula-and-christoffel-symbols">3] Local Coordinate Formula and Christoffel Symbols</h2> <p>Choose coordinates \((x^1, \dots, x^n)\) first.</p> <p>Let<br/> \(X_i = \frac{\partial}{\partial x^i}\)</p> <p>Note that for the following I might interchange these two symbols, but they are always equivalent.</p> <p>Define:</p> \[\nabla_{X_i} X_j = \Gamma^k_{ij} X_k\] <p>(Or, just in another format, to be crystal clear)</p> \[\nabla_{\frac{\partial}{\partial x_i}}\frac{\partial}{\partial x_j} = \sum_{k = 1}^n \Gamma^k_{ij} \frac{\partial}{\partial x_k}\] <p>We use above the Einstein summation notation as convention (to simplify notations). In below, whenever summation might be obvious I will drop it. Here the $\Gamma^k_{ij}$ are called the <code class="language-plaintext highlighter-rouge">Christoffel symbols</code>, which tell us how the coordinate basis vectors change as we move.</p> <p>Now use the above notation, say for two vector fields:</p> <p>\(X = X^i \frac{\partial}{\partial x^i}\) \(Y = Y^j \frac{\partial}{\partial x^j}\)</p> <p>We could obtain:</p> \[\boxed{ \nabla_X Y = \left(\underbrace{ X^i \frac{\partial Y^k}{\partial x^i} }_{\text{I. Flat-Space Derivative Term}} + \underbrace{X^i \Gamma^k_{ij} Y^j }_{\text{II. Curvature Correction Term}} \right)\frac{\partial}{\partial{x^k}} }\] <p>This will be our main formula. If we stair at it for a bit, the following should be intuitive:</p> <ul> <li>First term: change in components, what we shall expect in Euclidean space. It measures how the components of \(Y\) change. If the manifold were flat and the basis vectors didn’t vary, this would be enough.</li> <li>Second term: correction for changes in basis, because on a curved manifold, the basis vectors \(\frac{\partial}{\partial x^j}\) change from point to point. So even if the components \(Y^j\) stay constant, the vector field itself might change because the basis is rotating or stretching. This change is captured by the second term including the Christoffel symbols, which measure how the coordinate frame twists and turns.</li> </ul> <p>Consequently, covariant derivative is basically the combination of component change and basis change, and it’s obvious from above the Christoffel symbols entirely dictate the properties of the chosen connection.</p> <hr/> <h2 id="-4-a-short-reflection-and-discussion-">————— 4] A Short Reflection and Discussion —————</h2> <p>Let’s do a short summary and discussion before we continue. Based on the above definition, what is an affine connection conceptually? Well, an affine connection gives us a way to compare vectors at nearby points, which later will turn out to be our abilities to define</p> <ul> <li>1] a notion of parallel transport</li> <li>2] a notion of geodesics</li> <li>3] a notion of curvature</li> </ul> <p>It is the structure that tells us how to move vectors around on a manifold. Without it, differentiation makes no sense.</p> <p>At the same time, notice that apriori we <strong>choose</strong> this arbitrary connection. There are infinitely many affine connections on a manifold. Why? Recall the local definition of connection in coordinates \(\nabla_{X_i} X_j = \Gamma^k_{ij} X_k\). The Christoffel symbols \(\Gamma^k_{ij}\) can be any smooth functions. This already tells us that there are infinitely many possible connections. Nothing forces a specific choice at this stage. In fact, If \(\nabla\) is a connection and \(A\) is any smooth \((1,2)\)-tensor field, then \(\tilde{\nabla}_XY = \nabla_XY + A(X, Y)\) is another connection. So the space of connections is affine, not linear and thus there is enormous freedom. This is why it’s called “affine connection”.</p> <p>It’s only after we impose some natural geometric conditions that one becomes canonical (more to reveal below). And after introducing <code class="language-plaintext highlighter-rouge">Riemmanian metric</code>, there’re specific properties of connection that we demand which render only one unique option left (kind of miracle). That’s why we do not drown in choices.</p> <p>But let me also reinforce the idea that at this stage, no Riemannian metric at all is assumed. The definition of connection has nothing to do with any metric (we will see shortly that in some sense we do need to marry these two notions in a nice way; if this is your intuition, keep immersing yourself in it).</p> <p>Another crucial point is that, you might object that after all why can’t we just subtract tangent vectors using the ambient Euclidean structure? If you visualize a manifold as a curved surface floating in 3D, there’s clearly a nice inner product structure already defined in \(\mathbb{R}^3\) which we effortlessly borrow.</p> <p>Well, this is the deeper part, echoeing all the way back to why we even want a manifold. Recall that a general manifold has no built-in shape. A smooth manifold is just a set of points with coordinate charts which are glued together smoothly. That’s it. There’s no concept of length, angle, curvature, or embedding. Just smooth structure. We should <strong>NOT</strong> think of a general manifold as a curved surface floating in space. That is only a visualization aid.</p> <p>Shape appears only after adding extra structure. For a smooth manifold, only differentiability exists. We can talk about vector fields, Lie brackets, and differential forms. But there’s <strong>no metric</strong> yet. Now if we add an affine connection, we could then ifferentiate vector fields (and also define parallel transport and talk about geodesics, see below). However, there’s still <strong>no notion of length or angle</strong>. It’s only after we introduce a Riemannian metric that we get a notion of lengths, angles, volumes, and curvature. Only here does something resembling “shape” emerge. And remarkably, all of this can be defined without embedding.</p> <p>But before Riemannian metric, for a generic smooth manifold, embeddings are misleading. When we picture a sphere in \(\mathbb{R}^3\) or torus in space, we are seeing extrinsic geometry. But intrinsic geometry ignores how the object sits in space. A famous example is imagine a flat sheet of paper vs a cylinder made by rolling it. They look totaly different in \(\mathbb{R}^3\), but <strong>intrinsically they are the same geometry</strong>, because distances measured along the surface are unchanged. Consequently, intrinsic geometry does not care about bending in ambient space.</p> <p>Recall that Gauss proved in his Theorema Egregium that</p> <blockquote> <p>Curvature of a surface can be computed entirely from the <strong>metric</strong>.</p> </blockquote> <p>(Here Curvature specifically refers to Gaussian Curvature). It does not depend on how the surface sits in space. This was revolutionary, only to be pushed further by Riemann later that:</p> <blockquote> <p>Manifolds need not be embedded at all.</p> </blockquote> <p>They can exist abstractly.</p> <p>Consequently, if you are still thinking that a manifold is a curved surface in space, it might be helpful if you instead interpret manifold as an abstract smooth space and geometry is an additional structure placed on it. And more importantly,</p> <blockquote> <p>The <strong>metric</strong> determines what “curved” even means.</p> </blockquote> <p>The manifold is just an abstract collection of points with smooth structure, not an object with a fixed geometric shape. Shape emerges only after adding a metric.</p> <p>Now, with the above discussions, suppose we do have an embedding: \(M \in \mathbb{R}^n\). We embed it. Vectors lie in tangent planes inside \(\mathbb{R}^n\). Then yes, we can subtract them as vectors in \(\mathbb{R}^n\). So what’s the problem? Well, it lies in again the insight Gauss offered back in 1827: <strong>intrinsic vs extrinsic</strong> geometry.</p> <blockquote> <p>Let \(V(t) \in T_{\alpha(t)}M\). If we compute the ordinary derivative \(\frac{dV}{dt}\) (pretend that \(M\) never has existed), it sure lives in in \(\mathbb{R}^n\) , but generally \(\frac{dV}{dt} \notin T_{\alpha(t)}M\). It usually has a normal component. That normal component measures how the surface bends in space. But that bending <strong>depends on the specific embedding</strong> we chose.</p> </blockquote> <p>For example, on the sphere, if we carry a tangent vector along a great circle, the ambient derivative will point slightly inward (the normal component). But that normal part has nothing to do with intrinsic geometry, and it only reflects how the sphere sits in space. To get intrinsic change, we would take ambient derivative and project back to the tangent plane. (Spoiler alert: this projection then defines the Levi-Civita connection of the induced metric)</p> <p>However, projecting back to tangent space gives intrinsic derivative.</p> <ul> <li>Different embeddings give different projections.</li> <li>Intrinsic geometry should not depend on embedding.</li> </ul> <p>Connections defined intrinsically avoid embedding dependence.</p> <p>To answer the question why we cannot always use this trick?</p> <blockquote> <p>Not all manifolds come with a preferred embedding. Manifolds might be embedded but the embedding is not unique. Different embeddings give different ambient derivatives and the induced extrinsic correction indeed depends on embedding.</p> </blockquote> <blockquote> <p>On the other hand, intrinsic geometry should not depend on how we sit in space. If we rely on ambient subtraction, we’re measuring extrinsic curvature. But Riemannian geometry is centered around exploring intrinsic properties. The connection should not depend on embedding.</p> </blockquote> <p>Using Euclidean subtraction measures the intrinsic change plus the extrinsic bending, whereas the covariant derivative isolates and includes intrinsic change only.</p> <p>Enough deliberation for now, let’s continue our story.</p> <hr/> <h2 id="5-covariant-derivative-along-a-curve">5] Covariant Derivative Along a Curve</h2> <p>In many scenarios, we are interested in how a vector field “changes” on a specific trajectory. With an affine connection defined as above, how could we calculate such change?</p> <p>Given a curve \(\alpha(t)\) and vector field along the curve: \(V(t) \in T_{\alpha(t)}M\). Again, the ordinary derivative $\frac{dV}{dt}$ makes no sense because:</p> \[\frac{dV}{dt} = \lim_{h \rightarrow 0}\frac{V(t + h) - V(t)}{h}\] <p>but \(V(t+v) \in T_{\alpha(t+h)}M\), \(V(t) \in T_{\alpha(t)}M\), and subtraction requires vectors to live in the same space.</p> <p>To resolve this conflict and apply the defined connection, we define:</p> \[\frac{DV}{dt} = \nabla_{\alpha'(t)} V\] <p>where \(\alpha'(t) = \frac{d\alpha}{dt} \in T_{\alpha(t)}M\).</p> <p>which is called the <code class="language-plaintext highlighter-rouge">covariant derivative</code>. In other words, the covariant derivative along a curve is just the affine connection applied in the direction of the velocity vector. It’s not new machinery — it’s just the connection specialized to a curve.</p> <p>In coordinates, if we write \(V(t) = V^k(t)\frac{\partial}{\partial x^k}\):</p> \[\frac{DV^k}{dt} = (\frac{dV^k}{dt} + \Gamma^k_{ij} \frac{dx^i}{dt} V^j)\frac{\partial}{\partial x^k}\] <p>This is just the previous formula about coordinate representation of connections if we replace \(X = \alpha'(t)\). Consequently, we could carry the same interpretation: Covariant derivative = change in components+change in basis.</p> <p>We could interpret it this way: Imagine walking on a sphere. We carry an arrow that we try to keep “pointing in the same direction.” Even if the arrow looks constant to you, from an external viewpoint it may be rotating because:</p> <blockquote> <p>The tangent plane itself rotates as you move.</p> </blockquote> <blockquote> <p>The coordinate basis changes.</p> </blockquote> <p>So \(\frac{D V}{dt}\) actually measures the true rate of change of the vector relative to/correcting for the manifold’s geometry. There are two kinds of change: 1)Intrinsic change — the vector itself changes’ 2) Fake change — the coordinate frame changes. But \(\frac{D V}{dt}\) removes the fake change. It measures intrinsic change.</p> <hr/> <h2 id="6-parallel-vector-field-and-parallel-transport">6] Parallel Vector Field and Parallel Transport</h2> <p>Then the natural question to ask is what does it mean to have \(\frac{DV}{dt} = 0\)?</p> <p>Well, there’s a specific name for it — the vector field \(V\) such \(\frac{DV}{dt} = 0\) for a specific curve \(c\) is called a <code class="language-plaintext highlighter-rouge">parallel vector field</code> along the \(c\). Geometrically, this means that the vector is being transported without turning according to the manifold’s geometry.</p> <p>On the other hand, the vector \(V(t)\) on \(V\) is said to <code class="language-plaintext highlighter-rouge">parallel transport</code> along \(c\).</p> <p>Parallel transport is obtained by solving the following system of ODE:</p> \[\frac{dV^k}{dt} + \Gamma^k_{ij} \frac{dx^i}{dt}V^j = 0\] <p>Notice that this is just \(n\) first order equations. Pointwise Christoffel data determines global comparison via integration.</p> <p>This offers us another way to think of $\frac{D V}{dt}$:</p> <blockquote> <p>Take \(V(t+h)\), parallel transport it back to \(T_{\alpha(t)}M\), subtract \(V(t)\), divide by $h$. Then take the limit.</p> </blockquote> <p>This is the hidden geometric definition.</p> <hr/> <h2 id="7-geodesics">7] Geodesics</h2> <p>Closely building upon the above idea of parallelism, we define a curve to be a <code class="language-plaintext highlighter-rouge">geodesic</code> if:</p> \[\frac{D\alpha'}{dt} = 0\] <p>Meaning:</p> <blockquote> <p>The velocity vector transports itself parallelly.</p> </blockquote> <p>Geometric, this means that there is no intrinsic acceleration and the curve does not “turn” inside the manifold. This generalizes straight lines.</p> <p>Another physical way to interpret this is that a geodesic has only acceleration perpendicular to the surface.</p> <p>A quick sanity check: In Euclidean space \(\mathbb{R}\), since the basis vectors then Christoffel symbols vanish.</p> <p>Consequently, \(\frac{DV}{dt} = \frac{dV}{dt}\) — Covariant derivative reduces to ordinary derivative, and this demonstrates that the definition of connection is truly a generalization.</p> <p>And if we solve</p> \[\frac{d^2\alpha^k}{dt^2} + \Gamma^k_{ij} \frac{dx^i}{dt}\alpha^j = 0\] <p>where we plug in \(V^j = \alpha^j\),</p> <p>we will arrive at a straight line, which is <strong>the straight line</strong> in Euclidean space.</p> <hr/> <h2 id="8-metric-compatibility">8] Metric Compatibility</h2> <p>There’re several ways to characterize metric compatibility. Let’s start with the following: a connection \(\nabla\) is compatible with a given metric \(&lt;,&gt;\) if</p> \[X \langle Y,Z \rangle = \langle \nabla_X Y, Z \rangle + \langle Y, \nabla_X Z \rangle\] <p>This must hold for all vector fields \(X,Y,Z,\in \mathfrak{X}(M)\). Intuitively, this represents that the derivative of the inner product equals the inner product of the derivatives. In other words, the connection differentiates vectors in a way that respects the metric. Note that here’s where we start to marry connection with metric!</p> <p>Another way to look at metric compatibility is through the lens of geodesic, which I think more closely epitomizes the essence of this concept: If a vector field \(V(t)\) along a curve satisfies \(\frac{dV}{dt} = 0\) (i.e., it’s parallelly transported), then metric compatibility implies:</p> \[\frac{d}{dt}&lt;V(t), V(t)&gt; = 0\] <p>which means that the length of the geodesic \(V(t)\) stays constant.</p> <p>More generally, if both \(V\) and \(W\) are parallel, then</p> \[\frac{d}{dt}&lt;V(t), W(t)&gt; = 0\] <p>then the angles are also preserved.</p> <p>Consequently, this is equivalent to saying that, in coordinates,</p> \[\nabla_k g_{ij} = 0\] <p>or just</p> \[\nabla g = 0\] <p>This means that the metric tensor has zero covariant derivative. Consequently, the metric looks “constant” under parallel transport. Notice that this generalizes the idea that in Euclidean space \(\partial_k​ \delta_{ij} = 0\).</p> <p>A physical analogy will be that imagine each tangent space is a tiny rigid measuring device. Metric compatibility means that when we transport a measuring stick along a curve, its length does not change. If it did change, our notion of length would depend on the path we used to move it. That would be geometrically inconsistent.</p> <p>Intuitively, metric compatibility suggests that when we move vectors around using the connection, we are not distorting the metric structure. There’s no stretching, shrinking, nor skewing of angles. Metric compatibility ensures that parallel transport acts like an orthogonal transformation between tangent spaces. If we move a basis along a curve via parallel transport, it stays orthonormal. Or in other words, parallel transport behaves like rigid motion inside each tangent space. This is perhaps the cleanest geometric picture.</p> <p>And this comes not for free. We demand it by imposing such constraint. In general, connections are arbitrary. Most connections will change the length of a vector during parallel transport or distort angles. That would mean that the connection and the metric are fighting each other. However, metric compatibility forces harmony.</p> <hr/> <h2 id="9-torsion-free-symmetry">9] Torsion-Free (Symmetry)</h2> <p>We now understand that a connection is a rule for differentiating, and metric compatibility is to ensure that we preserve lengths and angles. There’s only last remaning piece before we narrow down our infinite choices of connection.</p> <p>The formal definition of torsion-free, or symmetric, starts with the torsion tensor:</p> \[T(X,Y) = \nabla_X Y - \nabla_Y X - [X,Y]\] <p>If \(T = 0\), then</p> \[\nabla_X Y - \nabla_Y X = [X,Y]\] <p>We call such connection torsion-free. In coordinate, we would have</p> \[\Gamma^k_{ij} = \Gamma^k_{ji}\] <p>that’s why many people also call it symmetric. Notice that the above equation is straightforward since coordinate vector fields (coming from a coordinate chart) always commute (\([\frac{\partial}{\partial x^i}, \frac{\partial}{\partial x^j}] = 0\); Lie bracket has nothing to do with a connection apriori). Then just plug in \(\nabla_{X_i} X_j - \nabla_{X_j} X_i = 0\) and the above resulst should surface naturally. In short, the symmetry of Christoffel symbols comes from both torsion-free condition and the fact that coordinate vector fields commute.</p> <p>Let me state it clearly: Coordinate vector fields always commute, independent of connection. Torsion-free means the connection respects that commutation structure. The connection does not determine the Lie bracket since the Lie bracket exists before any connection is chosen. Likewise, torsion-free does not make the Lie bracket zero — it only ensures that the connection respects whatever the Lie bracket already is.</p> <p>This formula appears very bizarre, what does it mean? Recall that the Lie bracket between two vector fields \([X, Y]\) measures failure of flows to commute. If we first flow along \(X\) and then \(Y\), v.s. first \(Y\) and then \(X\), the difference is the Lie bracket: it measures intrinsic non-commutativity of directions. Now we look at \(\nabla_X Y - \nabla_Y X\). This is what the connection says about the difference between directions.</p> <p>Consequently, Torsion compares between</p> <ul> <li>What the connection thinks the commutator is, and</li> <li>What the actual geometric commutator (Lie bracket) is</li> </ul> <p>and that’s why it measures how much the connection <em>artificially twists</em> the geometry beyond the natural commutator.</p> <p>An intuitive picture is that imagine walking on a surface. Take two small steps, first in direction \(X\), and then in direction \(Y\), and compare with reversing the order. There are two possible reasons the final positions differ:</p> <ul> <li>The surface itself curves (intrinsic geometry), or</li> <li>The connection is twisting things an extra amount.</li> </ul> <p>Torsion measures the second. If torsion = 0, the connection <strong>introduces no artificial twisting</strong>. All the remaning non-commutativity comes purely from the underlying geometry itself, which is exactly what we want.</p> <p>On the other way around, if we choose a connection with torsion. Then even though \([X_i, X_j] = 0\), we would have \(\nabla_{X_i} X_j - \nabla_{X_j} X_i \neq 0\). That “extra part” would be torsion. Consequently, we could say that torsion measures deviation from symmetry beyond the intrinsic commutation.</p> <p>Why does Riemannian geometry prefer torsion-free? Since Riemannian geometry models distance and angles, there is no natural “twisting” built into length geometry: the most natural connection is metric compatible and torsion-free, which ensures that geodesics behave like natural straightest paths.</p> <p>In summary, a connection that is torsion-free does not introduce any artificial twisting beyond the natural commutation of vector fields.</p> <hr/> <h2 id="10-levi-civita-connection">10] Levi-Civita Connection</h2> <p>From the above, we know that metric compatibility controls stretching, whereas torsion-free controls twisting. Together they mean that the connection is the most natural differentiation compatible with smooth geometry and measurement.</p> <p>Indeed, now we will manually impose</p> <ol> <li>Metric compatibility</li> <li>Torsion-free</li> </ol> <p>And, remarkably, there only exists exactly one such connection, which is called the <code class="language-plaintext highlighter-rouge">Levi-Civita connection</code>.</p> <p>As demanded by its defining property it removes stretching and twisting. Only intrinsic curvature remains.</p> <p>It could be shown without too much effort that the Levi-Civita connection entirely depends on the selected metric. The Christoffel symbols of the Levi-Civita Connection are thus entirely determined by the components of the metric $g_{ij}$ and their first derivatives:</p> \[\boxed{\Gamma^k_{ij} = \frac{1}{2} g^{k\ell} \left( \frac{\partial g_{j\ell}}{\partial x^i} + \frac{\partial g_{i\ell}}{\partial x^j} - \frac{\partial g_{ij}}{\partial x^\ell} \right)}\] <p>(or if we just use the other notation: \(\Gamma^k_{ij} = \frac{1}{2}g^{k\ell} \left( X_i(g_{j\ell}) + X_j(g_{i\ell}) - X_\ell(g_{ij}) \right)\))</p> <p>How do we obtain the above equation? We just directly calculate the Christoffel symbols using the metric compatibility and torsion-free conditions. It turns out to be not as messy as you might expect:</p> <p>By definition of the connection coefficients:</p> \[\nabla_{X_i} X_j = \Gamma^k_{ij} X_k.\] <p>By torsion-free condition we will have</p> \[T(X_i,X_j) = \nabla_{X_i} X_j - \nabla_{X_j} X_i - [X_i,X_j] = 0.\] <p>Since this is a coordinate frame, \([X_i,X_j] = 0\). Torsion-free implies: \(\nabla_{X_i} X_j = \nabla_{X_j} X_i\), so \(\Gamma^k_{ij} = \Gamma^k_{ji}\). Thus the lower indices are symmetric (remember I stated this property above!)</p> <p>By metric compatibility, we would have \(\nabla g = 0\). This means that</p> \[X_i(g_{jk}) = g(\nabla_{X_i}X_j, X_k) + g(X_j,\nabla_{X_i}X_k).\] <p>Now if we substitute the definition of \(\Gamma_{ij}^l\), since \(\nabla_{X_i} X_j = \Gamma^l_{ij} X_l\), we get:</p> \[g(\nabla_{X_i}X_j, X_k) = \Gamma^\ell_{ij} g_{\ell k}\] <p>and</p> \[g(X_j,\nabla_{X_i}X_k) = \Gamma^\ell_{ik} g_{j\ell}\] <p>Thus:</p> \[X_i(g_{jk}) = \Gamma^\ell_{ij} g_{\ell k} + \Gamma^\ell_{ik} g_{j\ell}\] <p>Now comes a little tricky step: we write out the above formula but permute the indices (since they are choosen arbitrarily). We obtain:</p> <p>(1) \(X_i(g_{jk}) = \Gamma^\ell_{ij} g_{\ell k} + \Gamma^\ell_{ik} g_{j\ell}\)</p> <p>(2) \(X_j(g_{ik}) = \Gamma^\ell_{ji} g_{\ell k} + \Gamma^\ell_{jk} g_{i\ell}\)</p> <p>(3) \(X_k(g_{ij}) = \Gamma^\ell_{ki} g_{\ell j} + \Gamma^\ell_{kj} g_{i\ell}\)</p> <p>Then, using the symmetry we obtained above from torsion-free condition, since</p> \[\Gamma^\ell_{ij} = \Gamma^\ell_{ji},\] <p>compute</p> \[(1) + (2) - (3).\] <p>The feft-hand side becomes</p> \[X_i(g_{jk}) + X_j(g_{ik}) - X_k(g_{ij})\] <p>Right-hand side simplifies to:</p> \[2\,\Gamma^\ell_{ij} g_{\ell k}.\] <p>Consequently,</p> \[X_i(g_{jk}) + X_j(g_{ik}) - X_k(g_{ij}) = 2\,\Gamma^\ell_{ij} g_{\ell k}.\] <p>By rearranging terms and multiplying by the inverse metric \(g^{km}\):</p> \[g^{km} g_{\ell k} = \delta^m_\ell.\] <p>We obtain:</p> \[\Gamma^m_{ij} = \frac{1}{2} g^{mk} \left(X_i(g_{jk})+X_j(g_{ik})-X_k(g_{ij})\right).\] <p>Renaming dummy indices:</p> \[\boxed{ \Gamma^k_{ij}=\frac{1}{2}g^{k\ell}\left(X_i(g_{j\ell})+X_j(g_{i\ell})-X_\ell(g_{ij})\right)}\] <p>Remark: This formula relies crucially on the fact that the frame ${X_i}$ is a <strong>coordinate frame</strong>, meaning:</p> \[[X_i, X_j] = 0.\] <p>If instead we used a general (non-coordinate) frame where</p> \[[X_i, X_j] \neq 0,\] <p>then additional terms involving the structure constants of the frame would appear in the formula for the connection. Thus the classical Christoffel formula holds precisely because the coordinate vector fields commute.</p> <hr/> <h2 id="11-quick-summary">11] Quick Summary</h2> <ul> <li>Connection: Infinitesimal rule for comparing tangent spaces.</li> <li>Christoffel symbols: Local coefficients describing how frames twist.</li> <li>Metric compatibility: No stretching under transport.</li> <li>Torsion-free: No artificial twisting.</li> <li>Levi-Civita connection: Unique natural connection for a Riemannian metric.</li> <li>Intrinsic geometry: Independent of embedding.</li> </ul> <hr/> <h2 id="12--discussions-">12] ————— Discussions —————</h2> <p>When I first learned about connection/parallel transport/geodesics, I have tons of questions and different topics seem to mingle with one another, each defying the others’ validity. After months of delibration and study, I finally figured out the inner workings of these concepts, and I have to admit that I’m still deepening my understanding.</p> <p>I’ll not regurgitate statements and clarifications made in the previous “short summary” section 4], but instead think back on a few other critical questions to ponder. I highly recommen that readers go through 4] before reading this section.</p> <h3 id="1-why-do-connections-exist-even-without-metrics">1) Why do connections exist even without metrics?</h3> <p>I had this thought which I deemed natural and hard to wrap my head aroud: If connections are about measuring changes, and metrics measure geometry, why can a connection exist without a metric?</p> <p>Well, let’s stay back a ste and think towards what a connection does and does <em>NOT</em> do. Notice that a connection gives us a way to differentiate vector fields, to compare nearby tangent spaces. From there the notion of parallel transport and geodesics. However, notice that there’s nowhere we mentioned lengths or angles. A connection is a rule about how vectors move, not about how long they are. It only needs to satisfies linearity in \(X\) and Leibniz rule in \(Y\). That’s purely algebraic and smooth structure, and there is no inner product in the definition. So logically:</p> <blockquote> <p>Connections exist on any smooth manifold; but metrics are optional.</p> </blockquote> <p>Furthermore, <em>differentiation itself does not require a metric</em>. Think about perhaps ordinary calculus. When we compute \(\frac{df}{dx}\), we do not need an inner product. Differentiation only needs smooth structure.</p> <p>Similarly, on a manifold, to differentiate vector fields, we only need a mooth structure, a rule for comparing tangent spaces which we call/define as the connection. A metric is not logically required for this.</p> <p>A simple analogy is roads vs ruler: Imagine that the manifold is a landscape. A connection tells us how to “carry arrows” while walking, and a metric tells us how long arrows are. We can describe how an arrow rotates along a path without knowing its length.</p> <p>A concrete example here: Lie group. Take a Lie group \(G\), we can define a connection using left translation:</p> \[\nabla_XY = 0\] <p>for left-invariant vector fields. This defines parallel transport algebraically, but there’s no metric involved. This works because the group structure already lets use compare tangent spaces.</p> <p>In Riemannian geometry, we care about length-minimizing curves and curvature measured from metric, and that’s why we impose metric compatibility and torsion-free which produces the unique Levi-Civita connection. Howver, that’s a <em>specialization</em>.</p> <p>In some sense, connections are more primitive than metrics, because</p> <blockquote> <p>we can define curvature and geodesics from a connection alone.</p> </blockquote> <p>Gauge theory in physics uses connections without metrics since metrics are additional structure, whereas connections are about differentiation — a more basic concept. A mental picture I have is that</p> <ul> <li>Smooth structure \(\rightarrow\) allows calculus</li> <li>Connection \(\rightarrow\) allows differentiation of vector fields</li> <li>Metric \(\rightarrow\) allows measurement</li> </ul> <p>Each layer adds new capability and they are independent choices.</p> <h3 id="2-point-wise-definition-of-christoffel-symbols">2) Point-wise definition of Christoffel symbols?</h3> <p>Another concern that haunted me for quite a while is the following: Now hopefully we are clear that connection is entirely dependent upon the Christoffel symbols, and connection is meant to differentiate vector fields at different points. Yet based on the definition of Christoffel symbols, it seems that it’s just using the connection upon pairwise basis vectors at the same exact point. So how can something defined pointwise encode comparison between different tangent spaces?</p> <p>Well, there’re several persectives to look at this.</p> <p>Firstly, the hidden “different points” are actually there already: when we compute \(\nabla_XY\), we are not subtracting \(Y(p)\) and \(Y(q)\) directly. Instead, we are asking: If I move an infinitesimal step from \(p\) in direction \(X\), how does \(Y\) change? This “infinitesimal move” already involves neighboring points. So even though the formula is evaluated at \(p\), it contains directional information about nearby points. Moreover, Christoffel symbols tell us how the frame twists and turns as we move. Once we know how the frame moves, we can differentiate any vector field. The “comparison across points” is encoded in how the frame changes.</p> <p>Secondly, notice that a connection does not directly subtract vectors at different points. Instead, it gives an “infinitesimal rule” for how vector fields change. Indeed, a connection describes infinitesimal change. The comparison between different points comes from integrating this infinitesimal rule.</p> <p>A simple analogy in \(\mathbb{R}^n\). The derivative of a function is defined by \(f'(x)\), which is computed at a single point. Yet from this local rule, we can compare values at different points by integrating:</p> \[f(b) - f(a) = \int_a^b f'(t)\,dt\] <p>so local derivative \(\rightarrow\) global comparison. The same story here: Christoffel symbols encode infinitesimal twisting of the frame, as the definition shows:</p> \[\nabla_{X_i} X_j = \Gamma^k_{ij} X_k\] <p>This is indeed pointwise. But what does it mean? It tells us how the coordinate basis vector \(\frac{\partial}{\partial x^j}\) changes when we move infinitesimally in direction \(\frac{\partial}{\partial x^i}\). Consequently, even though the expression is evaluated at a single point, it describes <strong>variation in a direction</strong>. That direction information encodes how things change from one point to nearby points. Then that means we are integrating these infinitesimal changes?</p> <p>Exactly! Parallel transport along a curve is obtained by solving</p> \[\frac{DV}{dt} = 0\] <p>To be more concrete, if we think in coordinates, we are solving</p> \[\boxed{ \frac{dV^k}{dt} + \Gamma^k_{ij} \frac{dx^i}{dt} V^j = 0 }\] <p>for all \(k\). This is an ODE! Given initial vector \(V(0)\), the Christoffel symbols determine \(V(t)\) uniquely. Consequently, they determine how vectors are transported from one point to another.</p> <p>In summary we could think of Christoffel symbols as the “connection coefficients” describing how the local coordinate grid bends and twists. They encode infinitesimal change, from which we build parallel transport, geodesics, and later curvature. Consequently, yes indeed that Christoffel symbols are defined pointwise because connections describe infinitesimal variation, and infinitesimal variation is all we need to compare vectors at different points via integration.</p> <hr/> <p><a href="This leads to yet another interesting observation: is covariant derivative just a projection of the Euclidean derivative back onto the manifold?">TODO</a> Now our earlier question becomes clearer:</p> <p>If we define derivatives using an embedding, we are using extra structure.</p> <p>But intrinsic geometry should depend only on data defined on the manifold itself.</p> <p>That’s why the Levi-Civita connection is defined purely from the metric.</p> <p>No embedding required.</p> <p>Theoretical (sanity check) questions to ponder: 1] Why curvature is intrinsic? Does intrinsic meaning only in need of a metric? Is metric required for intrinsic geometry or curvature? 2] Why do we say that we can define curvature and geodesics from a connection alone 3] Or how geodesics make sense without embedding 4] How curvature arises from metric compatibility</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Exploration of Connection]]></summary></entry><entry><title type="html">High Dimensional Nitty-gritty (2): Z-scoring before PCA?</title><link href="https://jasmineruixiang.github.io/blog/2026/covariance/" rel="alternate" type="text/html" title="High Dimensional Nitty-gritty (2): Z-scoring before PCA?"/><published>2026-02-09T01:22:40+00:00</published><updated>2026-02-09T01:22:40+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/covariance</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/covariance/"><![CDATA[<p>This blog originates from a daily discussion of neural signal (pre)processing with my mentors and peers. People utilize z-scoring and PCA all the time, and it’s a little shameful to admit by hindsight that I haven’t dwelled on the following question deep enough. Again, we reencounter the conundrum in high dimensional observations haunted by irreducible noise, under which lies our ambitious intent to extract robust and effective information.</p> <hr/> <h3 id="0-problem-setup">0] Problem Setup</h3> <p>Let’s say we have a collection of neural data in the format \(X \in \mathbb{R}^{n \times d}\), where \(n\) is the number of samples, and \(d\) the number of features. These are raw features each coming from a single electrode. For simplicity let’s assume that there is only 1 kind of feature, like threshold crossing. Let’s further say that we want to visualize the low dimensional structure of this collection of data, preserving its global geometry as much as possible (we’ll clarify this later).</p> <p>Now we’d like to apply PCA on it for the first try, meaning to transform from \(\mathbb{R}^{n \times d}\) into \(\mathbb{R}^{n \times d'}\), where \(d'\) might be just \(3\), for example. Many methods would start by z-scoring \(X\) for each feature (so for each column of \(X\), after z-scoring would be mean 0 and standard deviation 1; for more discussions please refer to the <a href="https://jasmineruixiang.github.io/blog/2026/zscore/">previous blog</a> of this series) before applying PCA.</p> <p>I’m a little unsure about the motivation behind it. There’re of course many but I cannot pinpoint a conclusive answer.</p> <p>More importantly, I’m naively concerned that if we apply this feature-wise normalization, whether that would still preserve the covariance matrix between the original features (the diagonal will be 1, but I’m wondering how the off-diagonal terms would possibly change, or … would they).</p> <p>Let’s begin.</p> <hr/> <h3 id="1-what-pca-actually-does">1] What PCA actually does</h3> <p>The very first step: a quick review of what PCA is doing:</p> <blockquote> <p>Standard PCA:</p> <ol> <li>Center the data (subtracts column-wise/global means across samples; For the following, \(X\) will denote the centered data, if without specifications)</li> <li>Compute the <strong>covariance matrix</strong>: \(\Sigma = \frac{1}{n}X^TX\)</li> <li>Find eigenvectors of \(\Sigma\)</li> </ol> </blockquote> <p>PCA finds directions of maximal variance in the original coordinate system. We could also interpret PCA as finding minimization of reconstruction error, but that’s not explicitly helpful for deepening our interpretation here. However, I’ll provide another useful perspective in section 3] from the angle of constrained optimization, but this covariance interpretation is what we will grapple with now for this section —</p> <p>Because it already makes it obvious that</p> <blockquote> <p>PCA is sensitive to feature scale.</p> </blockquote> <p>If one electrode has variance 100 and another has variance 1, the first will dominate the principal components — even if its structure might not be more meaningful. And that’s exactly where z-scoring makes a huge distinction.</p> <hr/> <h3 id="2-what-does-z-scoring-do">2] What does z-scoring do?</h3> <p>Column-wise z-scoring transforms</p> \[\tilde{X}_{ij} = \frac{X_{ij} - \mu_j}{\sigma_j}\] <p>(Here’s a bit of abuse of notation, as \(\tilde{X}_{ij}, X_{ij}\) are scalars while \(\mu_j, \sigma_j \in \mathbb{R}^{1\times d}\))</p> <p>If we use matrix notation, \(D = diag{(\sigma_1, \cdots, \sigma_n)}\), then the above could be simplified into (again, assume it’s already centered):</p> \[\tilde{X} = XD^{-1}\] <p>Now if we look at the new covariance matrix:</p> \[\tilde{\Sigma} = \frac{1}{n}\tilde{X}^T\tilde{X} = \frac{1}{n}D^{-1}X^TXD^{-1} = D^{-1}\Sigma D^{-1}\] <p>This is obviously <strong>not the same</strong> covariance matrix.</p> <p>In fact, it’s not hard to see that, from simple linear algebra:</p> \[\tilde{\Sigma}_{ij} = \frac{\Sigma_{ij}}{\sigma_i \sigma_j}\] <p>And this turns out to be exactly the <strong>correlation matrix</strong>.</p> <p>So:</p> <blockquote> <p>PCA on z-scored data = PCA on the <code class="language-plaintext highlighter-rouge">correlation matrix</code></p> </blockquote> <blockquote> <p>PCA on raw centered data = PCA on the <code class="language-plaintext highlighter-rouge">covariance matrix</code></p> </blockquote> <p>So back to one of our original questions: does normalization preserve <code class="language-plaintext highlighter-rouge">covariance</code> between original features?</p> <p>No. The off-diagonal terms change as:</p> \[\frac{\Sigma_{ij}}{\sigma_i \sigma_j} \leftarrow \Sigma_{ij}\] <p>So they become <code class="language-plaintext highlighter-rouge">correlations</code> (or we could say that the correlations are preserved). The important distinction is that <code class="language-plaintext highlighter-rouge">covariance</code> measures co-variation in physical units, while <code class="language-plaintext highlighter-rouge">correlation</code> measures co-variation relative to each variable’s scale. So z-scoring does not preserve the original covariance geometry: It preserves the <code class="language-plaintext highlighter-rouge">correlation</code> <em>structure</em> instead.</p> <hr/> <h3 id="3-a-geometric-way-to-think-about-this">3] A geometric way to think about this</h3> <p>The above covariance calculation is clear, yet we could reinterpret PCA in a different way by variational characterization of eigenvectors, i.e. the Rayleigh quotient formulation of PCA (depending on your views, these two migth be considered the exact same thing; but even as an explanation for why we care about eigenvectors of the covariance, let me elaborate below).</p> <h4 id="31-pca-as-a-variational-problem">3.1] PCA as a Variational Problem</h4> <p>Let \(X \in \mathbb{R}^{n \times d}\) be the centered data and the sample covariance matrix</p> \[\Sigma = \frac{1}{n} X^\top X\] <p>If we project the data onto a direction \(v \in \mathbb{R}^d\), the projected variance is:</p> \[\mathrm{Var}(Xv) = \frac{1}{n} \|Xv\|^2 = v^\top \Sigma v\] <p>Therefore, the first principal component solves:</p> \[\max_{\|v\| = 1} v^\top \Sigma v\] <p>This is the Rayleigh quotient, and the solution is the top eigenvector of \(\Sigma\) (not proved here; many other sources exist online).</p> <h4 id="32-pca-after-z-scoring">3.2] PCA After Z-Scoring</h4> <p>Suppose we z-score each feature (column-wise normalization). Again, let me reiterate from the above that</p> \[D = \mathrm{diag}(\sigma_1, \dots, \sigma_d)\] <p>and the transformed data is:</p> \[\tilde{X} = X D^{-1}\] <p>The new covariance matrix becomes:</p> \[\tilde{\Sigma} = \frac{1}{n} \tilde{X}^\top \tilde{X} = D^{-1} \Sigma D^{-1}\] <p>PCA on z-scored data solves:</p> \[\max_{\|v\| = 1} v^\top D^{-1} \Sigma D^{-1} v\] <hr/> <h4 id="33-equivalent-reformulation-changing-the-constraint">3.3] Equivalent Reformulation (Changing the Constraint)</h4> <p>Now let’s do a simple trick. Let:</p> \[w = D^{-1} v \quad \text{so that} \quad v = D w\] <p>Substitute into the objective:</p> \[v^\top D^{-1} \Sigma D^{-1} v = (Dw)^\top D^{-1} \Sigma D^{-1} (Dw) = w^\top \Sigma w\] <p>Now examine the constraint:</p> \[\|v\|^2 = 1\] \[v^\top v = (Dw)^\top (Dw) = w^\top D^2 w\] <p>So the optimization becomes:</p> \[\max_{w^\top D^2 w = 1} w^\top \Sigma w\] <hr/> <h4 id="34-geometric-interpretation">3.4] Geometric Interpretation</h4> <p>The raw PCA solves:</p> \[\max_{v^\top v = 1} v^\top \Sigma v\] <p>Now, the z-scored PCA solves:</p> \[\max_{w^\top D^2 w = 1} w^\top \Sigma w\] <p>So at a glance from 2], z-scoring rescales the covariance matrix into the correlation matrix.</p> <p>But viewed from this different perspective, it <strong>changes the metric constraint</strong>. Instead of using the standard Euclidean norm:</p> \[v^\top v\] <p>we now use a weighted norm:</p> \[w^\top D^2 w\] <p>This means:</p> <ul> <li>Raw PCA assumes the <strong>standard Euclidean</strong> inner product.</li> <li>Z-scored PCA uses a different inner product <strong>induced by \(D^2\)</strong>.</li> </ul> <hr/> <h4 id="35-multi-dimensional-pca-k-components">3.5] Multi-Dimensional PCA (k Components)</h4> <p>Up to this point, you might object that the above is just to find one single direction/one principal component. Usually we do multiple components. Well, there isn’t too much effort for an extension.</p> <p>Raw PCA solves:</p> \[\max_{V^\top V = I} \mathrm{Tr}(V^\top \Sigma V)\] <p>where \(V \in \mathbb{R}^{d \times k}\). The solution is the top-\(k\) eigenvectors of \(\Sigma\) (proof omitted, similar as 3.1]). As in 3.2], After z-scoring, we solve:</p> \[\max_{V^\top V = I} \mathrm{Tr}(V^\top D^{-1} \Sigma D^{-1} V)\] <p>Using the substitution \(V = D W\), this naturally becomes (similar to 3.3]):</p> \[\max_{W^\top D^2 W = I} \mathrm{Tr}(W^\top \Sigma W)\] <hr/> <h4 id="36-generalized-eigenvalue-interpretation">3.6] Generalized Eigenvalue Interpretation</h4> <p>What does this mean geometrically about the solution?</p> <ul> <li>For the raw PCA: <ul> <li>Orthonormal basis in standard <strong>Euclidean</strong> metric</li> <li>Maximizes variance</li> </ul> </li> <li>For Z-scored PCA: <ul> <li>Orthonormal basis under <strong>weighted</strong> metric \(D^2\)</li> <li>This is equivalent to solving a <strong>generalized eigenvalue</strong> problem:</li> </ul> </li> </ul> \[\Sigma w = \lambda D^2 w\] <p>I’ll also skip the details of why the solution is equivalent to finding the generalized eigenvalues/eigenvectors. However, this fact informs us of the fundamental framework of PCA:</p> <p>the big geometric insight is that PCA always solves:</p> \[\max_{W^\top G W = I} \mathrm{Tr}(W^\top \Sigma W)\] <p>where \(G\) defines the metric.</p> <ul> <li>Raw PCA: \(G = I\)</li> <li>Z-scored PCA: \(G = D^2\)</li> </ul> <p>So z-scoring means that we are not trusting that Euclidean length in raw coordinates is meaningful. We redefine what unit length means.</p> <p>Which is consistent with the previous observation that</p> <ul> <li>Raw PCA preserves covariance geometry.</li> <li>Z-scored PCA preserves correlation geometry.</li> </ul> <hr/> <h3 id="4-what-geometry-are-we-preserving">4] What geometry are we preserving?</h3> <p>To some extent, this is the real conceptual issue.</p> <p>If we do PCA without z-scoring, it preserves the Euclidean geometry in the original feature space. Variance magnitude is meaningful because we indeed keep such information. We’d hold the underlying premise that</p> <blockquote> <p>Electrodes with larger variance are considered more important.</p> </blockquote> <p>This suggests that this methood is good if variance magnitude reflects real neural signal strength or the feature scale is physically meaningful.</p> <p>On the other hand, if we do PCA after z-scoring, then it would preserve geometry under a <strong>reweighted metric</strong> and all dimensions are treated equally. Each electrode is thus given equal prior importance.</p> <p>This should work if variance differences are arbitrary (e.g., electrode gain differences) and we care about patterns of co-variation, not absolute magnitude.</p> <p>Since neural data often has:</p> <ul> <li>Different firing rates across electrodes</li> <li>Different noise levels</li> <li>Different dynamic ranges</li> </ul> <p>If we don’t z-score:</p> <blockquote> <p>High firing-rate neurons dominate PCA.</p> </blockquote> <p>whereas if we do z-score:</p> <blockquote> <p>Each neuron contributes equally in variance units.</p> </blockquote> <hr/> <h3 id="5-which-one-preserves-global-geometry">5] Which one preserves global geometry?</h3> <p>This depends on what geometry we think is meaningful.</p> <p>If our raw space is: \(\mathbb{R}^d\) with standard Euclidean metric, then PCA without z-scoring preserves global geometry better. If we believe that true geometry should not depend on firing rate scale, then z-scoring defines a more appropriate metric (as elaborated in section 3]):</p> \[&lt;x, y&gt;_D = x^T(D^{-1})^{2}y\] <p>which means we could alternatively interpret this as keeping the underlying space unchanged but essentially altering the metric before doing PCA.</p> <p>Usually in systems neuroscience people often z-score across time and then do PCA. The reason behind is that neural manifold studies often care about relative population patterns, not which neuron fires more. If we want true population variance magnitude, then we should not z-score. If we intend to obtain population structure independent of scale, then z-score.</p> <hr/> <h3 id="6-summary">6] Summary</h3> <p>In conclusion, if we do z-scoring before PCA, then</p> <ul> <li>Z-scoring does NOT preserve the <code class="language-plaintext highlighter-rouge">covariance</code> matrix.</li> <li>It converts <code class="language-plaintext highlighter-rouge">covariance</code> to <code class="language-plaintext highlighter-rouge">correlation</code>.</li> <li>Off-diagonal terms are divided by product of standard deviations.</li> <li>Equivalently, we are changing the <strong>metric</strong> of the space.</li> <li>PCA result can change dramatically depending on scaling.</li> </ul> <p>Finally, in practice we often have more than one kind of features. For example, we could obtain both threshold crossings and spike band power from each electrode at the same time. However, these two measures have drastically different scales. In this scenario, of course we could look into them separately, but if combined, the spike power would dominate. Consequently, z-scoring also helps to re-weight the feature importance apriori.</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Brain Computer Interface"/><summary type="html"><![CDATA[Zscoring, covariance, PCA]]></summary></entry><entry><title type="html">The Dance of Space: Geom/Topo/Dynam Mumble(5)</title><link href="https://jasmineruixiang.github.io/blog/2026/geodesics/" rel="alternate" type="text/html" title="The Dance of Space: Geom/Topo/Dynam Mumble(5)"/><published>2026-02-08T14:58:02+00:00</published><updated>2026-02-08T14:58:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/geodesics</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/geodesics/"><![CDATA[<p>This weekend I was (re)thinking about the geometric connotations of geodesics, and that reminds me of a brilliant illustration with an intuitive method the eloquent mathematician Tristan Needham noted down in his remarkable and mind-numbing book <em>Visual Differential Geometry and Forms</em>. I thought just a little more, and only to recover a long-lasting misconception about Gauss Theorem Egregium which I was both shameful to admit for possessing so long and excited to have it cleared out of my mind.</p> <p>However, as I dive more into Needham’s method, I discovered a potential and simple failure mode, which naturally echoes another important statement: The Gauss-Bonnet Theorem. This deepens my grasp of the tension between <strong>local isometry</strong> (the peeling method, see below) and <strong>global topology</strong> (the area enclosed).</p> <p>Again, since this is the mumble series, I’ll not give all definitions and assume you have already known some flavor of the basics. Let’s begin.</p> <hr/> <h2 id="0-the-problem-confusion-setup">0] The Problem (Confusion) Setup</h2> <p>On Page 12-13, Tristan introduced a method to easily and intuitively construct geodesics on a curved surface (see figure [1.11] below). He claimed that “If a narrow strip surrounding a segment G of a geodesic is cut out of a surface and laid flat in the plane, then G becomes a segment of a straight line.” Well, he presented an intuitive proof (figure [1.12] below), but I’m immediately reminded of <strong>Gauss’ Theorem Egregium</strong> and there seemed to be something inconsistent (I’ll omit other information and count on you to look up the basics).</p> <p>I was thinking: the peeling is an (local) isometry without doubt, so geodesics have to be preserved. That’s why geodesics “peeled” off from the surface (the fruit/vegetable used in the illustration) has to be geodesics in Eucliean 2D space, which is straight according to the normal definitions. However, as I’m thinking a bit more deeply, the Gaussian curvature does change (before it’s nonzero, on 2D it’s zero, which contradicts Theorem Egregium), which leads to me to think backwards towards using this Theorem Egregium again. Immediately I realize that Gaussian theorem egregium is applied only to 2D surfaces (not to 1D curve since there’s no “Gaussian curvature” for a curve). Ah, shame to have blundered upon conceptual confusion. But treating this as an opportunity for an upgrade overhaul of my conceptual framework, let’s sort this out step by step and see what we might also dabble into.</p> <div class="row mt-3"> <div class="col-sm-6 mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/VDGF/VDGF_1.11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/VDGF/VDGF_1.12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig [1.11] and [1.12] in <a class="citation" href="#VDGF">(Needham, 2021)</a>. </div> <h2 id="1-the-peeling-intuition">1] The “Peeling” Intuition</h2> <p>Let’s make it clear: in Figure [1.11] of Needham’s text, he introduces the following powerful intuitive tool:</p> <blockquote> <p>“If a narrow strip surrounding a segment $G$ of a geodesic is cut out of a surface and laid flat in the plane, then $G$ becomes a segment of a straight line.”</p> </blockquote> <p>At first glance, this indeed seems to challenge Gauss’s <strong>Theorema Egregium</strong>. Since Gaussian curvature (\(K\)) is an intrinsic invariant, how can a patch of a curved surface (\(K \neq 0\)) be laid flat (\(K = 0\)) without stretching or tearing?</p> <p>Well, the resolution lies in the <strong>limit</strong>. Gaussian curvature is a property of a 2D area. By taking a “narrow strip,” we are effectively reducing the 2D surface to a 1D curve and its immediate neighborhood. In the limit, as the width of the strip goes to zero, the “area” of the surface being considered goes to zero. In other words, the Gaussian curvature of the surface does not change because we aren’t flattening the whole surface; we are only flattening an “infinitesimally” narrow strip.</p> <hr/> <h2 id="2-geodesic-curvature-vs-gaussian-curvature">2] Geodesic Curvature vs. Gaussian Curvature</h2> <p>The Theorema Egregium applies to <strong>isometries</strong>. While we cannot map a patch of a sphere to a plane isometrically, we can map a curve and its first-order neighborhood to a plane isometrically (often called “developing” the strip; or just developable).</p> <p>We could think of it this way:</p> <blockquote> <p>The Surface: Has intrinsic curvature \(K\). The Strip: Because it is “infinitesimally narrow,” the Gaussian curvature doesn’t “<strong>trap</strong>” the strip. We aren’t forcing the 2D relationships across the strip to remain the same; we are only preserving the lengths along the geodesic \(G\).</p> </blockquote> <p>Consequently, the core of the questions (why such an intuition makes sense, beyond the simple proof shown in Fig.[1.12]) does not lie in Gauss’s Theorem, but in another concept: the geodesic curvature: The Theorema Egregium is about the surface (\(K\)), but the property of being a “straight line” is about Geodesic Curvature (\(\kappa_g\)):</p> <blockquote> <p>Gaussian Curvature (\(K\)) reflects the property of the surface. Geodesic Curvature (\(\kappa_g\)): A property of a curve relative to the surface. It measures how much the curve bends within the surface.</p> </blockquote> <p>The “straightness” of the peeled strip is governed by <strong>Geodesic Curvature ($\kappa_g$)</strong>, not Gaussian Curvature (\(K\)).</p> <ul> <li><strong>Gaussian Curvature (\(K\)):</strong> A property of the surface itself (\(K = \kappa_1 \kappa_2\)). It dictates whether a 2D patch can be flattened.</li> <li><strong>Geodesic Curvature (\(\kappa_g\)):</strong> A property of a curve <em>relative</em> to the surface. It measures how much the curve “veers” to the left or right within the surface.</li> </ul> <p>Recall that a geodesic is defined as a curve where \(\kappa_g = 0\) everywhere (Will do a another blog on the computation of geodesics from the perspective of modern (Riemannian) manifold). Because \(\kappa_g\) is an intrinsic property (it can be measured by flatlanders like intelligent ants living on the surface), it must be preserved under isometry.</p> <p>With this, we could demystify the underpinning logic of the peel:</p> <ol> <li>The act of peeling the strip is a local isometry along the curve.</li> <li>The \(\kappa_g\) of the curve on the surface was 0 (by definition of a geodesic).</li> <li>Therefore, the \(\kappa_g\) of the curve on the flat plane must also be 0 .</li> <li>In the Euclidean plane, a curve with zero curvature is a straight line.</li> </ol> <p>Notice that the 3rd step above relies on the fact that isometry preserves intrinsic properties and here the geodesic curvature, but again this is NOT Gauss Theorem Egregium (same fundamental fact but applied to different objects).</p> <p>Gaussian curvature is not defined for a 1D curve. A curve only has curvature (how much it bends in space) and geodesic curvature (how much it bends relative to the surface it’s on). By narrowing the strip until the 2D “surface” nature of the paper effectively vanishes, we are bypassing the restriction of the Theorema Egregium regarding the 2D area, while retaining the intrinsic measurement of the curve’s straightness. We are essentially “cheating” the theorem by reducing the 2D surface to a 1D line where the concept of Gaussian Curvature has no grip.</p> <p>The Theorem Egregium is a negative constraint (it tells us what we can’t do with a 2D patch), but the Geodesic Curvature (\(\kappa_g\)) is the positive proof. It yet emphasizes again that it’s important to separate the topological “budget” of the surface (the Gaussian Curvature) from the local behavior of the curve (the Geodesic Curvature).</p> <p>There’s only one last piece left to make the story rigorous.</p> <hr/> <h2 id="3-physical-approximation">3] Physical Approximation</h2> <p>Let me paraphrase again in another way. This narrow strip, in practice, since it cannot be infinitesimally narrow, by Theorem Egregium, <strong>CANNOT</strong> lie flat on the plane, but the more narrow we could get, the better aligned it is with respect to the plane. If it goes to limit, then Gaussian Theorem Egregium does not apply because this limit 1D curve is no longer constrained by this theorem.</p> <p>On the other hand, even in practice we assume that after peeling it “is” flat, this appeal to Theorem Egregium still does not “prove” the fact that once laid down the strip would become straight; we still need to assort to the preserved geodesic curvature, which is covered above.</p> <p>Now, back to the reality. In practice, a strip of finite width \(w\) cannot lie perfectly flat if \(K \neq 0\). The “error” or distortion required to flatten it is proportional to the area of the strip (\(Area \approx Length \times w\)). As \(w \to 0\):</p> <ul> <li>The area vanishes.</li> <li>The constraint of the Theorema Egregium vanishes.</li> <li>The 1D “straightness” remains perfectly preserved.</li> </ul> <p>Another way to view Needham’s trick is that the strip isn’t just being “flattened”; it is being identified with the unrolled <strong>tangent developable</strong> of the geodesic. If we imagine a sequence of tangent planes along the geodesic, they form a “ribbon” that has zero Gaussian curvature (because it’s a developable surface, like a cylinder or a cone). Because this ribbon has \(K=0\), it can be laid perfectly flat in a plane without any distortion at all. Needham’s “narrow strip” is essentially a physical approximation of this developable ribbon.</p> <hr/> <h2 id="4-extension-of-the-original-trick-connection-to-gauss-bonnet">4] Extension of the original trick: Connection to Gauss-Bonnet</h2> <h3 id="41-failure-mode-of-a-strip-when-in-closed-loop">4.1] Failure mode of a strip when in closed loop</h3> <p>Now with the above puzzle cleared out, let’s think a little deeper into the next single, perhaps most natural topic: How about peeling off not just a single strip segment, but instead a strip which loops back into itself?</p> <p>Think about the physical reality of Needham’s experiment. If we cut a strip along a great circle (a closed geodesic) of a sphere:</p> <ul> <li>1] The Segment: If we cut just a small arc (say 30°), the strip is essentially a tiny rectangle. Because it’s so narrow, the “surface tension” of the sphere’s curvature isn’t strong enough to prevent us from pressing it flat.</li> <li>2] The Loop: If we try to cut the entire great circle, we get a “ring” or a “hoop.” On the sphere, this hoop has a specific circumference (\(C = 2\pi R\)).</li> <li>3] The Failure: If we try to lay that hoop perfectly flat on a table without stretching it, we’ll find it impossible. To lie flat in Euclidean space as a circle, the relationship between its radius and circumference must be \(C = 2\pi r\). But on the sphere, the “radius” (the distance from the pole to the equator) is an arc length. <strong>The geometry of the 2D area inside the hoop “locks” the hoop’s shape</strong>.</li> </ul> <p>Or maybe another perhaps more intuitive example —</p> <blockquote> <p>The “Paper Cone” Analogy:</p> </blockquote> <p>Think of a paper cone (like a party hat).</p> <ul> <li> <p>1] Local Flattening: You can cut a narrow strip from the cone running from the tip to the base. You can lay this strip flat on the table perfectly. In fact, you can lay any part of the cone flat.</p> </li> <li> <p>2] Global Failure: But if you try to flatten the <em>entire</em> cone at once, you can’t. You have to make a cut. When you flatten it, the cut edges don’t meet; there is a angular gap.</p> </li> </ul> <p>Yet perhaps another thought experiment:</p> <blockquote> <p>The “Train Track” Experiment</p> </blockquote> <p>Imagine the “narrow strip” as a set of flexible but straight train tracks.</p> <p>On the Sphere: You lay the tracks along the equator. They go all the way around and connect perfectly at the start.</p> <p>The “Peeling” (Transfer to Plane): Now you transfer these tracks to a flat Euclidean floor. Because the tracks are geodesics (straight), you must lay them down as a straight line on the floor. You keep laying them down, inch by inch.</p> <p>Now the Problem: On the floor, a straight line goes on forever. It never comes back to start.</p> <p>The Contradiction: To make the tracks close a loop on the floor, you would have to bend them (add Geodesic Curvature). But we know geodesics are straight!</p> <p>So, the “narrow strip” of a closed geodesic loop on a sphere becomes an infinite straight line on the plane. It loses its “loop-ness” entirely.</p> <p>Later we will make it clear that this “angular gap” is precisely what the Gauss-Bonnet Theorem calculates (\(\iint K dA\)). The curvature \(K\) inside the loop on the sphere is responsible for “turning” the geometry so that it closes. The flat plane (\(K=0\)) lacks this “turning power,” so the strip simply runs away in a straight line.</p> <p>But anyway, for now, in short: We can flatten a line because a line has no “inside.” We cannot flatten a closed loop without accounting for the gap (which is what we will show later as the holonomy, or “equivalently” integration of the Gaussian curvature of the area it encloses.)</p> <hr/> <h3 id="42-the-formula-and-geodesic-loop">4.2] The formula and geodesic-loop</h3> <p>We resort to The Gauss-Bonnet Theorem, which almost fits in immediately, since it bridges the gap between the local straightness of the geodesic and the global curvature of the surface.</p> <p>Well, the Gauss-Bonnet Theorem is essentially a “budgeting” equation. It tells us exactly how much “straightness” we have to give up to account for the curvature of the surface. Let’s see if we could intuitively see its effect from the closed loop peeling failure.</p> <p>Let me state the theorem:</p> <p>For a simply connected region \(R\) bounded by a curve \(C\), the theorem states:</p> \[\iint_R K \, dA + \oint_C \kappa_g \, ds + \sum \alpha_i = 2\pi\] <p>\(K\) is the Gaussian curvature, \(\kappa_g\) is the geodesic curvature, \(\alpha_i\) are the exterior angles at any corners.</p> <p>Moreover, if we create a closed loop (like a triangle) using only geodesic segments, then \(\kappa_g = 0\) along the edges by definition. The middle term of the equation vanishes, leaving a direct relationship between the “area-integral of curvature” and how much the geodesics had to “turn” at the corners to close the loop:</p> \[\iint_R K \, dA = 2\pi - \sum \alpha_i\] <p>Furthermore, if we use a smooth closed geodesic, there is no geodesic curvature (\(\kappa_g = 0\)) and also no corners (\(\sum \alpha_i = 0\)). The equation thus becomes:</p> \[\iint_R K \, dA = 2\pi\] <p>This tells us that for a smooth closed geodesic to exist, the total Gaussian curvature of the area it encloses must equal \(2\pi\) (the “angle” of a full circle). This is why we can have a closed geodesic on a sphere (\(K &gt; 0\)), but we can never have a simple closed geodesic on a flat plane (\(K=0\)) or a saddle (\(K&lt;0\))—the “curvature budget” doesn’t add up to \(2\pi\).</p> <p>Let us dwindle here for a while, as I feel like building up intuition, a feeling of this curvature budge is of significant importance. So let’s just try again to reinterpret this equation by the following simple example.</p> <p>To walk in a simple closed loop (a circle, a square, a blob) and end up facing the same way we started, we must physically turn a total of 360° (\(2\pi\) radians).</p> <p>However, here the Gauss-Bonnet theorem says there are two ways to pay for this 360° budget:</p> <blockquote> <ol> <li>Steering (\(\kappa_g\)): we physically turn our body (like turning a steering wheel).</li> <li>Surface Curvature (\(K\)): The ground itself curves underneath us, effectively “turning” us without we realizing it.</li> </ol> </blockquote> <p>The equation is thus:</p> <p>\(\text{Steering} + \text{Surface Curvature} = 360^\circ\) \(\oint \kappa_g ds + \iint K dA = 2\pi\)</p> <p>Note that a geodesic is defined as a path where we do not steer. Our steering wheel is locked in the straight-ahead position.</p> <p>Therefore: Steering = 0.</p> <p>Now look at our budget equation again:</p> <p>\(0 + \text{Surface Curvature} = 360^\circ\) \(\iint K dA = 2\pi\).</p> <p>This means the only way to close a loop without steering is if the surface itself does 100% of the turning for us.</p> <p>Consequently, this would explain why it fails on the plane (\(K = 0\)). The surface or the plane is flat, which contributes 0° of turning. The geodesic contributes 0° of steering. Hence the result: \(0 + 0 = 0\). We have turned 0° and we are walking in a straight line forever. We will never close the loop. Thus our conclusion: simple closed geodesics cannot exist on a plane.</p> <p>Similarly, simple closed geodesics cannot exist on a saddle, because a saddle has negative curvature. It curves “away” from itself and contributes negative turning. Along a geodesic there’s no contribution of steering, and the net result is still negative, not \(2\pi\). We are actually diverging away from a closed loop. The surface is actively pushing us path apart.</p> <p>In contrast, a sphere has positive curvature. It curves “inward” toward itself and contributes positive turning. If we enclose enough area (specifically, a hemisphere), the total positive curvature adds up to exactly \(2\pi\). The surface has bent us around exactly enough to meet our own tail without us ever turning the wheel.</p> <p>In some sense, the “Curvature Budget” is like a tax. To close a loop, we must pay \(2\pi\). On a Geodesic, we refuse to pay (we won’t steer). Therefore, the Landlord (Surface) must pay the entire tax for us. Only a positively curved landlord (Sphere) has the cash (\(K&gt;0\)) to pay it. The plane is broke (\(K=0\)), and the saddle is in debt ($K&lt;0$).</p> <p>Well, you may wonder in the above interpretation where do the exterior angle sum go. Recall that we’re talking about smooth geodesic loops. Because the loop is smooth, there are no sharp corners (“kinks”), so there are no exterior angles to sum up. The term \(\sum \alpha_i\) becomes exactly 0. You may also wonder, then how would geodesic “turn” be different from the exterior angle kink turn? This is a little more subtle, but also intuitive.</p> <blockquote> <p>The “Conservation of Turning”</p> <ul> <li>Usually, when we smooth out a shape (like turning a square into a circle), we don’t lose the turning angles; we just spread them out.</li> <li>Square (Polygon): We walk straight (\(\kappa_g=0\)) and make four sharp \(90^{\circ}\) turns.</li> </ul> </blockquote> <ul> <li>\(\text{Turning} = \sum \alpha_i = 360^{\circ}\). I’d more think about these sharp turns/kinks as some colossal mechanical arm sticking from space and grabs our car to turn it around certain angles at the current position.</li> <li> <p>Circle (Smooth Curve): We never make a sharp turn (\(\alpha_i=0\)), but we are constantly steering a tiny bit to the side (\(\kappa_g = \text{constant}\)).</p> </li> <li>\(\text{Turning} = \oint \kappa_g \, ds = 360^{\circ}\).</li> </ul> <p>In both cases, we provided the turning. However, for the geodesic “miracle”, a smooth closed geodesic is a bizarre object because it refuses to turn in either way: there’s no sharp corners (smooth, \(\sum \alpha_i = 0\)) and no steering (geodesic, \(\kappa_g = 0\)). So where does the mandatory \(360^{\circ}\) (\(2\pi\)) turning come from to close the loop? It must come entirely from the Gaussian Curvature (\(K\)) of the area we enclosed. The surface itself has to rotate the universe under our feet by exactly \(360^{\circ}\) while we walk in a “straight” line.</p> <hr/> <h3 id="43-how-does-the-strip-fit-in">4.3] How does the “strip” fit in?</h3> <p>Now, imagine applying Needham’s “peeling” trick to each side of a geodesic triangle.</p> <blockquote> <ol> <li>On the surface: We would have three “straight” paths (geodesics) that enclose a region of curvature \(K\). Because of that \(K\), the interior angles sum to more than \(\pi\) (on a sphere).</li> <li><strong>The “Peeling” Conflict</strong>: If we tried to peel a “narrow strip” that followed the entire boundary of the triangle and lay it flat in one go, we would encounter a physical gap or an overlap where the ends meet.</li> </ol> </blockquote> <p>The Gauss-Bonnet Theorem effectively measures this “gap.” The amount of Gaussian curvature “trapped” inside the triangle is exactly equal to the “holonomy”—the amount a vector rotates when transported around that loop:</p> <hr/> <h3 id="44-holonomy-parallel-transport-and-the-gap">4.4] Holonomy, Parallel Transport, and the “Gap”</h3> <p>There’s a simple and intuitive relationship between holonomy, parallel transport, and exterior angles.</p> <p>Let’s start with a simple thought experiment about parallel transport (you might have seen this everywhere):</p> <p>Imagine walking along a geodesic triangle on a sphere, carrying a spear (a vector) pointing straight ahead.</p> <ul> <li>Along the edge: Because we are on a geodesic, we never turn our “steering wheel.” The spear stays parallel to our path.</li> <li>At the corner: we stop and turn our body by an exterior angle (\(\alpha_i\)). We do not turn the spear; it still points where it was pointing.</li> <li>Back at the start: When we complete the loop, we compare the spear’s current direction to its starting direction. They won’t match. This net rotation of the vector after the full trip, the “error” in direction, is called the <strong>Holonomy (\(\Delta \theta\))</strong>.</li> </ul> <p>The above way of sliding a vector along a geodesic while keeping it “parallel” is called <strong>Parallel Transport</strong>.</p> <p>At the same time, just by some simple calculation we would know that for the total turn on a flat plane, our total change in heading (sum of exterior angles \(\sum \alpha_i\)) must be \(2\pi\) <em>to close a loop</em>. In other words, the holonomy \(\Delta \theta = 0\). However, on a curved surface, the amount we actually turned, the holonomy, is \(2\pi - \sum \alpha_i \neq 0\).</p> <p>Also notice that if we do parallel transport along the “peeled” flat strip, the vector remains parallel in the Euclidean sense because the strip is a straight line.</p> <p>However, when we close the loop,</p> <ul> <li>On the flat plane, the vector would return to its start pointing in the original direction.</li> <li>On the curved surface, the vector returns rotated by an angle \(\Delta \theta\).</li> </ul> <p>The theorem tells us that this holonomy \(\Delta \theta\) is precisely the integral of the Gaussian curvature over the area we bypassed:</p> \[\Delta \theta = 2\pi - \sum\alpha_i = \iint_R K \, dA\] <p>Intuition Check: If you are on a flat plane, \(K=0\). Therefore, \(\iint K dA = 0\). This means \(0 = 2\pi - \sum \alpha_i\), or \(\sum \alpha_i = 2\pi\). This is just some high-school geometry rule that the exterior angles of any polygon sum to 360°. On a sphere, the curvature \(K\) “helps” us turn, so we don’t need as much “exterior angle” to close the loop.</p> <p>Consequently, if we apply Needham’s trick, i.e., peel off the strip of geodesic loop, we would find that</p> <blockquote> <p>While we can peel a <strong>single</strong> geodesic segment and lay it <strong>flat</strong> perfectly, we <strong>cannot</strong> peel a <strong>closed loop</strong> of geodesics and lay the resulting “frame” flat in the plane without a gap or overlap.</p> </blockquote> <p>The “angle” of that gap is the <strong>holonomy</strong>. The Gauss-Bonnet theorem tells us that this gap is exactly equal to the total Gaussian curvature “trapped” inside the area we just cut out.</p> <ul> <li><strong>On a Sphere (\(K&gt;0\)):</strong> The geodesics turn “toward” each other, and the interior angles sum to \(&gt;\pi\).</li> <li><strong>On a Saddle (\(K&lt;0\)):</strong> The geodesics flare “away” from each other, and the interior angles sum to \(&lt;\pi\).</li> </ul> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Needham’s trick works because a 1D line has no “area” to trap curvature. The “straightness” we see on the paper is the physical manifestation of zero geodesic curvature, an intrinsic property that survives the transition from the fruit’s skin to the flat desk.</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Manifold"/><category term="Differential Geometry"/><summary type="html"><![CDATA[Geodesic and its construction, Gauss Theorem Egregium, Gauss-Bonnet and Chern]]></summary></entry><entry><title type="html">High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring</title><link href="https://jasmineruixiang.github.io/blog/2026/zscore/" rel="alternate" type="text/html" title="High Dimensional Nitty-gritty (1): Equivalence (or Lack thereof) between Block-wise and Global Z-scoring"/><published>2026-02-06T11:16:09+00:00</published><updated>2026-02-06T11:16:09+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/zscore</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/zscore/"><![CDATA[<p>This short blog provides a detailed, self-contained computational analysis of whether two z-scoring procedures applied to block-structured neural data are equivalent, and if indeed different how.</p> <hr/> <h2 id="1-problem-setup">1] Problem setup</h2> <p>Let’s say that we have neural data organized into blocks:</p> <ul> <li>Number of blocks: \(B\)</li> <li>Each block has data matrix in shape: \((x_{i, j}^b) \in \mathbb{R}^{t \times n}, \quad b = 1, \dots, B\)</li> <li>\(t\) = number of time points (samples)</li> <li>\(n\) = number of neural features</li> </ul> <p>Z-scoring is performed <strong>feature-wise</strong>, so all derivations below consider <strong>one fixed feature</strong> (column) at a time. The argument applies independently to every feature.</p> <hr/> <h2 id="2-notation-for-a-single-feature">2] Notation for a single feature</h2> <p>For a fixed feature \(j\):</p> <ul> <li> <p>Let \(x_{i,j}^{b} \in \mathbb{R}^{1}\) denote the data at time \(i\) for the feature \(j\) in block \(b\). For the following paragraphs, I will simplify \(x_{:,j}^{b}\) into \(x_{j}^{b} \in \mathbb{R}^{t\times 1}\), and \(x_{i,:}^{b}\) into \(x_{i}^{b} \in \mathbb{R}^{1\times n}\). Naturally, \(x^b = (x_{ij}^b) \in \mathbb{R}^{t\times n}\) with the same shape for all blocks. Basically, \(i\) corresponds to the index of time \(t\), and \(j\) the index of the number of neurons \(n\).</p> </li> <li> <p>Block-wise mean: \(\mu_{j}^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i, j} \in \mathbb{R}^{1}\),</p> <p>\(\mu ^{b} = [\mu_{1}^{b}, \cdots, \mu_{n}^{b}] \in \mathbb{R}^{1 \times n}\).</p> <p>Or, we could simply write \(\mu ^{b} = \frac{1}{t}\sum_{i=1}^t x^{b}_{i} \in \mathbb{R}^{1 \times n}\)</p> </li> <li> <p>Block-wise variance: \((\sigma_{j}^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i, j} - \mu_{j}^b)^2 \in \mathbb{R}^{1}\),</p> <p>\((\sigma ^{b})^2 = [(\sigma_{1}^{b})^2, \cdots, (\sigma_{n}^{b})^2] \in \mathbb{R}^{1 \times n}\),</p> <p>Or, we could simplify the above as \((\sigma ^{b})^2 = \frac{1}{t}\sum_{i=1}^t (x^{b}_{i} - \mu^b) \odot (x^{b}_{i} - \mu^b) \in \mathbb{R}^{1\times n}\)</p> <p>where \(\odot\) is the Hadamard product between two vectors (or just elementwise multiplication), defined as \(x \odot y = diag(x)y = (x_i y_i)_i \in \mathbb{R}^{1\times n},\) where \(x,y \in \mathbb{R}^{1\times n}\)</p> </li> </ul> <hr/> <h2 id="3-method-a-block-wise-z-scoring-concatenate-and-then-global-z-scoring">3] Method A: Block-wise z-scoring, concatenate, and then global z-scoring</h2> <h3 id="step-3a-z-score-within-each-block">Step 3a]: Z-score within each block</h3> <p>Each block is normalized independently:</p> \[z^{b}_{i} = \frac{x^{b}_i - \mu^b}{\sigma^b} \in \mathbb{R}^{1\times n}\] <p>Notice that this is element-wise division (to not over-complicate the symbols, I’ll use this abuse of notation for the following).</p> <p>By construction, for every block \(b\):</p> \[\frac{1}{t}\sum_{i=1}^t z^{b}_{i} = \vec{0} \in \mathbb{R}^{1\times n}, \qquad \frac{1}{t}\sum_{i=1}^t (z^{b}_{i} - \vec{0}) \odot (z^{b}_{i} - \vec{0}) = \vec{1} \in \mathbb{R}^{1\times n},\] <p>Consequently, each feature in each block has mean 0 and standard deviation 1. \(z^b\) observes the same notation rule as I described above for \(x^b\).</p> <hr/> <h3 id="step-3b-concatenate-all-normalized-blocks">Step 3b]: Concatenate all normalized blocks</h3> <p>Concatenate all \(z^{b}\) into a single vector of length \(Bt\).</p> <h4 id="global-mean">Global mean</h4> \[\frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t z^{b}_i = \frac{1}{B}\sum_{b=1}^B \vec{0} = \vec{0} \in \mathbb{R}^{1\times n}\] <h4 id="global-variance">Global variance</h4> \[\frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) \\ = \frac{1}{B}\sum_{b=1}^B\frac{1}{t}\sum_{i=1}^t (z^{b}_i - \vec{0}) \odot (z^{b}_i - \vec{0}) = \frac{1}{B}\sum_{b=1}^B\vec{1} = \vec{1} \in \mathbb{R}^{1\times n}\] <hr/> <h3 id="step-3c-second-z-scoring">Step 3c]: Second z-scoring?</h3> <p>Since the concatenated data already has zero mean and unit variance, adding any other layers of z-scoring has no effect.</p> <p><strong>Final output of Method A:</strong></p> \[\boxed{z^{b} \in \mathbb{R}^{t\times n}}\] <p>for each block. So method A simply returns the block-wise standardized data.</p> <hr/> <h2 id="4-method-b-concatenate-first-then-global-z-scoring">4] Method B: Concatenate first, then global z-scoring</h2> <h3 id="step-4a-concatenate-raw-data">Step 4a]: Concatenate raw data</h3> <p>Concatenate all blocks \(x^{b}\) into a single matrix \(X \in \mathbb{R}^{Bt \times n}\).</p> <hr/> <h3 id="step-4b-compute-global-mean-and-standard-deviation">Step 4b]: Compute global mean and standard deviation</h3> \[\mu = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t x^{b}_i = \frac{1}{B}\sum_{b=1}^B \frac{1}{t}\sum_{i=1}^{t}x_i^b \\ = \frac{1}{B}\sum_{b = 1}^{B} \mu^b \in \mathbb{R}^{1\times n}\] <p>The global mean is the average of block-wise means (it’s not hard to show that if each block has different samples, this average will become <em>weighted average</em> by the ratio of the amount of each block’s data to total data amount).</p> <hr/> <h3 id="step-4c-compute-global-variance">Step 4c]: Compute global variance</h3> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (x^{b}_i - \mu) \odot (x^{b}_i - \mu) \in \mathbb{R}^{1 \times n}\] <p>Expand the product term:</p> \[(x^{b}_i - \mu) \odot (x^{b}_i - \mu) \\ = (x^{b}_i - \mu^b + \mu^b - \mu) \odot (x^{b}_i - \mu^b + \mu^b - \mu) \\ = (x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + 2(x^{b}_i - \mu^b) \odot (\mu^b - \mu) \\ + (\mu^b - \mu) \odot (\mu^b - \mu) \\\] <p>Let’s look at each of the terms more closely. Notice that when summing over \(i\) in \(\sigma^2\), the cross term vanishes:</p> \[\frac{1}{t}\sum_{i=1}^t ((x^{b}_i - \mu^b) \odot (\mu^b - \mu))\\ = (\frac{1}{t}\sum_{i=1}^t (x^{b}_i - \mu^b)) \odot (\mu^b - \mu)\\ = (\mu^b - \mu^b) \odot (\mu^b - \mu) \\ = \vec{0}\] <p>Thus,</p> \[\sigma^2 = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t((x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + (\mu^b - \mu) \odot (\mu^b - \mu)) \\ = \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{Bt}\sum_{b=1}^B\sum_{i=1}^t (\mu^b - \mu) \odot (\mu^b - \mu) \\ = \frac{1}{B}\sum_{1}^{B}\frac{1}{t}\sum_{i=1}^{t}(x^{b}_i - \mu^b) \odot (x^{b}_i - \mu^b) + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\\ = \frac{1}{B}\sum_{1}^{B}(\sigma^b)^2 + \frac{1}{B}\sum_{b=1}^B (\mu^b - \mu) \odot (\mu^b - \mu)\] <p>This is a <strong>variance decomposition</strong> into:</p> <ul> <li>average within-block variance</li> <li>variance of block means (between-block variance)</li> </ul> <p>which meets our intuitive expectation (what else could it be anyway…).</p> <hr/> <h3 id="step-4d-global-z-scoring">Step 4d]: Global z-scoring</h3> <p>Each sample is normalized as:</p> \[y^{b}_i = \frac{x^{b}_i - \mu}{\sigma} \in \mathbb{R}^{1\times n}\] <p>By the nature of z-scoring, if we now calculate the global mean and standard deviation on concatenated \(y^b\), as in 3b] for each feature, we will obtain 0 and 1 respectively.</p> <p>Now if we rewrite the above using block-wise z-scores, since</p> <p>\(z_i^b = \frac{x_i^b - \mu^b}{\sigma^b} \rightarrow x_i^b = \sigma^bz_i^b + \mu^b\), then:</p> \[\boxed{ y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} }\] <p>which reinforces the idea that these all are just linear transformations: Transformations being linear, linear into one another.</p> <hr/> <h2 id="comparison-of-method-a-and-method-b">Comparison of Method A and Method B</h2> <p>Method A output:</p> \[z^{b}_i \in \mathbb{R}^{1\times n},\; \forall i, b\] <p>Method B output:</p> \[y^{b}_i = \frac{\sigma^b}{\sigma} z^{b}_i + \frac{\mu^b - \mu}{\sigma} \in \mathbb{R}^{1\times n}, \; \forall i, b\] <p>For the two methods to be identical \(\forall i,b\), we must have:</p> \[\sigma_b = \sigma \quad \text{and} \quad \mu_b = \mu \quad \forall b\] <p>And again, if we concatenate all \(y^b\) and \(z^b\) together separately into \(Y, Z\), they <strong>BOTH</strong> have feature-wise mean 0 and std 1.</p> <hr/> <h2 id="final-result">Final result</h2> \[\boxed{ \begin{array}{l} \text{The two procedures are NOT equivalent in general, even though} \\ \text{both yield global mean } 0 \text{ and std } 1 \text{ after transformations}. \end{array} }\] <p>They are equivalent <strong>if and only if</strong> every block already has identical feature-wise means and variances.</p> <ul> <li><strong>Method A</strong> removes all block-level mean and variance differences before concatenation.</li> <li><strong>Method B</strong> preserves block-level differences and normalizes relative to the pooled distribution.</li> </ul> <p>Block-wise z-scoring and global z-scoring <strong>do not commute</strong>. These choices encode different assumptions about whether block identity (e.g., session, subject, condition) should be preserved or discarded. Our choice should be driven by whether block-to-block variability is meaningful signal or nuisance variability in our analysis.</p> <p>Fun quesitons:</p> <ul> <li>1] What if block size \(t\) is not the same across all blocks?</li> <li>2] What are other (useful/effective) ways of normalization which also return the same mean/std (0/1, e.g.)?</li> </ul> <p>Practical question: In practice, how much do the statistics from these two methods actually differ? How should we interpret such differences?</p>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Brain Computer Interface"/><summary type="html"><![CDATA[Zscoring, block vs session level comparisons]]></summary></entry><entry><title type="html">The Dance of Space: Geom/Topo/Dynam Mumble(4) (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/subspace/" rel="alternate" type="text/html" title="The Dance of Space: Geom/Topo/Dynam Mumble(4) (in progress)"/><published>2026-02-03T21:49:38+00:00</published><updated>2026-02-03T21:49:38+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/subspace</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/subspace/"><![CDATA[<h2 id="setup">Setup</h2> <p>Suppose we compute PCA on two datasets (e.g., train vs.\ test), and we keep the top-$r$ principal components. Let</p> \[U_{\text{train}} \in \mathbb{R}^{d \times r}, \qquad U_{\text{test}} \in \mathbb{R}^{d \times r},\] <p>where each matrix has <strong>orthonormal columns</strong> (so each is a basis for an $r$-dimensional subspace of $\mathbb{R}^d$).</p> <p>Our first obvious intuition might be to define the <strong>subspace overlap matrix</strong> as the following:</p> \[S = U_{\text{train}}^{\top} U_{\text{test}} \in \mathbb{R}^{r \times r}.\] <p>So, each entry is a dot product between basis vectors:</p> \[S_{ij} = u_i^{(\text{train})} \cdot u_j^{(\text{test})}.\] <p>At first glance, this looks like a direct “basis alignment” comparison, which is exactly what we aim for. How else could we characterize subspace change other than looking at pairs-wise relationships among two sets of basis vectors? Well, there’re a few caveats…</p> <hr/> <h2 id="1-why-basis-by-basis-alignment-is-unstable">1] Why basis-by-basis alignment is unstable</h2> <p>Comparing PCA vectors one-by-one (e.g., “PC1 vs.\ PC1”) is unstable because PCA eigenvectors are <strong>not uniquely defined</strong> in two common cases. The following problems might pop up —</p> <h3 id="i-sign-flips">(i) Sign flips</h3> <p>If \(u\) is an eigenvector, then \(-u\) is also an eigenvector.</p> <p>So dot products can flip sign even when the <em>subspace is identical</em>.</p> <h3 id="ii-degenerate--near-degenerate-eigenvalues-rotation-inside-the-subspace">(ii) Degenerate / near-degenerate eigenvalues (rotation inside the subspace)</h3> <p>If</p> \[\lambda_i \approx \lambda_{i+1},\] <p>then the corresponding principal directions inside the 2D span can rotate dramatically under tiny perturbations (noise, finite-sample effects, etc.).</p> <p>This means that even if the <em>span</em> is essentially the same, the individual vectors $u_i$ can change a lot.<br/> So comparing “PC1 to PC1” is not meaningful.</p> <hr/> <h2 id="2-what-singular-values-do-that-dot-products-dont">2] What singular values do that dot products don’t</h2> <p>The matrix</p> \[S = U_{\text{train}}^{\top} U_{\text{test}}\] <p>depends on the <em>chosen bases</em> inside each subspace. If we change bases within either subspace via orthogonal transformations:</p> \[U_{\text{train}} \to U_{\text{train}} R_1, \qquad U_{\text{test}} \to U_{\text{test}} R_2,\] <p>where $R_1, R_2 \in \mathbb{R}^{r \times r}$ are orthogonal (including sign flips as a special case), then</p> \[S \to (U_{\text{train}}R_1)^{\top}(U_{\text{test}}R_2) = R_1^{\top} S R_2.\] <p>So the <em>entries</em> of $S$ can change wildly.</p> <h3 id="key-fact-invariance">Key fact (invariance)</h3> <blockquote> <p>The <strong>singular values</strong> of \(S\) are invariant under left/right orthogonal rotations:</p> </blockquote> <ul> <li>Left-multiplying by an orthogonal matrix does not change singular values.</li> <li>Right-multiplying by an orthogonal matrix does not change singular values.</li> </ul> <p>Therefore, even if PCA “relabels,” flips signs, or rotates the basis vectors within the subspace, the <strong>singular values remain unchanged</strong>.</p> <p>This means singular values capture a property of the <strong>subspaces</strong>, not of the particular eigenvectors chosen.</p> <hr/> <h2 id="3-geometric-meaning-principal-angles">3] Geometric meaning: principal angles</h2> <p>Take the SVD:</p> \[S = Q \Sigma R^{\top},\] <p>where</p> \[\Sigma = \mathrm{diag}(\sigma_1,\dots,\sigma_r).\] <p>A fundamental result is:</p> \[\sigma_i = \cos(\theta_i),\] <p>where $\theta_i$ are the <strong>principal angles</strong> between the two $r$-dimensional subspaces.</p> <p>Interpretation:</p> <ul> <li>$\theta_i = 0 \implies$ perfectly aligned direction exists (since $\cos(\theta_i)=1$)</li> <li>$\theta_i = 90^\circ \implies$ orthogonal direction (since $\cos(\theta_i)=0$)</li> </ul> <p>So the singular values summarize <em>how much overlap</em> the two subspaces have along their best-aligned directions.</p> <hr/> <h2 id="4-why-this-is-the-stable-comparison">4] Why this is the “stable” comparison</h2> <p>Think of $U_{\text{train}}$ and $U_{\text{test}}$ as <strong>arbitrary coordinate systems</strong> inside their respective subspaces.</p> <p>A meaningful comparison should ignore that arbitrariness.</p> <p>Principal angles / singular values do exactly this: they compute the <strong>best possible matching</strong> between directions in the two subspaces.</p> <p>Instead of comparing “PC1 $\leftrightarrow$ PC1,” we solve an optimal alignment problem:</p> \[\max_{\|a\|=\|b\|=1} a^{\top}\bigl(U_{\text{train}}^{\top}U_{\text{test}}\bigr)b,\] <p>and the sequence of best matches yields</p> \[\sigma_1,\sigma_2,\dots\] <p>as the strengths of alignment along the best-aligned directions.</p> <hr/> <h2 id="5-tiny-example-intuition-2d-case">5] Tiny example intuition (2D case)</h2> <p>Suppose both subspaces are actually the same 2D plane in $\mathbb{R}^d$.</p> <p>You could pick:</p> <ul> <li>$U_{\text{train}}$ = standard basis in that plane</li> <li>$U_{\text{test}}$ = same plane but rotated by $45^\circ$ inside it</li> </ul> <p>Then $S$ might look like a rotation matrix:</p> \[S= \begin{pmatrix} \cos 45^\circ &amp; -\sin 45^\circ \\ \sin 45^\circ &amp; \cos 45^\circ \end{pmatrix}.\] <p>The entries are not the identity, so basis-by-basis dot products look “not aligned.”</p> <p>But the singular values of a rotation matrix are both $1$.</p> <p>So singular values correctly say: <strong>the subspaces are identical</strong>.</p> <p>That’s the whole point.</p> <hr/> <h2 id="bottom-line">Bottom line</h2> <p>We use singular values of</p> \[U_{\text{train}}^{\top}U_{\text{test}}\] <p>because:</p> <ul> <li>✅ they are invariant to sign flips / rotations / re-ordering of PCA vectors inside the subspace</li> <li>✅ they define principal angles, which are a true subspace-to-subspace comparison</li> <li>✅ they give a stable measure of drift even when eigenvectors are not uniquely defined</li> </ul> <hr/> <h2 id="a-sidenote-connection-to-projection-distance">A sidenote: Connection to projection distance</h2> <p>Let $P_{\text{train}}$ and $P_{\text{test}}$ be the orthogonal projection matrices onto the two subspaces:</p> \[P_{\text{train}} = U_{\text{train}}U_{\text{train}}^{\top}, \qquad P_{\text{test}} = U_{\text{test}}U_{\text{test}}^{\top}.\] <p>Then one can show the Frobenius-distance relationship:</p> \[\|P_{\text{train}} - P_{\text{test}}\|_F^2 = 2r - 2\|U_{\text{train}}^{\top}U_{\text{test}}\|_F^2 = 2\sum_{i=1}^r \sin^2(\theta_i).\]]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Subspace Geometry and Computation]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)"/><published>2026-01-14T20:22:25+00:00</published><updated>2026-01-14T20:22:25+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(3)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/"><![CDATA[<p>This is episode 3 on the smooth manifold series. Today we will be diving into the properties of maps between manifolds. I will first summarize how to understand and compute the differential of a smooth map between manifolds, both abstractly and concretely, culminating in the matrix-valued example<br/> \(F(A) = A^\top A.\)</p> <hr/> <h2 id="1-differential-canonical-definition">1] Differential: canonical definition</h2> <h3 id="11-differential-of-a-smooth-map-intrinsic-definition">1.1 Differential of a Smooth Map (Intrinsic Definition)</h3> <p>Let \(F : M \to N\) be a smooth map between smooth manifolds.</p> <p>For any point $ p \in M $, the <strong>differential</strong> is a linear map which is a <strong>pushforward of derivations</strong>. \(dF_p : T_p M \longrightarrow T_{F(p)} N.\)</p> <h3 id="derivation-based-definition">Derivation-based definition</h3> <p>If $ v \in T_p M $ is a tangent vector viewed as a derivation, then \((dF_p v)(g) := v(g \circ F), \qquad g \in C^\infty(N).\)</p> <p>This definition is <strong>coordinate-free</strong>.</p> <p>It might appear at first both unnecessarily abstract and underestimated as to its computation. We might claim that it is essentially just a Jacobian matrix in local coordinates. However, the essence of this concept resides on its definition to be conceptually a coordinate-independent linear map between tangent spaces.</p> <hr/> <h3 id="12-coordinate-representation-and-the-jacobian">1.2 Coordinate Representation and the Jacobian</h3> <p>To compute $ dF_p $ in practice:</p> <ol> <li>Choose a chart $ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>Choose a chart $ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ol> <p>Define the coordinate expression: \(\tilde F = \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Then: \(\boxed{ dF_p \;\text{is represented by}\; D\tilde F(\varphi(p)) }\)</p> <p>That is, <strong>the Jacobian matrix is the coordinate representation of the differential</strong>.</p> <blockquote> <p>The Jacobian depends on coordinates; the linear map $ dF_p $ does not.</p> </blockquote> <hr/> <h3 id="13-why-this-is-not-just-the-jacobian">1.3 Why this is not “just” the Jacobian</h3> <p>The Jacobian depends on coordinates;<br/> $ dF_p $ does not.</p> <p>More precisely:</p> <ul> <li>$ dF_p $ is a <strong>geometric linear map</strong></li> <li>The Jacobian is a <strong>matrix representation</strong> of that map in chosen bases: \(\frac{\partial \bigl(\psi^1 \circ F,\;\dots,\;\psi^n \circ F\bigr)} {\partial \bigl(x^1,\;\dots,\;x^m\bigr)}\)</li> </ul> <p>If you change charts, the matrix changes by:</p> \[\boxed{ J_{\text{new}} = D\psi\,\cdot\, J_{\text{old}} \,\cdot\, (D\varphi^{-1}) }\] <p>but the underlying linear map $ dF_p $ stays the same.</p> <hr/> <h2 id="2-differential-alternative-interpretation">2] Differential: alternative interpretation</h2> <h3 id="21-curve-based-definition">2.1 Curve-based definition</h3> <p>This is also coordinate-free:</p> <p>If<br/> \(\gamma : (-\varepsilon,\varepsilon) \to M\) is a smooth curve with \(\gamma(0) = p \quad \text{and} \quad \gamma'(0) = v \in T_p M,\) then \(\boxed{ dF_p(v) = (F \circ \gamma)'(0) \in T_{F(p)} N. }\)</p> <p>No coordinates anywhere. This viewpoint is often the most intuitive and is fully equivalent to the derivation definition.</p> <hr/> <h3 id="22-curves-in-local-coordinates">2.2 Curves in local coordinates</h3> <p>Choose charts:</p> <ul> <li>$ (U,\varphi) $ on $ M $ with $ p \in U $</li> <li>$ (V,\psi) $ on $ N $ with $ F(p) \in V $</li> </ul> <p>Define the coordinate representation: \(\tilde F := \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n.\)</p> <p>Now define the coordinate curve: \(\tilde\gamma := \varphi \circ \gamma : (-\varepsilon,\varepsilon) \to \mathbb{R}^m.\)</p> <p>Consequently, in coordinates the statement is the following:</p> \[\boxed{ D\tilde F(\varphi(p)) \cdot \tilde\gamma'(0) = D(\psi \circ F \circ \varphi^{-1})(\varphi(p)) \cdot (\varphi \circ \gamma)'(0). }\] <p>In the coordinate formula, \(\gamma'(0) \quad \text{really means} \quad \tilde\gamma'(0) = (\varphi \circ \gamma)'(0).\)</p> <p>Here’s a diagram that makes everything explicit</p> \[\begin{array}{ccc} T_p M &amp; \xrightarrow{dF_p} &amp; T_{F(p)} N \\ \downarrow d\varphi_p &amp; &amp; \uparrow d\psi^{-1}_{\psi(F(p))} \\ \mathbb{R}^m &amp; \xrightarrow{D\tilde F(\varphi(p))} &amp; \mathbb{R}^n \end{array}\] <p>Thus:</p> <ul> <li>$ \gamma’(0) $ lives in $ T_p M $</li> <li>$ (\varphi \circ \gamma)’(0) = d\varphi_p(\gamma’(0)) \in \mathbb{R}^m $</li> <li>$ D\tilde F(\varphi(p)) $ acts on that coordinate vector</li> </ul> <hr/> <h3 id="23-sidenote-graph-differential">2.3 Sidenote: Graph Differential</h3> <p>The <strong>graph</strong> of $ F $ is \(\Gamma_F = \{ (p, F(p)) \mid p \in M \} \subset M \times N.\)</p> <p>Define the graph map: \(\Phi : M \to M \times N, \quad \Phi(p) = (p, F(p)).\)</p> <p>Its differential is: \(\boxed{ d\Phi_p(v) = (v, dF_p(v)). }\)</p> <p>This is what is often called the <strong>graph differential</strong>.</p> <hr/> <h2 id="3-special-case-maps-between-vector-spaces">3] Special Case: Maps Between Vector Spaces</h2> <h3 id="31-a-great-simplification">3.1 A great simplification</h3> <p>If $ M = \mathbb{R}^m $, $ N = \mathbb{R}^n $, then: \(T_p M \cong \mathbb{R}^m, \quad T_{F(p)} N \cong \mathbb{R}^n.\)</p> <p>In this case:</p> <ul> <li>$ dF_p $ is a linear map $ \mathbb{R}^m \to \mathbb{R}^n $</li> <li>Its matrix is exactly the <strong>Jacobian matrix</strong></li> <li>$ dF_p(H) $ coincides with the <strong>Fréchet / directional derivative</strong></li> </ul> <hr/> <h3 id="32-worked-example--fa--atop-a-">3.2 Worked Example: $ F(A) = A^\top A $</h3> <p>Let \(F : \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}, \quad F(A) = A^\top A.\)</p> <p>Since $ \mathbb{R}^{n \times n} $ is a vector space, \(T_A(\mathbb{R}^{n \times n}) \cong \mathbb{R}^{n \times n}.\)</p> <h4 id="321-direct-computation">3.2.1 Direct computation</h4> <p>There are several ways to compute the differential. The most straight-forward method is to do the following (as you might imagine):</p> <p>For \(H \in T_A(\mathbb{R}^{n \times n})\), \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} (A+tH)^\top(A+tH).\)</p> <p>Expanding: \((A+tH)^\top(A+tH) = A^\top A + t(H^\top A + A^\top H) + t^2 H^\top H.\)</p> <p>Thus: \(\boxed{ dF_A(H) = H^\top A + A^\top H. }\)</p> <h4 id="322-curve-based-computation">3.2.2 Curve-based computation</h4> <p>Let \(A(t)\) be a smooth curve with: \(A(0)=A, \quad A'(0)=H.\)</p> <p>Then: \(dF_A(H) = \left.\frac{d}{dt}\right|_{0} A(t)^\top A(t) = H^\top A + A^\top H.\)</p> <p>This makes it clear that the definition of \(dF_A(H)\) is <strong>coordinate-free</strong>.</p> <p>Sidenote: if we restrict \(A\) to symmetric or SPD matrices, what will we see? Or what if we connect this to Riemannian geometry on \(GL(n)\) or \(SPD(n)\)? We’ll come back to this later when we discuss Lie group and Lie algebra.</p> <hr/> <h2 id="4-back-to-classical-regular-surfaces-parametrizations-immersions">4] Back to classical regular surfaces (parametrizations, immersions)</h2> <h3 id="the-question-in-do-carmo-diffgeom">The Question in do Carmo DiffGeom</h3> <p>In do Carmo’s definition of a <strong>regular surface</strong> in \(\mathbb{R}^3\), a coordinate map \(X : U \subset \mathbb{R}^2 \to \mathbb{R}^3\) is required to satisfy two conditions:</p> <ol> <li>\(X\) is a <strong>differentiable homeomorphism</strong> onto its image.</li> <li>The differential \(dX_p\) is <strong>injective</strong> at every point $p \in U$.</li> </ol> <p>Since \(X\) maps from \(\mathbb{R}^2\) to \(\mathbb{R}^3\), its differential can never be surjective, so injectivity (rank 2) is the meaningful requirement.</p> <p>A natural question arises:</p> <blockquote> <p>If \(X\) is already a differentiable homeomorphism, isn’t its differential automatically injective?</p> </blockquote> <p>The answer is <strong>no</strong>.</p> <p>Well, to make it explicit, let’s figure out first what a differentiable homeomorphism actually gives us: If \(X : U \to \mathbb{R}^3\) is a differentiable homeomorphism onto its image, then:</p> <ul> <li>\(X\) is <strong>continuous and injective</strong></li> <li>\(X^{-1}\) is <strong>continuous</strong> (but <em>not</em> necessarily differentiable)</li> <li>Topologically, \(X(U)\) looks like a 2‑dimensional surface</li> </ul> <p>This is a <strong>topological</strong> statement plus differentiability of $X$. It controls <em>points</em>, but says nothing about what happens to <em>directions</em>. Crucially, differentiability of the inverse is <em>not</em> assumed.</p> <p>Fine, but then why injectivity of the differential is not automatically assured? Notice that the differential</p> \[dX_p : \mathbb{R}^2 \to \mathbb{R}^3\] <p>being injective means it has <strong>rank 2</strong> meaning no tangent direction is collapsed. A map can be:</p> <ul> <li>injective,</li> <li>continuous with continuous inverse,</li> <li>differentiable,</li> </ul> <p>and <em>still</em> have rank drop somewhere. Let me give a concrete example:</p> <p>Consider \(X(u,v) = (u^3, v, 0).\)</p> <p>Properties of this map:</p> <ul> <li>It is <strong>injective</strong></li> <li>It is a <strong>homeomorphism onto its image</strong></li> <li>It is differentiable everywhere</li> </ul> <p>However, its differential is \(dX_{(u,v)} = \begin{pmatrix} 3u^2 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix}\)</p> <p>At \($u = 0\), this matrix has <strong>rank 1</strong>, not 2. One tangent direction is squashed. Therefore, this map is <strong>not an immersion</strong>, and it does <strong>not</strong> define a regular surface in do Carmo’s sense.</p> <hr/> <p>However, you might observe here a seemingly apparent paradox: “But the image is <strong>Just</strong> a plane!”</p> <p>Well, the image of \(X(u,v) = (u^3, v, 0)\) is \({(x,y,0) : x,y \in \mathbb{R}},\) which is the entire (xy)-plane indeed. As a <strong>subset</strong> of \(\mathbb{R}^3\), this plane is perfectly flat, so one might expect its tangent plane at every point to be the whole plane. So then why does the tangent collapse under this map?</p> <hr/> <p>The key point here is that here are <strong>two distinct notions</strong> at play:</p> <ol> <li>The tangent plane of a <strong>subset</strong> of \(\mathbb{R}^3\)</li> <li>The tangent plane <strong>defined by a parametrization</strong></li> </ol> <p>In do Carmo’s approach, tangent planes are defined <em>via parametrizations</em>. The tangent plane at a point is \(T_pS = \operatorname{span}{X_u(p), X_v(p)}.\)</p> <p>For the map above: \(X_u = (3u^2, 0, 0), \quad X_v = (0,1,0).\)</p> <p>At (u=0): \(X_u(0,v) = (0,0,0), \quad X_v(0,v) = (0,1,0),\) so the span is <strong>1‑dimensional</strong>.</p> <p>This means:</p> <blockquote> <p>The parametrization fails to distinguish two independent directions in the parameter domain.</p> </blockquote> <p>Geometrically, the $u$-direction has been crushed.</p> <hr/> <p>But does the plane still have a 2D tangent plane?</p> <p>Yes — but <strong>not via this parametrization</strong>.</p> <p>If instead we parametrize the same plane by \(Y(s,t) = (s,t,0),\) then \(dY = \begin{pmatrix} 1 &amp; 0 \ 0 &amp; 1 \ 0 &amp; 0 \end{pmatrix},\) which has rank 2 everywhere.</p> <p>So the <em>same subset</em> becomes a <strong>regular surface</strong> under a different map.</p> <hr/> <p>Finally, what does this meana conceptually? The key lesson is:</p> <blockquote> <p><strong>Regularity is not a property of the subset alone — it is a property of the subset together with its smooth structure.</strong></p> </blockquote> <p>Different parametrizations can induce:</p> <ul> <li>a <strong>good</strong> smooth structure (immersion)</li> <li>or a <strong>bad</strong> one (rank collapse)</li> </ul> <p>This is why do Carmo requires that <strong>there exists</strong> a local parametrization with injective differential.</p> <hr/> <p>In other words, if you ask “Does that mean there might exist other maps which make the plane a regular surface?”, then the answer is Yes — absolutely. The plane <em>is</em> a regular surface because such maps exist.</p> <p>Behind that is another question: “Does immersion entirely depend on which maps we pick?” Yes!</p> <ul> <li><strong>Immersion is a property of the map</strong>, not of the set.</li> <li>A <strong>regular surface</strong> is a set for which <em>good immersions exist everywhere</em>.</li> </ul> <p>In this seense, do Carmo does separate the conditions, being deliberately modular:</p> <ol> <li><strong>Homeomorphism</strong> → good topology (no self‑intersections)</li> <li><strong>Injective differential</strong> → good differential geometry</li> </ol> <p>whether neither condition implies the other.</p> <p>Finally, some of key points:</p> <ul> <li>The same subset of \(\mathbb{R}^3\) can support <strong>many different smooth structures</strong></li> <li>Differential geometry only works once a smooth structure is fixed</li> <li>Parametrizations are how do Carmo <em>builds</em> that structure</li> </ul> <p>This is why modern texts often say:</p> <blockquote> <p>“A surface is a 2‑dimensional smooth manifold embedded in $$\mathbb{R}^3$.”</p> </blockquote> <p>Do Carmo reaches this notion <strong>from parametrizations upward</strong>, rather than assuming it at the start.</p> <hr/> <blockquote> <p><strong>Topology sees points.</strong> <strong>Differential geometry sees directions.</strong></p> </blockquote> <p>A homeomorphism controls points. Injectivity of the differential controls directions.</p> <p>You need <strong>both</strong> to get a regular surface.</p> <h2 id="5-association-with-inverse-function-theorem">5] Association with inverse function theorem</h2> <h2 id="6-immersion-submersion">6] Immersion, submersion</h2> <p>Immersion basics</p> <p>Here I want to emphasize one key fact that:</p> <blockquote> <p>immersion imply a nonzero determinant in coordinates</p> </blockquote> <p>Why? Again, let’s recall that immersion means the differential is injective Let<br/> \(\phi:M^m\to N^n,\qquad m\le n\) and let $p\in M$.<br/> Saying <strong>(\phi) is an immersion at (p)</strong> means the differential \(d\phi_p:T_pM\to T_{\phi(p)}N\) is <strong>injective</strong>.</p> <p>Equivalently (in linear algebra language):<br/> \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Now, if write it in local coordinates → Jacobian matrix has rank \(m\) Pick coordinate charts:</p> <ul> <li>on \(M\): \(x=(x^1,\dots,x^m)\) around \(p\)</li> <li>on \(N\): \(y=(y^1,\dots,y^n)\) around \(\phi(p)\)</li> </ul> <p>Then locally \(\phi\) looks like a smooth map between Euclidean spaces: \(y^a = \phi^a(x^1,\dots,x^m),\qquad a=1,\dots,n.\)</p> <p>Its differential in these coordinates is represented by the <strong>Jacobian matrix</strong> \(J(p)=\left(\frac{\partial \phi^a}{\partial x^i}(p)\right)\) which is an (n\times m) matrix.</p> <p>The immersion condition says: \(\operatorname{rank}(J(p))=m.\)</p> <p>So the columns of \(J(p)\) are linearly independent.</p> <p>Now we use a standard linear algebra fact:</p> <blockquote> <p>An \(n\times m\) matrix has rank \(m\) <strong>iff</strong> there exists an \(m\times m\) submatrix (choose \(m\) rows) whose determinant is nonzero.</p> </blockquote> <p>Why?</p> <ul> <li>If <strong>every</strong> \(m\times m\) minor determinant were zero, then <strong>every</strong> set of $m$ rows would be linearly dependent, so the rank would be \(&lt;m\).</li> <li>Since the rank is \(m\), at least one choice of $m$ rows gives an invertible \(m\times m\) matrix.</li> </ul> <p>Concretely: there exist indices \(1\le a_1&lt;\cdots&lt;a_m\le n\) such that the matrix \(\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)_{\alpha,i}\) has \(\det\left(\frac{\partial \phi^{a_\alpha}}{\partial x^i}(p)\right)\neq 0.\)</p> <p>If we now <strong>rename/reorder the target coordinates</strong> so that those special indices become \(1,\dots,m\), then we can assume:</p> \[\det\left(\frac{\partial (\phi^1,\dots,\phi^m)}{\partial (x^1,\dots,x^m)}(p)\right)\neq 0.\] <p>That’s exactly the statement: in coordinates (after renumbering if needed), an immersion gives a nonzero determinant of an \(m\times m\) Jacobian block.</p> <p>In one line summary: an immersion means \(\phi\) “doesn’t collapse any tangent directions,” so locally you can find $m$ coordinate functions of \(\phi\) that vary independently — and “vary independently” is exactly “Jacobian block has nonzero determinant.”</p> <hr/> <h4 id="side-note-connection-to-the-constant-rank-theorem--local-normal-form-of-an-immersion">Side note: Connection to the Constant Rank Theorem / local normal form of an immersion?</h4> <p>Remember that Constant Rank Theorem (specialized to immersions) says: Let \(\phi:M^m\to N^n\) be smooth, and suppose \(\phi\) is an <strong>immersion at \(p\)</strong>.<br/> That means \(\operatorname{rank}(d\phi_p)=m.\)</p> <p>Then the constant rank theorem says:</p> <blockquote> <p>There exist coordinate charts<br/> \((U,x)\ \text{around }p,\qquad (V,y)\ \text{around }\phi(p)\) such that in these coordinates the map becomes \(y\circ \phi\circ x^{-1}(u^1,\dots,u^m) \;=\; (u^1,\dots,u^m,0,\dots,0).\)</p> </blockquote> <p>So locally, \(\phi\) looks like the <strong>standard inclusion</strong> \(\mathbb{R}^m \hookrightarrow \mathbb{R}^n,\qquad u\mapsto (u,0).\)</p> <p>That is the precise geometric meaning of “immersion.”</p> <p>In these special coordinates, \(\phi^1(u)=u^1,\;\dots,\;\phi^m(u)=u^m,\qquad \phi^{m+1}(u)=0,\dots,\phi^n(u)=0.\)</p> <p>So the Jacobian matrix is literally \(J= \begin{pmatrix} I_m\\ 0 \end{pmatrix}\) (an \(n\times m\) matrix).</p> <p>Now look at the top \(m\times m\) block: \(\frac{\partial(\phi^1,\dots,\phi^m)}{\partial(u^1,\dots,u^m)} = I_m,\) so \(\det(I_m)=1\neq 0.\)</p> <p>That’s exactly the coordinate statement.</p> <p>How this matches the “minor is nonzero” argument? Before using the constant rank theorem, we only know:</p> <ul> <li>\(J(p)\) has rank \(m\)</li> <li>therefore some \(m\times m\) minor determinant is nonzero</li> </ul> <p>Then the constant rank theorem tells us that we can actually <strong>choose coordinates</strong> so that the “good minor” becomes the <em>first</em> \(m\) coordinates, and the map becomes \($(u,0)\).</p> <p>So:</p> <ul> <li><strong>Linear algebra fact:</strong> full rank \(\Rightarrow\) some minor \(\neq 0\)</li> <li><strong>Constant rank theorem:</strong> we can change coordinates to make that minor the obvious identity matrix.</li> </ul> <p>The geometric picture (why \((u,0)\) is the right normal form) is that an immersion means \(\phi\) “injects tangent vectors,” so locally \(\phi(U)\subset N\) is an $m-dimensional “sheet” sitting inside an n-dimensional space. In good coordinates on N, that sheet looks like:</p> \[\{(y^1,\dots,y^n): y^{m+1}=\cdots=y^n=0\},\] <p>i.e. an embedded copy of \(\mathbb{R}^m\\).</p> <p>So locally, \(\phi\) is just a parametrization of that sheet.</p> <h2 id="7-embedding">7] Embedding</h2> <h3 id="72-a-key-difference-between-embedding-and-immersion">7.2 A key difference between embedding and immersion</h3> <p><strong>“no self-intersections” is one of the key geometric consequences of being <em>embedded</em></strong> (as opposed to merely <em>immersed</em>). As stated above, if a manifold \(M\) is <strong>embedded</strong> in \(\mathbb{R}^k\), it sits inside \(\mathbb{R}^k\) as a “nice subset,” like a surface you could physically draw without crossing itself. More formally, an <strong>embedding</strong> \(F: M \to \mathbb{R}^k\) means:</p> <ol> <li>\(F\) is a <strong>smooth immersion</strong> (its differential is injective everywhere), and</li> <li>\(F\) is a <strong>homeomorphism onto its image</strong> \(F(M)\) (with the subspace topology).</li> </ol> <p>That second condition is exactly what rules out the classic “self-crossing” pathology. If the image “intersects itself” in the sense that two different points \(p\neq q\in M\) map to the same point in \(\mathbb{R}^k\), i.e. \(F(p)=F(q),\) then \(F\) is <strong>not injective</strong>, so it can’t be an embedding.</p> <p>So: <strong>an embedded submanifold cannot cross itself as a set in \(\mathbb{R}^k\)</strong>.</p> <p>On the other hand, an <strong>immersion</strong> can look like a manifold with self-crossings in \(\mathbb{R}^k\). An example will be the “figure-eight curve” in \(\mathbb{R}^2\) can be parametrized smoothly with nonzero derivative everywhere, so it’s an immersion, but it’s <strong>not embedded</strong> because it fails injectivity / fails to be a homeomorphism onto its image.</p> <p>In shoft,</p> <ul> <li><strong>Embedded \(\Rightarrow\)</strong> injective + “topologically correct” inclusion<br/> \(\Rightarrow\) <strong>no self-intersections</strong>.</li> <li><strong>Immersed \(\Rightarrow\)</strong> locally nice but can globally overlap<br/> \(\Rightarrow\) <strong>self-intersections possible</strong>.</li> </ul> <h2 id="8-submanifolds">8] Submanifolds</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Explore relations of maps between manifolds]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)"/><published>2025-09-17T23:30:02+00:00</published><updated>2025-09-17T23:30:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(2)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/"><![CDATA[<p>This is episode 2 on the smooth manifold series. Today we will be diving into concepts that appear initially very intuitive at first glance, but then the extended version of which is indeed quite abstract.</p> <h2 id="0-familiar-examples">0] Familiar examples</h2> <p>Let me start with two simple concrete examples to illustrate what tangent vectors and tangent space are. Indeed they match up to our intuition!</p> <p>Let’s first say that we have a unit circle in \(\mathbb{R}^2\), or basically let’s denote it as \(S^1\) (this is a standard notation as</p> <p>\(S^{n} = \{ x \in \mathbb{R}^{n+1} \mid |x| = 1 \}\),</p> <p>representing the surface of an \((n+1)\)-dimensional ball). Pick \(p = (1, 0)\). If I ask you what the tangent vector is starting at \(p\) and tangnet to \(S^1\)? Your answer is probably a vector pointing upward or downward with its tail at \(p\). Indeed,</p> <h2 id="1-three-equivalent-definitions-of-tangent-space">1] Three equivalent definitions of Tangent Space</h2> <p>We’ll cover three equivalent definitions tangent space.</p> <h3 id="11-tangent-vectors-are-equivalence-classes-of-curves">1.1] Tangent vectors are equivalence classes of curves</h3> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>tangent vectors are equivalence classes of smooth curves through \(p\):</p> \[T_pM = \{\frac{d}{dt}\Big|_{t = 0} \gamma(t) \Big| \gamma: (\epsilon, \epsilon) \rightarrow M, \; \gamma(0) = p \}\] <p>where \(\gamma_1 \sim \gamma_2\) if in some (equivalently, any) coordinate chart \(\phi: U \subset M \rightarrow \mathbb{R}^n\):</p> \[\frac{d}{dt}\Big|_{t = 0} \phi(\gamma_1(t)) = \frac{d}{dt}\Big|_{t = 0} \phi(\gamma_2(t))\] <p>Consequently, each tangent vector is represented by the velocity of a curve through \(p\).</p> <h3 id="12-tangent-vectors-are-derivations-at-p">1.2] Tangent vectors are derivations at \(p\)</h3> <p>Let \(M\) be a manifold, the tangent space at \(p \in M\), denoted as \(T_pM\), is the the vector space of all derivations at \(p\). Notice that a derivation at \(p\) is a <strong>linear</strong> operator (at \(p\)):</p> \[D: C^{\infty}_{p}(M) \rightarrow \mathbb{R}\] <p>satisfying the Leibniz rule:</p> \[D(fg) = f(p)D(g) + g(p)D(f), \; \forall f, g \in C^{\infty}(M)\] <p>where \(C^{\infty}_{p}\) is the equivalence class of \((f, U)\), where \(f \in C^{\infty}\) and \(U\) is a neighbourhood of \(p\). Two functions \((f, U)\) and \((g, V)\) are equivalent iff \(\exist \; W \subset U \cap V\) a neighbourhood of \(p\) such that \(f(x) = g(x), \forall x \in W\), where \(g \in C^{\infty}\) and \(V\) is a neighbourhood of \(p\).</p> <p>Intuitively, \(D\) is like a directional derivative operator acting on smooth functions near \(p\). Consequently,</p> \[T_p{M} = \{ D \mid D \; \mathrm{is\ a\ derivation\ at} p \}\] <p>and a tangent vector \(v \in T_p{M}\) is a derivation \(D\). In fact, Tu (Theorem 2.2, An introduction to manifolds) showed that there’s a bijection between derivations at \(p\) and directional derivaties.</p> <h3 id="13-tangent-vectors-as-equivalences-on-function-germs">1.3] Tangent vectors as equivalences on function germs</h3> <h2 id="2-local-coordinate-description">2] Local coordinate description</h2> <p>Specifically if we adopt interpretation [2], then with \((U, \phi)\) a chart with coordinates \((x^1, \dots, x^n)\) near \(p\)</p> \[\{ \frac{\partial}{\partial x^1}\Big|_p, \dots, \frac{\partial}{\partial x^n}\Big|_p \}\] <p>form a basis of \(T_pM\).</p> <p>Thus any tangent vector \(v\) can be written uniquely as</p> \[v = \sum_{i = 1}^{n} v^i \frac{\partial}{\partial x^i}\Big|_p\] <p>where \((v^1, \dots, v^n)\) are the components of the vector in this coordinate system.</p> <h2 id="3-equivalence-between-derivations-and-curves">3] Equivalence between derivations and curves</h2> <p>Perhaps not so surprisingly, the above two definitions are compatible and even equivalent to one another.</p> <p>From curves to derivations:</p> <h2 id="4-a-concrete-computational-example">4] A concrete computational example</h2> <p>A coherent walkthrough using the sphere \(S^2\)</p> <p>Let’s re-emphasize again that a vector field on a Manifold is defined as the following:</p> <p>Let \(M\) be a smooth manifold. A <strong>vector field</strong> on \(M\) is a smooth section/assignment \(p \mapsto X_p \in T_p M\) where \(T_p M\) is the tangent space at \(p\).</p> <p><strong>Fundamental viewpoint</strong>:</p> <blockquote> <p>A tangent vector is a <strong>derivation</strong>: a linear map \(X_p : C^\infty(M) \to \mathbb{R}\) satisfying the Leibniz rule.</p> </blockquote> <p>We’ll also give the local coordinates and coordinate vector fields as the following:</p> <p>Let \((U, \varphi), \quad \varphi : U \subset M \to V \subset \mathbb{R}^n\) be a coordinate chart, with coordinates \((x^1, \dots, x^n).\)</p> <p>Each coordinate \(x^i\) is itself a <strong>function on the manifold</strong>: \(x^i : U \to \mathbb{R}.\)</p> <p>Definition of the “Coordinate Vector Field”:</p> <p>The coordinate vector field \(\left.\frac{\partial}{\partial x^i}\right|_p\) is defined <strong>intrinsically</strong> by its action on smooth functions: \(\boxed{ \left.\frac{\partial}{\partial x^i}\right|_p(f) := \frac{\partial}{\partial x^i} \big(f \circ \varphi^{-1}\big) \Big(\varphi(p)\Big) }\) where \(x^i\) is the \(i\)-th coordinate on \(\mathbb{R}^n\).</p> <p>Notice that there’s an abuse of notation (the left “partial” is what we define, whereas the right partial is the usual partial differentiation). Hopefully, it’s obvious that the above definition stems from the following basic definition of graph differential (which I will talk more into next time):</p> \[\boxed{ (dF_p(v))(f) := v_p(f \circ F) }\] <p>More importantly, this definition:</p> <ul> <li>uses <strong>only</strong> the chart,</li> <li>does <strong>not</strong> require an embedding and thus does $NOT$ give us components in $\mathbb{R}^n$</li> <li>explains what “\(\partial f / \partial x^i\)” actually means: It means the ordinary partial derivative of the coordinate expression of $f$ after pulling $f$ back to a coordinate chart.Note that nothing is being differentiated on $\mathbb{R}^n$ unless we explicitly choose an embedding (see below).</li> </ul> <hr/> <p>Now, let’s take a look at the simple example: Sphere \(S^2\) and spherical coordinates. Given the manifold \(S^2 = \{(x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}\)</p> <p>Coordinate chart (away from poles)</p> <p>\((\theta, \varphi)\) with embedding map \(F(\theta,\varphi) = (\sin\varphi\cos\theta,\; \sin\varphi\sin\theta,\; \cos\varphi).\)</p> <p>Here:</p> <ul> <li>\(\theta, \varphi\) are <strong>functions on \(S^2\)</strong>,</li> <li>not abstract variables.</li> </ul> <p>According to the above discussions, the <strong>intrinsic</strong> meaning of \(\partial / \partial \theta\) and \(\partial / \partial \varphi\) are the following:</p> <p>For any \(f : S^2 \to \mathbb{R}\), \(\frac{\partial}{\partial \theta}(f) = \frac{\partial}{\partial \theta} \big(f \circ \varphi^{-1}\big)(\theta,\varphi), \quad \frac{\partial}{\partial \varphi}(f) = \frac{\partial}{\partial \varphi} \big(f \circ \varphi^{-1}\big).\)</p> <p>Again, this is the <strong>definition</strong>, not an interpretation. This offers us one way to calculate the vector field/tangent vectors at $\forall p \in M$.</p> <p>If we set</p> <p>\(f(x, y, z) = z\), then we could pull it back and obtain \((f \circ \phi^{-1})(\theta, \phi) = \cos(\phi)\). Consequently,</p> <blockquote> <p>Example 1: $ X = \partial/\partial\theta $</p> </blockquote> \[X_p(f)= \frac{\partial \cos(\phi)}{\partial \theta} = 0\] <p>at $p = (\theta, \phi)$.</p> <hr/> <blockquote> <p>Example 2: $ Y = \partial/\partial\varphi $</p> </blockquote> \[Y_p(f)= \frac{\partial \cos(\phi)}{\partial \phi} = -\sin(\phi)\] <p>at $p = (\theta, \phi)$.</p> <hr/> <p>However, because \(S^2 \subset \mathbb{R}^3\), we may compute concrete representatives by viewing it as an embedded in $\mathbb{R}^3$ and to extrinsically compute the vector fields by pushforward of coordinate basis vectors via the embedding map of the manifold. Consequently, such computation lives in the embedded picture, not the intrinsic definition.</p> \[\frac{\partial}{\partial \theta} = \frac{\partial F}{\partial \theta} = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0)\] \[\frac{\partial}{\partial \varphi} = \frac{\partial F}{\partial \varphi}= (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi)\] <p>These are <strong>actual vectors in \(\mathbb{R}^3\)</strong> tangent to \(S^2\).</p> <blockquote> <p>Important:<br/> This is <strong>not the definition</strong> of coordinate vector fields —<br/> it is a <strong>representation</strong> using the embedding.</p> </blockquote> <p>Consequently, we could act on functions without pulling back to coordinate space:</p> <p>Let \(f(p) = z(p)\) be a function on \(S^2\).</p> <p>Choose an extension \(\tilde f(x,y,z) = z \quad\Rightarrow\quad \nabla \tilde f = (0,0,1).\)</p> <p>Here’s the key observation:</p> <blockquote> <p>Once a tangent vector is represented in \(\mathbb{R}^3\), its action on \(f\) is given by the directional derivative of an extension of \(f\).</p> </blockquote> <p>Formally, \(\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\) where</p> <ul> <li> \[\tilde{f}$ is any smooth extension of $f$ to $\mathbb{R}^3\] </li> <li>\(X_p \in T_pS^2 \subset \mathbb{R}^3\).</li> </ul> <p>This works because tangent vectors annihilate normal components.</p> <hr/> <blockquote> <p>Example 1: \(X = \partial/\partial\theta\)</p> </blockquote> \[X(f)= (0,0,1) \cdot (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\; 0) = 0\] <hr/> <blockquote> <p>Example 2: \(Y = \partial/\partial\varphi\)</p> </blockquote> \[Y(f)= (0,0,1) \cdot (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\; -\sin\varphi) = -\sin\varphi\] <p>Same results as the intrinsic definition.</p> <p>Note that to be very concrete, in the above examples, at $p = (\theta, \phi)$, the tangent vector is:</p> <p>\(X_p = (-\sin\varphi\sin\theta,\; \sin\varphi\cos\theta,\;0)\) (we could interpret this as horizontal circles of latitude) and \(Y_p = (\cos\varphi\cos\theta,\; \cos\varphi\sin\theta,\ -\sin\varphi)\). This should make it very clear that we are treating the tangent vectors as actually “vectors” living in $\mathbb{R}^3$ like how we usually refer them to be. With this extrinsic embedding, the general form of a vector field coincides with the intrinsic notion, but more tangible:</p> <p>\(X = a(\theta, \phi)\frac{\partial}{\partial \theta} + b(\theta, \phi)\frac{\partial}{\partial \phi}\),</p> <p>then as a vector in \(\mathbb{R}^3\): \(X_p = a \frac{\partial F}{\partial \theta} + b\frac{\partial F}{\partial \phi}\), and</p> \[X_p(f) = \nabla \tilde{f}(p) \cdot X_p\] <p>Notice that for this computation no coordinate pullback is required. But to be crystal clear, we do <strong>not</strong> compute the vector field “without coordinates”. We simply You replaced intrinsic coordinates on $\mathbb{S}^2$ by ambient Cartesian coordinates in $\mathbb{R}^3$. So coordinates are still there — just in a different space.</p> <hr/> <p>Before we continue, let’s dwell on this formula for a while:</p> \[\boxed{ X_p(f) = \nabla \tilde f(p) \cdot X_p }\] <p>This identity guarantees consistency between 1] the derivation definition and 2] the embedded vector representation (Question: is this just chain rule?s). It explains why the extrinsic calculation agrees with the intrinsic definition. If we have a nice embedding, two views are nice. However, if we go to the general abstract case without the ambient space, we could only assort to the intrinsic definition.</p> <hr/> <p>To be rigorous, you may be concerned about the specific use of the function extension here. However, we could show that extensions do not matter, because <strong>“tangent vectors annihilate normal components”</strong>. Let me explain below:</p> <p>If \(g : \mathbb{R}^3 \to \mathbb{R}\) satisfies \(g|_{S^2} = 0,\) then for all \(v \in T_p S^2\), \(v(g) = 0.\) Equivalently, \(\nabla g(p) \perp T_p\mathbb{S}^2\)</p> <hr/> <h3 id="proof-via-curves">Proof (via curves):</h3> <p>Let \(\gamma(t) \subset S^2\) with \(\gamma(0)=p, \quad \gamma'(0)=v.\)</p> <p>Since \(g(\gamma(t)) = 0\) for all \(t\), \(v(g) = \frac{d}{dt} g(\gamma(t))\big|_{t=0} = 0.\)</p> <hr/> <h3 id="concrete-example">Concrete example</h3> <p>Let \(g(x,y,z) = x^2 + y^2 + z^2 - 1.\)</p> <p>Then: \(\nabla g = (2x,2y,2z)\) which is normal to \(S^2\).</p> <p>For any tangent vector \(v\), \(v(g) = \nabla g \cdot v = 0.\)</p> <hr/> <h3 id="consequence-crucial">Consequence (crucial)</h3> <p>If \(\tilde f_1\) and \(\tilde f_2\) are two extensions of \(f\), then \(g = \tilde{f_1} - \tilde{f_2}\) vanishes on $\mathbb{S}^2$. For any tangent vector $v$:</p> \[v(\tilde f_1) - v(\tilde f_2) = v(\tilde g) = 0\] <p>thus:</p> \[v(\tilde f_1) = v(\tilde f_2) \quad\forall v \in T_p S^2.\] <p>Hence directional derivatives along tangent vectors are <strong>well-defined</strong>: this is why directional derivatives along tangent vectors do not depend on how f extends off the manifold.</p> <p>View from another perspective, this indicates that a function that is identically zero on the manifold cannot change when we move tangentially: so tangent vectors “don’t see” normal variations. What I mean by the former statement “Tangent vectors annihilate normal components” is that derivatives along tangent directions ignore any part of a function that only varies off the manifold, because those variations are invisible to curves that stay on the manifold.</p> <hr/> <h3 id="intrinsic-vs-extrinsic-viewpoints-summary">Intrinsic vs Extrinsic Viewpoints (Summary)</h3> <table> <thead> <tr> <th>Aspect</th> <th>Intrinsic</th> <th>Extrinsic</th> </tr> </thead> <tbody> <tr> <td>Tangent vector</td> <td>Derivation</td> <td>Vector in \(\mathbb{R}^3\)</td> </tr> <tr> <td>Definition</td> <td>Via charts</td> <td>Via embedding</td> </tr> <tr> <td>Computation</td> <td>Pullback</td> <td>Directional derivative</td> </tr> <tr> <td>Dependence</td> <td>Coordinate-dependent</td> <td>Embedding-dependent</td> </tr> </tbody> </table> <p>Both viewpoints are <strong>equivalent</strong> when an embedding exists.</p> <hr/> <p>Something to note from the above example:</p> <blockquote> <p>Coordinate vector fields are <strong>defined intrinsically as derivations</strong>,<br/> may be <strong>represented extrinsically</strong> using embeddings,<br/> and act on functions in a way that is <strong>independent of extensions</strong> because tangent vectors annihilate normal components.</p> </blockquote> <p>Through this simple example we already develop a taste of the difference between intrinsic and extrinsic geometry, the abstract definitions and concrete calculations. Such theme will be recurring for our journey in studying differential geometry.</p> <p>One of the advantages of the intrinsic view is that it is independent of coordinate charts. It paves the ground for explaining why</p> <blockquote> <p><strong>Gradients, geodesics, and Lie brackets make sense without embedding (or ever mentioning) the abstract manifold into $\mathbb{R}^n$</strong>.</p> </blockquote> <h2 id="4-discussions">4] Discussions</h2> <p>Firstly, note that tangent/cotangent space is an intrinsic? property. A usual image of interpreting tangent space is \(S^2\) embedded in \(\mathbb{R}^3\). Our definition above does not require so, and in fact the shift of perspective from extrinsic into intrinsic properties of geometric objects is a grand evolution starting from Gauss and Rieman.</p> <p>[TODO: An illustrative figure of a sphere embedded in Euclidean space]</p> <p>From derivations to curves:</p> <h3 id="41-abuse-of-notation">4.1] Abuse of notation</h3> <p>Many textbooks will carry abuse of notation throughout their content, most often conflating functions/derivations defined on the manifold with them composed with local coordinates. I find it really confusing and hard to interpret many times, so I decide to, once again, present one simple example where no abuse of notation whatsoever happens, so we could get it crystal clear. Let’s begin.</p> <hr/> <h4 id="411-clear-statement">4.1.1] Clear Statement</h4> <p>Let:</p> <ul> <li>\(M\) be a smooth manifold.</li> <li>\(c : I \subset \mathbb{R} \to M\) be a smooth curve.</li> <li>\((U,\varphi)\) be a coordinate chart with<br/> \(\varphi : U \to \mathbb{R}^n\)</li> <li>Assume \(c(t_0) \in U\).</li> </ul> <p>Then the tangent vector \(c'(t_0) \in T_{c(t_0)}M\) satisfies:</p> \[c'(t_0) = \sum_{i=1}^n \left. \frac{d}{dt}(x^i \circ c)(t) \right|_{t_0} \; \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)}\] <p>where all objects are defined precisely below.</p> <hr/> <h4 id="412-definition-of-the-tangent-vector-to-a-curve">4.1.2] Definition of the Tangent Vector to a Curve</h4> <p>Fix \(t_0 \in I\).</p> <p>The tangent vector \(c'(t_0) \in T_{c(t_0)}M\) is defined as the derivation:</p> \[c'(t_0)(f) := \left. \frac{d}{dt}(f \circ c)(t) \right|_{t=t_0}\] <p>for every smooth function \(f \in C^\infty(M)\). Notice that this is the <strong>definition</strong> of the velocity vector, and there’s no coordinates are used here.</p> <hr/> <h4 id="413-introduce-a-coordinate-chart">4.1.3] Introduce a Coordinate Chart</h4> <p>Let \((U,\varphi)\) be a coordinate chart with:</p> <ul> <li> \[c(t_0) \in U\] </li> <li> \[\varphi: U \to \mathbb{R}^n\] </li> </ul> <p>Write:</p> \[\varphi(p) = (x^1(p),\dots,x^n(p))\] <p>where</p> \[x^i := \pi^i \circ \varphi\] <p>and \(\pi^i : \mathbb{R}^n \to \mathbb{R}\) is the standard projection.</p> <p>Thus each coordinate function</p> \[x^i : U \to \mathbb{R}\] <p>is smooth.</p> <hr/> <h4 id="414-coordinate-basis-of-the-tangent-space">4.1.4] Coordinate Basis of the Tangent Space</h4> <p>For each \(p \in U\), define the coordinate vector:</p> \[\left.\frac{\partial}{\partial x^i}\right|_p\] <p>as the derivation:</p> \[\left.\frac{\partial}{\partial x^i}\right|_p (f) := \left. \frac{\partial}{\partial u^i} \Big( f \circ \varphi^{-1} \Big) (u) \right|_{u=\varphi(p)}\] <p>for all smooth functions \(f\) defined near \(p\).</p> <p>This definition proceeds in three steps:</p> <ol> <li>Pull \(f\) back to \(\mathbb{R}^n\) via \(\varphi^{-1}\)</li> <li>Take the ordinary partial derivative</li> <li>Evaluate at the coordinate point \(\varphi(p)\)</li> </ol> <p>These vectors form a basis of \(T_pM\), as we know from above.</p> <hr/> <h4 id="415-express-the-curve-in-coordinates">4.1.5] Express the Curve in Coordinates</h4> <p>Define the coordinate representation of the curve:</p> \[\gamma := \varphi \circ c\] <p>Thus:</p> \[\gamma : I \to \mathbb{R}^n\] <p>Write:</p> \[\gamma(t) = (\gamma^1(t),\dots,\gamma^n(t))\] <p>where</p> \[\gamma^i(t) = \pi^i(\gamma(t)) = \pi^i(\varphi(c(t))) = (x^i \circ c)(t)\] <p>So:</p> \[\gamma^i = x^i \circ c\] <p>These are ordinary real-valued functions.</p> <hr/> <h4 id="416-compute-the-action-of-ct_0">4.1.6] Compute the Action of \(c'(t_0)\)</h4> <p>Let \(f \in C^\infty(M)\).</p> <p>By definition:</p> \[c'(t_0)(f) = \left. \frac{d}{dt}(f \circ c)(t) \right|_{t_0}\] <p>Insert identity:</p> \[f \circ c = (f \circ \varphi^{-1}) \circ (\varphi \circ c)\] <p>Define:</p> \[F := f \circ \varphi^{-1}\] <p>Then:</p> \[c'(t_0)(f) = \left. \frac{d}{dt} F(\gamma(t)) \right|_{t_0}\] <hr/> <h4 id="417-apply-the-multivariable-chain-rule-in-mathbbrn">4.1.7] Apply the Multivariable Chain Rule in \(\mathbb{R}^n\)</h4> <p>Since everything is now in \(\mathbb{R}^n\), apply the usual chain rule:</p> \[\frac{d}{dt}F(\gamma(t)) = \sum_{i=1}^n \frac{\partial F}{\partial u^i}(\gamma(t)) \, \frac{d\gamma^i}{dt}(t)\] <p>Evaluating at \(t_0\) gives:</p> \[c'(t_0)(f) = \sum_{i=1}^n \frac{d\gamma^i}{dt}(t_0) \, \frac{\partial F}{\partial u^i} (\gamma(t_0))\] <hr/> <h4 id="418-return-to-manifold-language">4.1.8] Return to Manifold Language</h4> <p>Recall:</p> <ul> <li> \[\gamma(t_0)=\varphi(c(t_0))\] </li> <li> \[F = f \circ \varphi^{-1}\] </li> </ul> <p>Thus:</p> \[\frac{\partial F}{\partial u^i}(\gamma(t_0)) = \left. \frac{\partial}{\partial u^i} (f \circ \varphi^{-1}) \right|_{u=\varphi(c(t_0))}\] <p>By definition of the coordinate vector fields:</p> \[= \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)}(f)\] <p>Therefore:</p> \[c'(t_0)(f) = \sum_{i=1}^n \frac{d\gamma^i}{dt}(t_0) \, \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)}(f)\] <p>Since this equality holds for every smooth function \(f\), we conclude:</p> \[c'(t_0) = \sum_{i=1}^n \frac{d\gamma^i}{dt}(t_0) \, \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)}\] <p>Finally substitute:</p> \[\gamma^i = x^i \circ c\] <p>so:</p> \[\frac{d\gamma^i}{dt}(t_0) = \frac{d}{dt}(x^i \circ c)(t_0)\] <hr/> <h4 id="419-final-fully-precise-formula-and-summary">4.1.9] Final Fully Precise Formula and Summary</h4> \[\boxed{ c'(t_0) = \sum_{i=1}^n \left. \frac{d}{dt}(x^i \circ c)(t) \right|_{t_0} \; \left. \frac{\partial}{\partial x^i} \right|_{c(t_0)} }\] <p>In summary, the above elaboration consists of:</p> <ol> <li>Pulling everything to \(\mathbb{R}^n\)</li> <li>Applying the ordinary multivariable chain rule</li> <li>Translating back via the definition of coordinate vector fields</li> </ol> <p>Thus, the coordinate representation of the velocity vector is precisely the multivariable chain rule expressed in intrinsic manifold language.</p> <hr/> <h3 id="42-orthonormal-basis">4.2] Orthonormal basis?</h3> <p>Another subtle point that haunted me for while is the questions of whether the basis \(\left\{\frac{\partial}{\partial x_i}\Big|_p\right\}_{i=1,2,...,n}\) are orthonormal or not.</p> <p>The short answer is No. The coordinate basis is <strong>not necessarily orthonormal</strong>. It’s true that these vectors:</p> <ul> <li>Form a basis of \(T_pM\).</li> <li>Depend on the coordinate choice.</li> <li>Are defined geometrically as directional derivatives.</li> </ul> <p>However, nothing in their definition guarantees that they are orthogonal or normalized. In fact, even more basic, orthonormality requires a <strong>metric</strong>! To even ask whether a basis is orthonormal, we must have a <strong>Riemannian metric</strong> \(g\) (will talk about this in details later; readers not familiar with metric/Riemannian metric feel free to please come back later; I include this short discussion for completeness and I apologize for grabbing future yet-to-be-learned knowledge as part of this section’s story). Without a metric, we cannot measure angles and lengths, and thus “orthonormal” is meaningless.</p> <p>But even given a Riemannian metric \(g\), usually orthonormality is not guaranteed. We can easily compute</p> \[g_{ij}(p) = g\!\left(\frac{\partial}{\partial x_i}, \frac{\partial}{\partial x_j}\right)_p.\] <p>In general,</p> \[g_{ij}(p) \neq \delta_{ij}.\] <p>So the coordinate basis is usually <strong>not orthonormal</strong>.</p> <p>Let’s do a simlpe example: the sphere. Consider the 2-sphere \(S^2\) with spherical coordinates \((\theta, \phi)\).</p> <p>Given a metric:</p> \[ds^2 = d\theta^2 + \sin^2\theta \, d\phi^2.\] <p>Then:</p> \[g\left(\frac{\partial}{\partial \theta}, \frac{\partial}{\partial \theta}\right) = 1,\] \[g\left(\frac{\partial}{\partial \phi}, \frac{\partial}{\partial \phi}\right) = \sin^2\theta.\] <p>These coordinate vectors are indeed:</p> <ul> <li>Orthogonal.</li> <li>But <strong>not normalized</strong> unless \(\sin\theta = 1\).</li> </ul> <p>Even in nice coordinates, the basis is typically not orthonormal.</p> <p>So then when are they orthonormal? As said above, they are orthonormal only if</p> \[g_{ij}(p) = \delta_{ij}.\] <p>This happens in special cases, like in</p> <ul> <li>Case 1: Euclidean Space. In \(\mathbb{R}^n\) with the standard metric,</li> </ul> \[g_{ij} = \delta_{ij},\] <p>so the coordinate basis is orthonormal everywhere.</p> <ul> <li>Case 2: Riemannian Normal Coordinates: On any Riemannian manifold, we can construct <strong>normal coordinates</strong> around \(p\) such that</li> </ul> \[g_{ij}(p) = \delta_{ij}.\] <p>However, this holds only at the single point \(p\). Away from \(p\), the metric is no longer Euclidean. On a side note, this reflects a deep geometric fact:</p> <blockquote> <p>Every Riemannian manifold looks Euclidean to first order at a point.</p> </blockquote> <p>To do a short summary and comparison:</p> <p>Coordinate basis vectors:</p> <ul> <li>Depend on how we label the manifold.</li> <li>Reflect the coordinate grid.</li> <li>May stretch or skew according to the geometry.</li> </ul> <p>An orthonormal basis instead:</p> <ul> <li>Is chosen using the <strong>metric</strong>.</li> <li>Is independent of coordinates.</li> <li>Can be obtained via Gram–Schmidt if needed.</li> </ul>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Intuition and definition of tangent/cotangent space]]></summary></entry><entry><title type="html">The Dance of Space: Geom/Topo/Dynam Mumble(2) (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/" rel="alternate" type="text/html" title="The Dance of Space: Geom/Topo/Dynam Mumble(2) (in progress)"/><published>2025-09-17T17:49:33+00:00</published><updated>2025-09-17T17:49:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Today I will cover a beautiful subject in differenetial geometry: Ricci flows and Ricci curvature.</p> <p>Generally, in Riemannian geometry, curvature measures how space bends. For instance, on a sphere, geodesics (shortest paths) come closer together compared to flat space; on a hyperbolic surface, they diverge.</p> <p>Ricci curvature is a particular way of summarizing curvature: Instead of describing how all directions bend (that’s what the full <strong>Riemann curvature tensor</strong> does), Ricci curvature focuses on <strong>volume distortion</strong>. More concretely, it tells us how the volume of a small geodesic ball deviates from the volume we’d expect in flat Euclidean space.</p> <p>Intuitively, we could summarize into the following:</p> <blockquote> <p>Positive Ricci curvature (like on a sphere) means geodesics tend to converge, and small balls have less volume than in flat space.</p> <p>Zero Ricci curvature (like in Euclidean space) means geodesics neither converge nor diverge, so volumes match Euclidean.</p> <p>Negative Ricci curvature (like on a hyperbolic space) means geodesics diverge, so small balls have more volume than Euclidean.</p> </blockquote> <p>Mathematically, Ricci curvature is obtained by “tracing” the <strong>Riemann curvature tensor</strong>. It compresses information about how different directions curve into a symmetric 2-tensor <code class="language-plaintext highlighter-rouge">Ric</code>.</p> <h3 id="riemannian-geometry-and-tensor">Riemannian Geometry and Tensor</h3> <p>The Riemann tensor is written in the following way:</p> \[R(X, Y)Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X, Y]} Z\] <p>This tensor captures all information about curvature. Succinctly, this is a 4-tensor: \(R_{ijkl}\).</p> <p>The above is an unfair treatment of Riemannian geometry. I’ll have a separate blog on that subject soon.</p> <p>How to understand: Riemannian metric tensor informs the manifold where to expand, shrink, and curve. How does Riemannian metric tensor relate with curvature?</p> <h3 id="ricci-curvature">Ricci Curvature</h3> <p>Based on the Riemann tensor, what is the curvature?</p> <p>To get Ricci curvature, we take a <strong>trace</strong> of the Riemann tensor:</p> \[Ric_{ij} = R^{k}_{ikj} = g^{kl}R_{kilj}\] <p>This reduces the information down to a 2-tensor (like the metric itself). Geometrically, this represents the volume distortion of geodesic balls.</p> <h3 id="ricci-flow">Ricci Flow</h3> <p>Introduced by <code class="language-plaintext highlighter-rouge">Richard Hamilton (1982)</code>, Ricci flow is a process that evolves a Riemannian metric \(g(t)\) over time:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>The factor -2 is just convention (to simplify later computations). We could think of this as a heat equation for geometry: Just as heat diffuses to smooth out temperature differences, Ricci flow smooths out <strong>irregularities</strong> in curvature. As illustrated in the above section of Ricci curvature, we could naturally arrive at the result that positive curvature regions tend to shrink and negative curvature regions expand. Over time, the underlying geometry becomes more “regular”, like ironing out wrinkles.</p> <p>The effect of Ricci flow on curvature could be expressed in the following way. The derivative of scalar curvature \(R\) under this flow is:</p> \[\frac{\partial R}{\partial t} = \Delta R + 2 |Ric|^2\] <p>This resembles a heat equation $(\Delta R)$ plus a positive correction. Consequently, curvature tends to diffuse out but also grows in positive-curvature regions.</p> <h4 id="examples-of-ricci-flow">Examples of Ricci Flow</h4> <p>Perhaps the simplest example is to imagine how a sphere evolves under Ricci flow. We all know that a round sphere has positive Ricci curvature. The Ricci flow equation says:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>Since Ricci is positive, the metric shrinks, and thus makes the sphere to contract uniformly, eventually collapsing to a point. This closely mirrors the idea that positive curvature makes geodesics converge. Under the Ricci flow, it tightens further.</p> <p>Another simple example is the flat torus. Since the Ricci curvature is 0 everywhere,</p> \[\frac{\partial g_{ij}}{\partial t} = 0\] <p>the torus will stay unchanged after the flow forever. This is analogous to heat diffusion on a perfectly uniform temperature field where nothing would effectively changes.</p> <p>Likewise, a hyperbolic surface has negative Ricci curvature. Consequently, under the Ricci flow, the metric would expand and the hyperbolic surface would grow larger and more uniform in curvature.</p> <p>In a nutshell, irregular geometries with bumps or folds (different curvature in different regions) get “smoothed” over time. All the high-curvature “wrinkles” would get flatten out, like how heat equalizes temperature.</p> <h3 id="application-of-ricci-flow">Application of Ricci Flow</h3> <p>Poincare conjecture, Ricci flow, surgery theory, what Terrence Tao called “one of the most impressive recent achievements of modern mathematics”</p> <p>Poincare conjecture:</p> <blockquote> <p>Any closed 3-manifold that is simply-connected, compact, and boundless is homeomorphic to a 3-sphere.</p> </blockquote> <p>Specifically, Poincare conjecture in higher dimensions has been solved around 1961, and dimension 4 case has been proved by Michael Freedman who by which won Fields medal in 1986. The \(n = 3\) case seemed really difficult to crack and it was only at 2002 that Grisha Perelman proved it using Ricci flow.</p> <p>Very briefly, since a sphere has positive curvature, by applying Ricci flow through time such sphere will contract and eventually vanish. Perelman proved the opposite also holds: if metric goes to 0, it must have been a sphere. To prove Poincare’s conjecture using Ricci flow,</p> <p>One of the most triumphant use of Ricci flow happens when Grigori Perelman (2002–2003) to prove <strong>the Poincaré conjecture</strong> and the more general <strong>Thurston geometrization conjecture</strong>. He showed how Ricci flow with “surgery” (cutting and patching when singularities form) classifies 3-manifolds.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Introduction of Ricci flows and Ricci curvatures]]></summary></entry><entry><title type="html">The Dance of Space: Geom/Topo/Dynam Mumble(3) (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/" rel="alternate" type="text/html" title="The Dance of Space: Geom/Topo/Dynam Mumble(3) (in progress)"/><published>2025-09-16T16:34:22+00:00</published><updated>2025-09-16T16:34:22+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Differential_Forms</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/"><![CDATA[<p>The second episode to appreciate the inner workings of space is through the differential forms. Differential forms is not an ancient subject, …</p> <h3 id="vector-outer-product">Vector Outer Product</h3> <h3 id="wedge-productexterior-derivative">Wedge Product/Exterior Derivative</h3> <h3 id="three-in-one">Three in One</h3> <p>Green’s theorem, Gauss’ theorem, Stoke’s theorem.</p> <p>Green’s theorem:</p> \[\int_{L} Pdx + Qdy = \iint_{D}(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y})dxdy\] <p>Generalized Stoke’s theorem.</p> <p>A k-form is supposed to be integrated over an oriented k-dimensional manifold</p> <h3 id="fundamental-theorem-of-calculus-ftoc">Fundamental Theorem of Calculus (FTOC)</h3> <p>High dimensional Stoke’s theorem is exactly the fundamental theorem of calculus (FTOC) in high-dimensional space.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Intuite and define differential forms]]></summary></entry><entry><title type="html">Equivalence: What does “being equal” represent? (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Equivalence/" rel="alternate" type="text/html" title="Equivalence: What does “being equal” represent? (in progress)"/><published>2025-09-07T00:23:16+00:00</published><updated>2025-09-07T00:23:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Equivalence</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Equivalence/"><![CDATA[<p>Explain and compare multiple equivalences from differential geometry and topology, including homeomorphism, diffeomorphism, homotopy equivalence, homomorphism, isomorphism, etc.</p> <p>Invariant and equivariant functions (CNN is equivariant).</p> <p>Also discussions on cardinality among sets (including finite and infinite (countably infinite \(N0\)? and uncountably infinite), Hillbert Hotel problem, equipotent sets)</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Topology"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Compare various equivalences in geometry/topology/group theory]]></summary></entry></feed>