<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jasmineruixiang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasmineruixiang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-29T06:02:06+00:00</updated><id>https://jasmineruixiang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Differentiable Manifold (1): Rigorous definition (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(1)/" rel="alternate" type="text/html" title="Differentiable Manifold (1): Rigorous definition (in progress)"/><published>2025-08-29T02:32:56+00:00</published><updated>2025-08-29T02:32:56+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(1)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(1)/"><![CDATA[<p>A rigorous definition of manifold, and a general introduction to its properties and different categories.</p> <p>Set the basis for further rigorous discussions.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Rigorous definition of manifold and related topology concepts]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble (1): Exponential map (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Exponential-map/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble (1): Exponential map (in progress)"/><published>2025-08-29T02:25:33+00:00</published><updated>2025-08-29T02:25:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Exponential-map</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Exponential-map/"><![CDATA[<p>Exponential map as dynamical flow in differential geometry and dynamical systems.</p> <h1 id="differential-geoetry-lie-group">Differential geoetry (Lie group)</h1> <h1 id="dynamical-system-linear">Dynamical system (Linear)</h1>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Illustrate exponential maps applied in Lie group and dynamical systems]]></summary></entry><entry><title type="html">Learning and constraints (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Constraint-Learning/" rel="alternate" type="text/html" title="Learning and constraints (in progress)"/><published>2025-08-25T22:09:34+00:00</published><updated>2025-08-25T22:09:34+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Constraint-Learning</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Constraint-Learning/"><![CDATA[<p>This blog covers two papers, with the emphasis on the first to …</p> <p>Both spatial (neural manifold view) and temporal (dynamical system view) on constraints of neural activities.</p> <h1 id="spatial-constraints-sadtler-etal-2014">Spatial constraints (Sadtler et.al. 2014)</h1> <h2 id="experiment-setup">Experiment setup</h2> <p>For <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>, two male Rhesus macaques were trained to perform closed-loop BCI cursor task (Radial 8). Around 85-91 neural units (threshold-crossings) were recorded. The experiment pipeline is demonstrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1a.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 1a in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. Note that on the right hand side the authors already presented where to place the two kinds of perturbation. The color scheme (green, yellow, and black) is consistent throughout figures in this paper. </div> <h2 id="decoding-paradigm">Decoding paradigm</h2> <h3 id="dimensionality-reduction-technique">Dimensionality reduction technique</h3> <p>The control space is just 2D because the decoder output is cursor velocities in \(\mathbb{R}^2\), illustrated as black line in Fig.2 black line (Note: it’s actually a 2D plane, but here for simplicity shown as a black line (\(\mathbb{R}^1\))).</p> <p>They used Factor Analysis ({cite (Factor-analysis methods for higher-performance neural prostheses) }{cite (Gaussian Process Factor analysis)}; I’ll write a blog on GPFA later) to extract what they called the “intrinsic manifold”, which captures the co-modulation patterns among the recorded neural population. This is shown as the underlying yellow plane in Fig.2 (might be confusing, but it’s not the 2D control space). Note that at the time of publication, neural manifold was not yet in a popular trend, so the authors briefly characterized the term “intrinsic manifold” with the following illustration:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1b.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 1b in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. . </div> <p>The authors further elaborated on the intrinsic manifold and its associated dimensionality at the end of the paper. For consistency of comparisons, Sadtler et. al. used a <strong>linear</strong> 10-D intrinsic manifold across all days. They then performed some offline analysis to explore if 10 is a legitimate choice. Specifically, they estimated the intrinsic dimensionality (EID) as the peak of maximal cross-validated log-likelihood (LL). In panel a the vertical bars represent the standard error of LL from across 4 cross-validation folds. Panel b. shows EID for all days and both 2 monkeys. The authors also showed the LL difference between 10D manifold vs manifold with EID (panel c., with units being the the number of standard errors of LL for the EID model). From panel c. the authors observed that 89% of days the 10-D manifold only differs within one standard error of LL with the EID manifold. Panel d. indicates the cumulative explained variance by the 10-D manifold. Notice that 10 dimensions already explained almost all neural variance.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_4.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.4 in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <p>The factor analysis method works in the following way (I’ll keep the same notation as the paper). Let’s assume the high dimensional neural signal (here the z-scored spike counts) acquired every 45ms time bin is denoted as \(u \in \mathbb{R}^{q \times 1}\) (naturally, \(q\) neural units), and \(z \in \mathbb{R}^{10}\) the latent variable. Factor analysis assumes the observed neural activity is related to the unobservable latent variables under a Gaussian distribution:</p> \[\begin{equation} u \mid z \sim N(\Lambda z + \mu, \psi) \end{equation}\] <p>where the latent vector is assumed to come from</p> \[\begin{equation} z \sim N(0, I) \end{equation}\] <p>Here the covariance matrix \(\psi\) is diagonal. Consequently, the intrinsic manifold is defined on the span of the columns of \(\Lambda\), and each column of \(\Lambda\) represents a latent dimension where \(z\) encodes the corresponding projections/coordinates. All three parameters \(\Lambda, \mu, \psi\) are estimated from <strong>Expectation-Maximization (EM)</strong> method (I’ll also write a blog about this later, especially how it as a classical inferenc engine is closely related to Evidence Lower Bound (<strong>ELBO</strong>), a populat loss/objective function for modern-day generative models based on DNN like VAE and Diffusion).</p> <h3 id="intuitive-mapping">Intuitive mapping</h3> <p>The intuitive mapping is selected by fitting a modified Kalman Filter ({cite Wu W. Gao Y., Bayesian population decoding of motor cortical activity using a Kalman filter}). Specifically, for each <strong>z-scored</strong> spike count \(z_t\), after obtaining the posterior mean \(\hat{z}_{t} = E[z_t \mid u_t]\) and <strong>z-scoring</strong> each dimension (these z-scorings are important, which will be stressed a multiple times later), the authors started with the common linear dynamical system (LDS) assumption of Kalman Filter:</p> \[\begin{align} x_t \mid x_{t-1} &amp;\sim N(Ax_{t-1} + b, Q) \\ \hat{z}_t \mid x_t &amp;\sim N(Cx_t + d, R) \end{align}\] <p>The parameters \(A,b,Q,C,d,R\) are obtained by maximum likelihood estimation, where \(x_t\) is the estimate of monkey’s intended velocity (label for the data). Since the spike counts and the latent factors were both <strong>z-scored</strong> and the calibration kinematics were centered, \(\mu = d = b = 0\).</p> <p>Consequently, by filtering the goal is to estimate \(\hat{x}_{t} = E[x_t \mid \hat{z}_{1}, \;, ... \;, \hat{z}_{t}]\). The authors directly gave out the formula below to express \(\hat{x}_t\) in terms of the final decoded velocity at the previous step \(\hat{x}_{t-1}\) and the current z-scored spike count \(u_{t}\):</p> \[\begin{equation} \hat{x}_t = M_1 \hat{x}_{t-1} + M_2 u_t \end{equation}\] \[\begin{equation} M_1 = A - KCA \end{equation}\] \[\begin{equation} M_2 = K\Sigma_{z}\beta \end{equation}\] \[\begin{equation} \beta = \Lambda^T(\Lambda \Lambda^T + \Psi)^{-1} \end{equation}\] <p>where \(K\) is the steady-state Kalman gain matrix. As part of the process of z-scoring the latent factors, \(\Sigma_z\) is a <strong>diagonal</strong> matrix whose diagonal element (\(p, p\)) refers to the inverse of standard deviation of the \(pth\) factor. Since both spike counts and latent factors are <strong>z-scored</strong>, the perturbed mappings (see in the next section) “would not require a neural unit to fire outside of its observed spike count range”.</p> <p>The above formula might sound confusing, so I present below a detailed derivation. It’s not so complicated but readers who are not interested in derivation feel free to skip it.</p> <h4 id="derivation-of-the-iterative-filtering-equation">Derivation of the iterative filtering equation</h4> <p>I’ll derive the above formula (5 - 8) in the following 3 steps. In the end, this is nothing brand new and elusive.</p> <blockquote> <p>Step 1: Obtain the posterior of \(z \mid u\)</p> <p>Step 2: z-score the latents</p> <p>Step 3: Apply Kalman filter</p> </blockquote> <h5 id="step-1-linear-gaussian-system">Step 1: Linear Gaussian system</h5> <p>I’ll start with a well-known fact about linear Gaussian system (this derivation is also the core of Gaussian Process and Kalman Filter; Stop for a second and marvel again at the all-encompassing power of Gaussian distribution). Assume two random vectors \(z \in \mathbb{R}^m\) and \(x \in \mathbb{R}^n\) which follow the Gaussian distribution:</p> \[\begin{equation} p(z) = N(z \mid \mu_z, \Sigma_z) \end{equation}\] \[\begin{equation} p(x \mid z) = N(x \mid Az + b, \Omega) \end{equation}\] <p>The above illustrates a <strong>linear Gaussian system</strong>. Note that \(A \in \mathbb{R}^{n \times m}\). Consequently, the correponsding joint distribution \(p(z, x) = p(z)p(x \mid z)\) is also a Gaussian with an \((m + n)\) dimensional random vector:</p> \[\begin{equation} p(z, x) = N( \begin{bmatrix} z \\ x \end{bmatrix} \mid \tilde{\mu}, \tilde{\Sigma}) \end{equation}\] <p>where</p> \[\begin{align} \tilde{\mu} &amp;= \begin{bmatrix} \mu_z \\ A\mu_z + b \end{bmatrix} \\ \tilde{\Sigma} &amp;= \begin{bmatrix} \Sigma_z &amp; \Sigma_z A^T \\ A\Sigma_z &amp; A\Sigma_z A^T + \Omega \end{bmatrix} \end{align}\] <p>The above could be easily derived from matching the corresponding moments, so I will not show in full details. From this joint Gaussian, we could thus easily continue to write out the posterior distribution:</p> \[\begin{equation} p(z \mid x) = N(z \mid \mu', \Sigma') \end{equation}\] <p>where</p> \[\begin{equation} \mu' = \mu_z + \Sigma_z A^T(A\Sigma_z A^T + \Omega)^{-1}(x - (A \mu_z + b)) \end{equation}\] \[\begin{equation} \Sigma' = \Sigma_z - \Sigma_z A^T(A\Sigma_z A^T + \Omega)^{-1}A\Sigma_z \end{equation}\] <p>The above posterior is known as <strong>Bayes’ rule for Gaussians</strong>. It states that if both the prior \(p(z)\) and the likelihood \(p(x \mid z)\) are Gaussian, so is the posterior \(p(z \mid x)\) (equivalently, Gaussian prior is a <strong>conjugate prior</strong> of Gaussian likelihood or Gaussians are <strong>closed under updating</strong>, <a class="citation" href="#pml2Book">(Murphy, 2023)</a> P29). One interesting fact is that although the posterior mean is a linear function of \(x\), the posterior covariance is entirely independent of \(x\). This is a peculiar property of Gaussian distribution (Interested readers please see more explanations in <a class="citation" href="#pml2Book">(Murphy, 2023)</a> sections 2.3.1.3, 2.3.2.1-2, and 8.2). Finally, keen readers might already perceive the equation (15,16) prelude the form of the Kalman Filter posterior update equations.</p> <p>From the above posterior Gaussian form, by plugging in the notations specified in (1-2) with \(z = z_t, \; x = u_t, \; \mu_z = 0, \; \Sigma_z = I \;, A = \Lambda, \; b = \mu \;, \Omega = \Psi\) we obtain the following:</p> \[\begin{equation} p(z_t \mid u_t) = N(z_t \mid \mu_{post}, \Sigma_{post}) \end{equation}\] <p>where</p> \[\begin{align} \mu_{post} &amp;= 0 + I \Lambda^T(\Psi + \Lambda I \Lambda^T)^{-1}(u_t - (\Lambda 0 + \mu)) \\ &amp;= \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}u_t \end{align}\] <p>and</p> \[\begin{align} \Sigma_{post} &amp;= I - I\Lambda^T(\Psi + \Lambda I \Lambda^T)^{-1}\Lambda I \\ &amp;= I - \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}\Lambda \end{align}\] <p>I deliberately used a different set of notations for the posterior linear Gaussian system, so if you are interested please do this little derivation on your own. Since \(\hat{z}_t = E[z_t \mid u_t]\), from above we know that</p> \[\begin{equation} \hat{z}_t = \Lambda(\Psi + \Lambda \Lambda^T)^{-1}u_t \end{equation}\] <h5 id="step-2-perform-z-scoring">Step 2: Perform z-scoring</h5> <p>The second step is to z-score the posterior mean \(\hat{z}_t\), which, here, is dividing each position of this vector by the corresponding standard deviation:</p> \[\begin{equation} \hat{z}_{t, z-scored} = \begin{bmatrix} \hat{z}_{t}^{1} / \sigma_{1} \\ \hat{z}_{t}^{2} / \sigma_{2} \\ \vdots \\ \hat{z}_{t}^{p} / \sigma_{p} \end{bmatrix} = \begin{bmatrix} \frac{1}{\sigma_1} &amp; 0 &amp; 0 &amp;\cdots &amp; 0 \\ 0 &amp; \frac{1}{\sigma_2} &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; &amp; \ddots &amp; &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \frac{1}{\sigma_{p}} \end{bmatrix} \hat{z}_t = \Sigma_z \hat{z}_t \end{equation}\] <p>For simplicity, for the below I’ll replace \(\hat{z}_{t, z-scored}\) with \(\hat{z}_t\).</p> <h5 id="step-3-apply-kalman-filter">Step 3: Apply Kalman Filter</h5> <p>As in Step 1, let me right down the filtering equation for a pure Kalman Filter based on the Gaussian linear assumption made in (9-10) (For the following I’ll drop off \(b, d\) since they are 0). The notations below might be a little messy, but I want to keep it rigorous. The hat on \(x, P\) refer to estimates not ground truth, and \(t \mid t-1\) indicates prior estimation while \(t \mid t\) indicates posterior estimation at time point \(t\).</p> \[\begin{align} \hat{x}_{t|t-1} &amp;= A\hat{x}_{t-1|t-1} \\ \hat{P}_{t|t-1} &amp;= A\hat{P}_{t-1|t-1}A^T + Q \\ K_t &amp;= \hat{P}_{t|t-1}C^T(C\hat{P}_{t|t-1}C^T + R)^{-1}\\ \hat{x}_{t|t} &amp;= \hat{x}_{t|t-1} + K_t(\hat{z}_{t} - C\hat{x}_{t|t-1}) \\ \hat{P}_{t|t} &amp;= (I - K_tC)\hat{P}_{t|t-1} \end{align}\] <p>Again, we play the trick of substitution, starting with (27):</p> \[\begin{align} \hat{x}_{t\mid t} &amp;= \hat{x}_{t \mid t-1} + K_t(\hat{z}_{t} - C\hat{x}_{t \mid t-1}) \\ &amp;= A\hat{x}_{t-1 \mid t-1} + K_t(\hat{z}_t - CA\hat{x}_{t-1 \mid t-1}) \\ &amp;= (A - K_tCA)\hat{x}_{t-1 \mid t-1} + K_t \hat{z}_t \\ &amp;= (A - K_tCA)\hat{x}_{t-1 \mid t-1} + K_t\Sigma_z( \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}u_t) \\ &amp;= (A - K_tCA)\hat{x}_{t-1 \mid t-1} + K_t\Sigma_z \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}u_t \end{align}\] <p>Finally, if we define \(M_1 = A - KCA\), \(M_2 = K\Sigma_z \beta\), \(\beta = \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}\) (formula (6-8)), we’d claim what we saw in (5):</p> \[\hat{x}_t = M_1 \hat{x}_{t-1} + M_2 u_t\] <p>What I wrote as \(\hat{x}_{t \mid t}\) is the posterior prediction which the authors denoted \(\hat{x}_{t}\) for simplicity (same logic for \(t-1\)). The only subtlety that remains here is that in the above derivation I used the dynamic Kalman Gain \(K_t\), calculated for every time point $t$. In the paper the authos utilized steady Kalman Gain (that’s why there’s no subscript \(t\)) but that’s basically iterate the above filtering equations many times with the given set of model parameters (\(A,Q,C,R\)) until \(K_t\) converges to some matrix \(K = \lim_{t \rightarrow \infty} K_t\), and then use that matrix for all time steps. Basically, you could just replace \(K\) with \(K_t\) without changing the backbone of the inference structure.</p> <p>Before we jump into the perturbation method, the formula (5) does inform us the central philosophy of Kalman Filter: it integrates model prediction by its specified linear dynamics and the observation together, to arrive at an optimal (I’ll not dive deep into what optimality represents here) posterior inference. Extracting out the linear relationship between prediction at timestep \(t\) and the previous step \(t-1\) together with the observation input would help understanding the perturbation method below.</p> <h2 id="perturbation-method">Perturbation method</h2> <p>Then the core methodology of this study is to change the BCI mapping so that the altered control space would be lying either within or outside of the insintric manifold. The paper does present some confusion as to how intuitive mapping and control space would be distinguished. My interpretation is that the control space refers to the ideal potential neural subspace for which to control the cursor optimally. Since within a short time neural connectivity is kept unaltered, the true intrinsic manifold is approximately invariant and thus the required potential neural subspace might not be reachable. By default the control space/intuitive mapping lies within the intrinsic manifold (that’s why it’s called “intuitive”, because that’s is what the neural network system has learned to achieve).</p> <p>The neuronal connectivity statistics is referred to as the natural co-modulation patterns.</p> <p>In short, a within-manifold perturbation only reoriented the control space such that it still resides in the intrinsic manifold (shown in Fig.3 red line). This does not require monkeys to readapt neuronal connectivity patterns to achieve such new control space. It only altered the function from the intrinsic manifold to cursor kinematics. On the other hand, an outside-manifold perturbation alters the control space allowing it to live off the intrinsic manifold (Fig.3 blue line). Notice that if such outside-manifold perturbation pushes the control space along the orthogonal subspace that passes through the original control space, then the mapping from the neural comodulation patterns to cursor kinematics is preserved (basically just project the altered control space to the intrinsic manifold, then would recover the original control space). However, the underlying comodulation/covariation patterns among the neural population are altered.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1c.png" class="img-fluid rounded z-depth-1" width="45%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.1c in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <p>After the perturbation, the authors observed if the monkeys could eventually learn to readapt to the new mapping, to achieve great cursor control performance. For within-manifold perturbation, the monkeys only need to learn to associate cursor kinemaitcs to a new set of neural comodulation patterns (still within reach because lying in the same intrinsic manifold). However, for outside-manifold perturbation, they had to generate new co-modulation patterns in order to reach outside of the existing intrinsic manifold. Consequently, the authors predicted that within-manifold perturbation is easier to learn compared to outside-manifold perturbation. The authors did find results to back up this claim and since they are not the main focus of this blog, I’ll refer interested readers to the original paper to take a look (FIgure 2).</p> <h3 id="perturbation-as-permutation">Perturbation as permutation</h3> <p>Other than that, I do want to dive deep into how such within/outside-manifold perturbations were implemented. Specifically, from the derivation section readers should be already familiar with equations (5-8), the intuitive mapping. The perturbed mappings are the corresponding modified versions.</p> <p>The within-manifold perturbation still manages to maintain the relationship between neural units to latent factors, but perturb that between latents and cursor kinematics: \(\hat{z}_{t}\) is permuted before going into the Kalman inference (multiplying with the Kalman Gain)(Figure 1 red). Since permutation is simply re-orientation, the within-manifold perturbation is equivalent to re-orientating the coordinate system within the manifold (the manifold is preserved because the row space of \(\Lambda^T\) is preserved (equation (7))). Mathematically,</p> \[\begin{align} \hat{x}_t = M_1 \hat{x}_{t-1} + M_{2, WM}u_t \\ M_{2, WM} = K\eta_{WM}\Sigma_z\beta \end{align}\] <p>and \(\eta_{WM}\) is a \(10 \times 10\) permutation matrix (10 because of the latent dimensionality)</p> <p>For outside-manifold perturbation, it changes the relationship between the neural units and latent factors by permuting \(u_t\) before passing it into factor analysis (Figure 1 blue). Specifically,</p> \[\begin{align} \hat{x}_t = M_1 \hat{x}_{t-1} + M_{2, OM}u_t \\ M_{2, OM} = K\Sigma_z\beta \eta_{OM} \end{align}\] <p>and \(\eta_{OM}\) is a \(q \times q\) permutation matrix (\(q\) is the number of neural units). The underlying logic is that in the <strong>neural space</strong> for \(u_t\), the monkeys might not be able to adapt to conteract the perturbation brought by permutation, since \(\eta_{OM}\) directly acts upon \(u_t\).</p> <h3 id="select-perturbed-mappings">Select perturbed mappings</h3> <p>The authors also devised a clever plan to dictate the specific permutation matrices to choose (for a permutation matrix with size \(k \times k\), there’re \(k!\) numbers of them) in three steps, with the central goal that the perturbed mapping would not be too difficult nor easy to learn:</p> <h4 id="step-1-find-the-candidate-set">Step 1: Find the candidate set</h4> <p>For within-manifold perturbations, all \(10!\) possible permutation matrices are treated as the candidate set. For outside-manifold perturbations, th strategy differs for two monkeys. For one monkey only permutations of nueral units with largetst modulation depths are selected. For the other monkey, the solution is to randomly put all units with the highest modulation depths into 10 groups of \(m\) each (the rest with low modulation forms an 11th group). The outside-manifold perturbation is formed by permutating these 10 groups instead of each unit (thus \(10!\) in total matching that of within-manifold perturbation).</p> <h4 id="step-2-open-loop-velocities-prediction-per--perturbation">Step 2: Open-loop velocities prediction per perturbation</h4> <p>The second step hinges on estimating the open-loop velocities for each candidate permutation. Specifically, it approximates the decoded cursor kinematics if the monkeys did not learn to adapt:</p> \[\begin{equation} x_{OL}^i = M_{2, P}u_{B}^i \end{equation}\] <p>where \(u_{B}^i\) is the mean z-scored spike counts across all trials on the \(i^{th}\) target(\(8\) in total), and \(P\) represents either \(OM\) or \(WM\). The method here echoes the dissection made explicity in equation(5), where the current step prediction \(\hat{x}_t\) is a linear combination of prediction from last step \(\hat{x}_{t-1}\) and neural activities at the present \(M_2 u_t\).</p> <h4 id="step-3-determine-potential-perturbations">Step 3: Determine potential perturbations</h4> <p>Finally, to determine a perturbation the authors compared the open-loop velocities under the perturbed mapping with those under the intuitive mapping for each target. These velocities should only differ in an acceptable range so the monkeys would not find it too simple nor difficult to learn. The authors quantified this metric by defining a range over differences in velocity angles and magnitude, and chose perturbations that fall in this specified range for all targets.</p> <h2 id="quantifiable-metric">Quantifiable metric</h2> <h3 id="amount-of-the-learning">Amount of the learning</h3> <p>To quantify the potential amount of learning under two perturbation kinds, the authors resorted primarily to two performance metric: (the change of) relative acquisition time and relative success rate across perturbation blocks. Specifically, as shown below, the black dot represents the intuitive mapping, while the red and blue dots indicate the imediate performance just after corresponding perturbations. Red and blue asterisks represent the best performance during the within the perturbation sessions. The dashed line indicates the maximum learning vector \(L_{max}\) (note that it starts on the red dot), and thus the aount of learning (\(A_i \in \mathbb{R}\)) is quantified as the length of the projection of the raw learning vector onto the maximum learning vector, normalized by the length of the maximum learning vector:</p> \[A_i = \frac{L_{raw, i} \cdot L_{max}}{\|L_{max}\|^2}\] <p>where \(i \in \{red, blue\}\). Pictorially, it’s illustrated as below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_2cd.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.2c and Fig.2d in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <p>Note that the asterisks in the above represent the time point/bin corresponding to <strong>maximal</strong> amount of learning. In real case, for each time bin the authors would pinpoint the end points for the learning vectors (for calculations in details, please refer to the METHODS section of the paper), and compute the amount of learning correspondingly (the red and blue learning vectors might end in different positions with a diferent set of relative acquisition time and success rate up to that time bin).</p> <p>The amount of learning for all sessions was presented above in the right pannel. Notice that a value of 1 indicates “complete” learning of the new relationship between the required neural co-modulation and cursor kinematics, reverting to the performance level of the intuitive control, while 0 indicates no learning. The authors did observe that there’s significant amount of learning for within-manifold perturbations than outside-manifold perturbation. To see changes in success rates and acquisition time during perturbation blocks, instead of a single metric \(L\) as shown above, the authors also plotted them separately as below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Ext_Fig_2.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.2 in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <h3 id="after-effects">After-effects</h3> <p>A second metric Sadtler et. al. employed is observe how the monkeys performed as they reintroduce the intuitive mapping, or the so-called after-effects after washout of the perturbed mapping. Specifically, the after-effect is measured as the amount of performance imparement (tentative: acquisition time, success rate) at the beginning at the wash-out block (like how impairement was measured at the beginning of a perturbation block). A large wash-out effect indicates that the monkeys have learned and adapted to the perturbed mapping. For within-manifold perturbation, the authors did observe brief impaired performance but not so for outside-manifold perturbation, indicating that learning did occur during the former.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Ext_Fig_3.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.3 in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a>. </div> <h3 id="principalcanonical-angles">Principal/canonical angles</h3> <p>To quantify the comparisons between the intuitive and the perturbed mappings, Sadtler et.al. also calculated the principal angles between two linear subspaces. Notice that by formula () above, both subspaces are spanned by the rows of \(M_2\) (\(M_{2, WM}\) for within-manifold perturbation, and \(M_{2, OM}\) for outside-manifold perturbation). Consuquently, the two principal angles specify the maximum and minimum angles of separation between the intuitive and the perturbed control spaces. Notice that since the spike counts are z-scored, the control spaces also center at the origin.</p> <p>Sidenote: How do principal angles relate with principal eigenvalues and principal curvatures?</p> <p>To give a short summary of principal angles calculation:</p> <p>Note the role of SVD decompsition.</p> <h2 id="discussions">Discussions</h2> <p>Since Sadtler et. al. employed closed-looop BCI control, they were able to causally alter the model/map from neural activities to the decoded cursor velocities.</p> <p>This paper highlights a potential methodology of BCI research: since the mapping from neural activities to control correlates is <strong>fully specified</strong>, thus could be causally perturbed to explore the corresponding changes of controlled behavior. This allowed the authors to <strong>design/know apriori</strong> the optimal/required neural activities (specified by the altered mapping) to achieve task success, and thus to observe if animals could generate such neural patterns.</p> <p>An outside-manifold perturbation does not necessarily specify that it lives in orthogonal subspace of the intrinsic manifold. Also, because here the intrinsic manifold is illustrated as a plane, in real world scenario, it is unlikely that it exists as a linear subspace. Consequently, specifying a space that is “orthogonal” to the potential manifold (nonlinear, other than a linear subspace) might be problematic.</p> <p>The amount of learning is entirely dependent upon performance itself, which is difficult to causally link to neural changes.</p> <p>For the amount of learning metric, why would 0 indicate no learning? Orthogonal learning?</p> <p>Notice that the after-effects analysis echoed a lot of research methodology in force-field or curl-field perturbation for cursor control in monkey motor control studies.</p> <p>The authors also showed that learning did not improve through sessions (readers might refer to Extended Figure 4 in <a class="citation" href="#Sadtler2014">(“Neural Constraints on Learning,” 2014)</a> for further information).</p> <p>The perspective and consideration that Sadtler et.al took to ensure alternative explanations for the observed distinction of learnability do not hold are informative. I enjoyed its rigorosity, especially when they considered perturbed maps which might be initialy difficult to learn and carefully implement the controls (demonstrate that they have controled). To not diverge from the main focus of this blog, I’ll not cover those dicussions. The way that the authors listed clearly alternative explanations and how they tackled each is a very inviting, powerful, and efficient way of writing.</p> <p>From the methods the authors used, they only estimated a linear manifold. According to {cite}, linear manfolds should require more dimensions than a nonlinear manifold which could explain similar amount of variance.</p> <p>The dissection of control into an estimation of intrinsic manifold (Factor Analysis) and then build an intuitive mapping (decoder, Kalman Filter) to relate the latent factors to cursor kinematics is different from directly mapping neural activities to movement. Such dissection becomes a common theme for the past two decades, with the emphasis on latent manifold structure beecomes increasingly popular. More than a theoretical refinement, practically speaking, such dissection also allows the authors to perform two different types of perturbation.</p> <p>Talk more here and also relate this to the nonlinear manifold paper 2025.</p> <p>For defining the candidate set of potential outside-manifold perturbation for the second monkey, while the group looping strategy is a clever way to equalize possible permutation matrices with within-manifold perturbation (\(10!\)), I’m not completely persuaded by the logic behind it. When explaning the logic for the second monkey outside-manifold perturbation, the authors stated that the within-maifold perturbation only permutes the neural space with \(10\) dimensions, while the outside-manifold perturbation on average deal with 39 dimensions (number of permuted units). Thus the monkey would have to search for more space for outside-manifold perturbation. I think the subtlety here lies in the fact that within-manifold perturbation is not performed on the level of the ambient neural space, but on the latent 10-dimensional space which is extracted out of the original neural space and each basis vector of the latent space is a <strong>linear combination</strong> of the neural units. Its is not explicitly clear to me whether/how these two spaces should be compared solely based on the dimensonality it carries.</p> <p>Additionally, though I do appreciate the flavor of group assignment (there’s even a flavor of group action here; anyway, permutation matrices form a permutation group), I am not sure if each of the \(10!\) permutations on groups of neural units is “equivalent” to a permutation among 10 latent axes.</p> <p>Another question is outside-manifold perturbation is not necessarily guaranteed to be outside the manifold?</p> <h3 id="required-changes-in-preferred-directions">Required changes in preferred directions</h3> <p>There’s one interesting method that I don’t have enough time to delve into, which is the calculation of changes in preferred directions for each neural unit. This is calculated to make sure that learning two perturbation types would require similar effort for the monkeys to adapt (interested readers might refer to Figure 3b in the paper). The authors began by discussing comparing the columns of \(M_2, \; M_{WM, or OM}\), which reflects how each neural unit impacts the decoded cursor kinematics. This strategy is not adopted because for the second monkey \(M_2\) and \(M_{OM}\) share many columns for the un-permuted columns, making it an unfair comparison. However, it’s informative if we associate this view of \(M_2 u_t\) by assiging each column of \(M_2\) with a coordinate in \(u_t\) with that mentioned in the principal angles section where rows of \(M_2\) represnet an axis upon which \(u_t\) is projected. These two perspectives which interpret linear matrix multiplication as either a <strong>transformation</strong> (of basis vectors in space) or a <strong>projection</strong> which will be further illustrated in an upcoming post.</p> <p>Then, the authors came up with another technique. They assumed that</p> <blockquote> <p>1] under perturbation the monkeys would still manage to keep the same cursor velocity in the intutive mapping,</p> <p>2] The perturbed firing rates should be as close as possible to those in the intuitive mapping</p> </blockquote> <p>and these two assumptions transform into a constrained optimization problem: Find \(u_p^{i}\) such that its Euclidean distance with \(u_B^{i}\) is minimized when \(M_2u_B^i = M_{2, P}u_p^i\):</p> <blockquote> \[u_p^{i, *} = \arg\min_{u_p^i} ||u_p^i - u_B^i||_2\] <p>s.t. \(M_2u_B^i = M_{2, P}u_p^i\)</p> </blockquote> <p>This can be solved in closed-form with Lagrange multipliers (for rigorosity, the inverse below should be replaced with the Moore-Penrose Pseudoinverse, unless \(M_{2, P}\) is full-rank, which I’m not sure I could theoretically make that claim). Due to limited space, I’ll not leave the proof to another blog on linear transformation, which I also illustrate its relationship with CCA and another paper (Juan Gallego, trajectory alignment):</p> \[\begin{equation} u_p^i = u_B^i + M_{2, P}^T(M_{2, P}M_{2, P}^T)^{-1}(M_2 - M_{2, P})u_B^i \end{equation}\] <p>where \(u_B^i\) is the mean normalized spike count vector across all trials for each target \(i\) in the basline blocks.</p> <p>Then the authors fit a standard cosine tuning model for each unit \(k\) with all targets:</p> \[\begin{equation} u_B^i(k) = m_k \cdot cos(\theta_i - \theta_B(k)) + b_k \end{equation}\] <p>where for each neural unit \(k\), its preferred direction is encoded as \(\theta_B(k)\), \(m_k\) the depth of modulation, \(b_k\) the model offset, \(\theta_i\) the direction of the \(ith\) target. Apply the same calculation for \(u_P^i\) to obtain the preferred direction \(\theta_P(k)\) for each unit \(k\) under the perturbaed mapping. Finally, the preferred direction changes (for each neural unit) is calculated as:</p> \[\begin{equation} \mid \theta_P(k) - \theta_B(k) \mid \end{equation}\] <h3 id="selection-of-intrinsic-dimensionality">Selection of intrinsic dimensionality</h3> <p>Usually we do not have a coherent and systematic way to detemrine the optimal intrinsic dimensionality. Here for factor analysis, based on its explicit probabilistic inference structure, the authors could easily compute the likelihood for cross-validated data.</p> <h3 id="measurement-of-cumulative-shared-variance">Measurement of cumulative shared variance</h3> <p>Based on equations (13), the original covariance of \(u\) is decomposed (with minor substitutions) into a shared component \(\Lambda \Lambda^T\) and an independent component \(\Psi\). In order to calculate the amount of shared variance along orthogonal directions within the manifold (notice this is a linear manifold), consequently, the authors calculated the eigenvalues of \(\Lambda \Lambda^T\) which present the shared variances, each corresponds to an orthonormalized latent dimension. This is similar to Churchland 2012 the last blog…</p> <h2 id="conclusions">Conclusions</h2> <p>The neural manifold reflects the inherent connectivity which constrains (in a short term) the potentially learnable patterns. Consequently, the neural connectivity network structure dictates possible neural patterns and corresponding behavior repertoire the animals are capable of performing.</p> <p>This paper strengthens my belief in the legit usability of the low dimensional structure among neural population, and more crucially the value of perturbation methods to causally verify the neural manifold. Specifically, the extraction of latent factors, other than directly mapping neural activties to cursor kinematics, not only adds more interpretability to the framework, but also provies a readily distinguishable strategy of within/outside-manifold perturbations. This reminds me of many other models with latent factors in between: (xxx, xxx, xxx, xxx).</p> <h1 id="dynamical-constraints-oby-etal-2025">Dynamical Constraints (Oby et.al. 2025)</h1> <h2 id="different-views-of-the-high-dimeensional-neural-space">Different views of the high dimeensional neural space</h2> <h2 id="task-1">Task 1:</h2> <h2 id="task-2-it-task">Task 2: IT task</h2> <h2 id="task-3-instructed-path-task">Task 3: Instructed path task</h2> <h2 id="discussions-1">Discussions</h2> <p>A dynamical system does not simply allow flowing back!</p> <h2 id="conclusions-1">Conclusions</h2> <h1 id="discussions-2">Discussions</h1> <p>These two studies offer powerful information that dimensionality reduction could be not just a visualization tool, but a causal summary of the underlying neural connectivity and anatomical constraints, which correlates to the neural computations that neural population could implement.</p> <p>Associating Sadtler 2014 with Churchland 2012, there’s a common convergence on using matrices to explore transformations. This again reinforces my idea that perhaps group theory needs to be rigorously introduced into neursocience for … (also associate with Barack and Kraukauer “Two views on the cognitive brain”, which relates <strong>computation</strong> to <strong>transformation of representations</strong> to explain cognitive phenomena)</p> <h1 id="conclusions-2">Conclusions</h1>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Learning"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Employ perturbations of neural manifold to explore learning constraints]]></summary></entry><entry><title type="html">Rotational dynamics in neural population</title><link href="https://jasmineruixiang.github.io/blog/2025/jPCA/" rel="alternate" type="text/html" title="Rotational dynamics in neural population"/><published>2025-08-16T18:30:16+00:00</published><updated>2025-08-16T18:30:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/jPCA</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/jPCA/"><![CDATA[<h2 id="preface">Preface</h2> <blockquote> <p>All changed, changed utterly:</p> <p>A terrible beauty is born.</p> <p>–––– William Butler Yeats, <em>Easter, 1916</em></p> </blockquote> <hr/> <p>Many years later, when I reflect back on the first quarter of the 21st century studies on computational neuroscience and brain-computer interface (BCI), among papers on fancy Neural Network based decoders and clinical breakthroughs expanding from cursor and motor BCI to speech and vision, I might still recollect a distant afternoon when I first heard the name Mark Churchland, and more specifically, neural population dynamics and jPCA. “Terrible beauty”, that’s the phrase which came to my mind at that time. Since then, such impression has taken its roots only deeper as research of similar flavor supported the dynamical system view from neural population.</p> <p>I perhaps first read the paper <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a> at least a decade after it got published, but the astonishing finding and the elegance of the algorithm, together with its implicit influences which reshaped my views on interpreting neural population and thus the currently suring word “emergence”, made it a classic enduring the test of time. This post is both a flashback and an exploration, where I extract a few key components from the article, specifically its dynamical systems interpretation and the jPCA algorithm itself, and raised a few questions and extended from the paper to some open queries in the end.</p> <h2 id="dynamical-systems-perspective">Dynamical systems perspective</h2> <p>A traditional perspective characeterizes neural activities from the primary motor cortex (M1) as representing the corresponding movement parameters. Equivalently, we could write out a parametric equation:</p> \[r_n(t) = f_n(param_{1}(t), param_{2}(t), param_{3}(t), ...)\] <p>where \(r_n(t)\) is the firing rate for the \(n_{th}\) neuron, tuned by the corresponding function \(f_n\). Alternatively, instead of a representational model, another perspective based on neural population encoding which reflects behavior parameters not on the single neuron level, but on the population level with a <strong>dynamical system</strong>, could be written as follows:</p> \[\dot{r}(t) = f(r(t)) + u(t)\] <p>Here \(f\) might represent a linear/nonlinear dynamical system, and \(u(t)\) is an unknown external input. In this view, the dynamics, i.e., the evolution of population response, encodes the movement parameters. Or put differently, within a dynamical system model, each single-unit response should reflect the “dynamical factors” exhibited from each latent state in the latent space, which we aim to identify from the observed high-dimensional neural pouplation recordings.</p> <h2 id="quasi-rthymic-responses">Quasi-rthymic responses</h2> <p>As made clear in the article, the critical finding of this study is that reaching, a non-oscillatory movement (unlike the swimming leech or a walking monkey), leads to a quasi-oscillatory neural trajectory. More surprisingly, the rotations are distinct not by reaching curvatures but determined from the initial conditions, which are encoded by the <strong>preparatory activities</strong>. We will see that <strong>preparatory activities</strong> feature as an essential ingredient in Churchland and colleagues’ research, resulting in surpising analysis on nullspace/output-potent space later <a class="citation" href="#nullspace">(“Cortical Activity in the Null Space: Permitting Preparation without Movement,” 2014)</a><a class="citation" href="#prep_review">(Mark M Churchland, 2024)</a> in the years to. Specifically, as the authors summarized, the trajectories have the following primary properties, which support the dynamical system perspective:</p> <blockquote> <p>1] Rotation is a ubiquitous phenomenon during behavior;</p> <p>2] Trajectories have the same directions for all rotations;</p> <p>3] Preparatory activities determine the initial conditions which govern trajectories;</p> <p>4] Rotations do not directly correlate with the curvature</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_3.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 3 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. jPCA projections into the first 2 dimensions of different monkeys. Each trajectory plots the first <strong>200ms</strong> activities following the preparatory state (initial conditions). Colors correpond to projection onto jPC1. </div> <p>An interesting reflection is that with the dynamical system perspective hypothesis, rotations of neural states should be similar no matter what reaching conditions: only the initial conditions determine the trajectories, with the underlying \(f\) being identical. I found this line of thought extremely insightful, which demonstrates how important it is to deeply understand theory to deduce results, though they might look unorthodox — for example, the neural states were expected be similar even when the reaches are in opposite directions if they share similar preparatory activities, because they are highly correlated wit initial conditions.</p> <p>The authors also carried some control (shuffling) analyses, and corroborated that such rotational pattern does explain a significant amount of data variance. Interestingly, the authors also discovered that, although rotations are consistent for all conditions in the same jPCA plane (similar orietations and speeds), such rotations actually exist in multiple jPCA planes. As shown below, all top 3 jPCA planes contain rotatinos, but with higher-number jPCA planes carrying less ordered rotations with slower speed. Perhaps not so surprisingly, both PMd and M1 exhibit such rotational structures, with initial states/prepatory activities better distinguished in PMd (again not surpising, because PMd is known for movement planning and with stronger preparatory activities. Eight years later, Russo et al. <a class="citation" href="#russo">(“Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation,” 2020)</a> will exhibit an obvious distinction between how M1 and PMd encode movement sequence/planning by exploring corresponding latent trajectories, providing further analyses between these two brain regions)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_4.png" class="img-fluid rounded z-depth-1" width="55%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 4 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. jPCA projections into different planes exhibit similar rotation structures. </div> <h2 id="jpca">jPCA</h2> <p>Based on the dynamical systems perspective, since we focus on time-dependent variations, a naive PCA is not sufficent to extract such temporal structures from neural activities (PCA is not specifically designed for incapsulating dynamical structures). Here Churchland et. al. developed an algorithm called jPCA to reoslve this issue. Specifically, it finds orthonormal axes (thus basis which define linear subspaces) which capture the strongest rotational components from the subspace identified by PCA (to ensure that the rotational dynamics come from subspaces that efficiently “represent” the high dimensional neural space). Conseuqently, this is equivalent to rotating the PCA projections to help viewers better “see” the rotation most clearly (as shown in Supplementary Movie 2 below). In the paper the authors chose the PCA dimension as 6, and the data projected from the 6-D PCA space to the first 2 jPCA components, thus a plane which captures the strongest rotations, is displayed (Adapted Figuer 3) to reveal the underlying oscillatory structure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 2 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>, showing jPCA projections are essentially a different view of PCA projections. </div> <p>The core of jPCA lies in the following steps:</p> <blockquote> <p>1] Fit a linear dynamical system for \(\dot{X}_{red} = M_{skew}X_{red}\), where \(X_{red}\) is of size \(6 \times ct\), \(c\): conditions, \(t\): time. Notice that this is really just a matrix formulation of \(\dot{x}_{red, i} = M_{skew}x_{red, i}\), where \(\dot{x}_{red, i}, \; x_{red, i}\) are column vectors and \(\dot{X}_{red} = [\dot{x}_{red, 1}, \; \dot{x}_{red, 2}, \; ..., \; \dot{x}_{red, (c(t-1))}], X_{red} = [x_{red, 1}, \; x_{red, 2}, \; ..., \; x_{red, c(t-1)}]\). Since \(\dot{X}_{red}\) is of size \(6 \times c(t-1)\) (use discrete time difference as approximations for the differentice), \(M_{skew} \in \mathbb{R}^{6 \times 6}\). Notice that to be precise, \(X_{red}\) is also truncated to be \(6 \times c(t-1)\). 6 is the dimensionlality of a PCA projection before starting jPCA, as mentioned above;</p> <p>2] Since \(M_{skew}\) is a skew-symmetric matrix, ithas pure imaginary eigenvalues and thus captures rotational dynamics;</p> <p>3] Identify the complex vectors \(V_1\) and \(V_2\) corresponding to the largest two imaginary complex values. From these locate the real planes: \(jPC_1 = V_1 + V_2\), and \(jPC_2 = V_1 - V_2\);</p> <p>4] The first jPCA projection is thus \(X_{jPC} = [jPC_1, jPC_2] \times X_{red}\). Similar projections for other jPCA planes.</p> </blockquote> <p>For a given jPCA plane, the choice of orthogonal basis is arbitruary, so the authors pick \(jPC_1\) and \(jPC_2\) such that the rotation is anti-clockwise and the preparatory activities spread along most clearly on \(jPC_1\).</p> <p>For the sections below, I’ll dive into details of the jPCA algorithm. Readers might want to skip the following if the above information is sufficient. For a more detailed description of jPCA, let’s start by reordering \(X\) as \(X \in \mathbb{R}^{ct \times n}\), and for instance \(X\) might be choosen as \(X_{red} \in \mathbb{R}^{ct \times k}\), for \(k = 6\) after PCA projection. For simpler and more general notation, for the following I will use X for illustration.</p> <h3 id="dynamical-summary-of-data">Dynamical summary of data</h3> <p>For a canonical PCA projection, we would start by finding the covaraince matrix \(\Sigma = X^TX \in \mathbb{R}^{n \times n}\) (notice that here n represents not the number of samples, but the number of features). We assume that \(X\) is already mean-centered. To capture dynamical structure, we will need a different \(n \times n\) matrix to summarize the data: consider a time-invariant linear dynamical system: \(\dot{x}(t) = x(t)M, M \in \mathbb{R}^{n \times n}\). Consequently, this reduces to solving</p> \[M^{*} = \argmin_{M \in \mathbb{R}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>Or simply put, \(M^{*} = X \backslash \dot{X}\). Notice that this optimization has a simple and closed-form solution:</p> \[M^{*} = (X^TX)^{-1}X^T\dot{X}\] <p>As the authors argued, the data covariance matrix \(\Sigma\) captures <strong>an ellipsoid which best fits the data</strong> (deep interpretation of this, what ellipsoid and what it represents when it says “best fit”), but with no temporal information, whereas \(M^*\) characterizes a linear dynamical system which best fits the data \(X\).</p> <p>I find this analysis particularly intriguing. In order to stay focus here, I’ll discuss a few reflections in the last section.</p> <h3 id="skey-symmetric-specification-for-rotation">Skey-symmetric specification for rotation</h3> <p>For a general linear dynamical system, the dynamics include both expandions/contractions and rotations. In this study, the authors are particularly interested in dissecting a potentially pure rotational dynamics. The key observation that Churchland and colleagues offered here is that every linear transformation \(M\) can be decomposed into two components:</p> \[M = M_{symm} + M_{skew}\] <p>where \(M_{symm} = (M + M^T)/2, M_{skew} = (M - M^T)/2\), so that \(M_{symm} = M_{symm}^T, \; M_{skew} = -M_{skew}^T\). This raises an interesting point whether all linear transformations can be decomposed into symmetric and skew-symmetric components: well, not quite, since here the formula only exist for \(M\) being a square matrix. However, even if \(M \in \mathbb{R}^{k \times l}, \; k \neq l\), it is still a linear transformation. How should we reconcile and think deeper here? I will leave this to the last section for further discussions.</p> <p>With such decomposition, the benefit it brings is that since \(M_{symm}\) have purely real eigenvalues and \(M_{skew}\) have purely imaginary eigenvalues (in complex conjugate pairs), thery describe expansions/contractions and rotations, correspondingly (the explanation for this and matrix exponential is further illustrated at the last section).</p> <p>Consequently, if we specify the set of skew-symmetric matrices as \(\not \mathbb{S}^{n\times n}\), and</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <h3 id="solution-of-the-constrained-optimization">Solution of the constrained optimization</h3> <h4 id="matrix-problem-rewritten-into-vector-form">Matrix problem rewritten into vector form</h4> <p>For solving the above optimization constrained to only skew-symmetric matrices, Churchland and colleagues did the following modifications: first notice that when solving \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\), each column of M is independently determined by the corresponding column of \(\dot{X}\). Consequently, the matrix optimization problem could be written in vector format: unroll \(M \in \mathbb{R}^{n \times n}\) into \(m = M(:)\), where \(m \in \mathbb{R}^{n^2}\), and thus the unconstrained least squares problem could be rewritten as:</p> \[m^{*} = \argmin_{m \in \mathbb{R}^{n^2}} ||\dot{x} - \tilde{X}m||_{F}\] <p>where \(\dot{x} = \dot{X}(:)\), and \(\tilde{X}\) is a block diagonal matrix with \(X\) repeated on the \(n\) diagonal blocks. I totally agree that the matrix/vector formats are equivalent, but it does not take \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\) to realize such format conversion. By inspecting upon \(\dot{X} = XM\), we should know that each column of \(\dot{X}\) is independently informed by the corresponding column of \(M\).</p> <h4 id="skew-symmetric-matrix-constraint">Skew-symmetric matrix constraint</h4> <p>To add skew-symmetricity constraint to the optimization problem, notice that for \(\not \mathbb{S}^{n \times n}\), the degrees of freedom is \(n(n-1)/2\) (notice that the diagonals are automatically 0, thus \(n\) less than \(n(n+1)/2\) as in symmetric matrix. Could think more on the underlying manifold, Lie group, and Lie algebra, see the last section).</p> <p>The crucial step is to realize that thus a skew-symmetric matrices is equivalently represented as vectors of the form \(k \in \mathbb{R}^{n(n-1)/2}\). This also reinforces my view that acquiring geometric priors, or basically whatever kinds of priors, encoded in the decoding models is alleviating model’s “cognitive burden”. I’d consider the speech BCI, which entails large language models (LLM) for improving RNN decding as adopting a “similar” strategy/line of thinking.</p> <p>Back to the central topic, with this vector \(k \in \mathbb{R}^{n(n-1)/2}\), we could likewise simply form a linear map \(H\) which transforms \(\mathbb{R}^{n(n-1)/2}\) into \(\mathbb{R}^{n^2}\). In a nutshell, \(k\) encodes the lower/upper triangle of \(M_{skew}\) and \(H\) is hand-coded (not shown here, but sparse and has only \(n(n-1)/2\) positions of 1 and \(n(n-1)/2\) positions of -1).</p> <h4 id="final-closed-form-optimization">Final closed-form optimization</h4> <p>Notice that now we finally see the benefit of the vector format: \(Hk\) returns a vector of shape \(\mathbb{R}^{n^2}\) which equivalently represents \(M_{skew} \in \not \mathbb{S}^{n \times n}\). Consequently, the original constrained optimzation problem</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>is eventually rewritten as</p> \[k^{*} = \argmin_{k \in \mathbb{R}^{n(n-1)/2}} ||\dot{x} - \tilde{X}Hk||_{2}\] <p>Notice that this is similar in form to the OLS problem can thus be written in closed form:</p> \[k^{*} = (\tilde{X}H)\backslash \dot{x}\] <p>The authors also briefly discussed a few implementation techniques, including solving using a gradient descent based method. However, since this optimization problem has a unique global optimum, they should return the same results.</p> <h4 id="what-does-it-represent-to-decompose-the-dynamics">What does it represent to decompose the dynamics?</h4> <p>Now with \(M^{*}_{skew}\), the authors found out the corresponding eigenvectors and eigenvalues and do projections like PCA. Since skew-symmetric matrices only have pure imaginary eigenvalues and the eigenvectors come in <strong>orthogonal, complex conjugate</strong> pairs (thus could think about the decomposition of \(M_{skew}\) as extracting orthogonal planes), the authors find the real projection planes \({u_{i, 1}, u_{i, 2}}\) from the complex conjugate vector pairs \({v_{i, 1}, v_{i, 2}}\) by \(u_{i, 1} = v_{i, 1} + v_{i, 2}, \; u_{i, 1} = v_{i, 1} - v_{i, 2}\) with normalization. Notice that the eigenvector/eigenvalue decomposition logics are different here: For PCA, eigenvectors of the covariance matrix serve as principal components of the underlying data (geometric view: ellipsoid); For jPCA, the eigenvectors stipulate the axes along which the solutions of the underlying dynamical system evolve.</p> <p>To explain this clearly, let’s take an example linear dynamical system problem:</p> <p>[TODO]</p> <p>Matrix exponential</p> <p>(Associate to Suppl.Fig.11 panels d, e)</p> <p>Projections onto jPC’s with largest magnitude of the corresponding eigenvalues (explain what this means) present most significant rotations: higher frequency and more consistency (Supplemantary Figure 4)</p> <h2 id="discussions">Discussions</h2> <p>The extracted neural trajectory is only “quasi-oscillatory”, and usually exists for barely 1 cycle.</p> <p>For many neurons, the preparatory and movement-related single neuron responses do exhibit some quasi-oscillations which last for about 1 cycle (See in below).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_2.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 2 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. Trace colors are according to the strengthof preparatory activities (red with greatest preparotory response). It allows clear view of the evolution of preparatory patterns during the movement. </div> <p>Not sure yet if this explains the observed rotation structure at the population level. However, the population level state-space rotations direcly to neural responses at single unit level: notice that each axis of jPCA exhibits similar patterns as single-neuron response(shown below)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_1.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 1 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. The projections onto jPC's are similar to single neuron response. </div> <p>As Churchland et al argued, this could be explained by the fact that jPC’s capture the most conscipuous patterns in the data, and each jPCA projection is a weighted sum/average of individual-neuron resopnses. Likewise, each single neuron could also be interpreted as a combination of all the jPC projected patterns, except that the intertwined relationship is not obvious if just viewed from a single neuron or a pattern itself.</p> <p>We might argue that perhaps a sufficient condition for the rotation structures is that neurons exhibit multiphasic responses. However, this turns out to be not necessarily true. The authors also tested on a “complex-kinematic model” and recorded EMG. Though both carry multi-phasic reponses, they do not show clear rotation patterns. In the end, as the authors argued, multiphasic responses are not enough: we also need the multi-phasic patterns to have consistent phases for around 90 degrees apart.</p> <p>More exploration on the 90 degrees phase difference here:</p> <p>Another core question is how such rotational pattern generates non-oscilatory movement patterns, e.g. reaching. The authors conducted some further simulation analyses and demonstrated that it’s possible to reconstruct EMG (deltoid) activities from weighted sum of rotations with different magnitude and phases (they call this the generator-model). It’s interesting to see this because EMG does not acquire the latent rotational patterns, yet it could be reconstructed from rotational dynamics. Further quantification analyses also showed that only the neural data and generator-model exhibit rotational patterns, whereas two other models (velocity-tuned, complex-kinematic models (not explained here in this blog)) do not. Since EMG showed only weak rotation, as the authors put, this further illustrates that the latent rotation dynamics do not necessarily result from a muli-phasic reponse, but how such response is constructed.</p> <p>One interesting finding is that the speed of movements is not encoded as the speed of the latent trajectories (they keep similar angular velocity). The amplitude of rotation dictates different movement speeds (for example faster or slower reaches). As Churchland et al. argued, this could be nicely explained since rotations with larger amplitude represent more strongly multi-phasic responses, with which EMG frequently entails larger accelerataions.</p> <p>The rotational structure is dependent upon the initial states (dynamical systems view) specified by the prepratory activities, which captures the condition-dependent activities (here condition refers to experiment conditions). Indeed, to clearly observe such differences, the cross-condition mean is substracted first for each single neuron before PCA/jPCA. The authors also illustrated the effects of keeping the cross-condition mean, as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_11.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 11 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>. </div> <p>The goal of mean subtraction is to preserve only data distinguished in different conditions. In panel a. and b., each trace represents a trial-averaged firing rate for one (experiment) condition, with the same coloring scheme by the strength of preparatory activities. The yellow trace indicates the cross-condition mean for that neuron. Panel b. substracts the mean (yellow line being 0 all along) while panel a. keeps it. If without substraction of cross-condition mean, it’s observed that the first jPCA plane captures condition-independent curved trajectories. The authors argued that this is not surprising, since many neurons exhibit similar behaviors across different conditions, which are thus naturally captured by PCA projections. Since jPCA is extracting patterns from PCA space, it’s fairly expected that such patterns are condition independent. Notice that even in panel c., these trajectories still carry curvature, though it’s difficult to pinpoint how it’s theoretically generated. Panel d. displays the jPCA projection onto the second jPC plane (\(jPC3_, \; jPC_4\)) and we do see similar rotational structures dependent upon initial states specified by the preparatory activities. Consequently, to always explore the condition-dependent dynamics, the authors argue to substract cross-condition mean first before PCA/jPCA.</p> <p>As the authors put at the end of the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf">Supplementary Information</a>, the jPCA method could be easily adapted to find the projections for most significant expansion/contraction.</p> <h2 id="conclusions">Conclusions</h2> <p>In summary,</p> <blockquote> <p>1] “state-space rotations produce briefly oscillatory temporal patterns that provide an effective basis for producing multiphasic muscle activity, suggesting that non-periodic movements may be generated via neural mechanisms resembling those that generate rhythmic movement”;</p> <p>2] Preparactory encoders the initial conditions for the underlying dynamical system;</p> </blockquote> <p>The shift of views from single neuron analysis in pursuit of movement/behavior correlates/representaitons, to a dynamical system analysis on population level, is becoming an increasingly populat domain. As we will see later, the other side of the same story, geometry instead of dynamics, will quickly come into the arena under the name of neural manifold. This, with both aboundance of aesthetic elegance and disatisfying lack of mathematical rigor from either topology or differential geometry, will be an eternal theme for the following research, projects, and blogs of mine.</p> <h2 id="open-querries">Open Querries</h2> <p>The annotomical circuitry that leads to the dynamical system/latent trajectory is still unclear.</p> <p>What happens after the 200ms? As shown in Supp Movie 3 below, how would the observed rotations evolve later in time (notice that all the rotations exist for merely 1~1.5 cylces)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_3.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 3 in <a class="citation" href="#jpca">(“Neural Population Dynamics during Reaching,” 2012)</a>, showing the evolution of the projected states. </div> <p>Although in the Discussions section, I briefly talked about the reason for existence of the rotational structures, The problem remains as to how individual oscillations of different phases and amplitudes would result in a consistent rotation.</p> <p>At the algorithmic level, the jPCA method stems from dissecting the rotation component away from the general linear transformation. Can this be further stripped away from PCA?</p> <p>Churchland argued that the dynamics quantified by a skew-symmetric matrix hints at the underlying functionally antisymmetric connectivity. The authors wrote: “a given neural dimension (for example, \(jPC_1\)) positively influences another (for example, \(jPC_2\)), which negatively influences the first. However, it is unclear whether this population-level pattern idrectly reflects a circuit-level dominance of antisymmetric connectivity”</p> <p>For jPCA algorithm, apart from \(\Sigma\) or \(M^*\), is there other way of capturing the data statistics from different perspectives?</p> <p>Notice that within jPCA, the authors made the following decomposition: \(M = M_{symm} + M_{skew}\), which naturally dissects the linear dynamics into expansions/contractions and rotations. I’m wondering whether similar decomposition can be made for general linear transformation \(M \in \mathbb{R}^{k \times l}, \; k \neq l\). This definitely cannot be a dissection into symmetric and skew-symmetric matrices because this general \(M\) is not a square matrix. Moreover, if we want to extend to non-square matrix, we are already out of the realm of dynamical system, since that would require the linear matrix to be square: \(\dot{x}(t) = Mx(t)\). How do we characterize a general linear transformation?</p> <p>Why do dynamical systems specified by symmetric and skew-symmetric matrices encode expansions/contractions and rotations? The reasons for that is the solution for linear dynamical systems exhibits as a matrix exponential, which leads to eventual solutions dependent upon the eigenvalues/eigenvectors. Notice that a symmetric matrix has real eigenvalues and skew-symmetrix matrix has imaginative eigenvalues. One related question I have is: can matrix exponential here in jPCA for extrapolation, potentially applied for data later than the first 200ms? How would the extrapolated dynamical trajectories align with the observed data?</p> <h3 id="hermitian-unitary-and-normal-matrices">Hermitian, unitary, and normal matrices</h3> <p>Churchland and colleagues put the following at the end of <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf">Supplementary Information</a>:</p> <blockquote> <p>“The convenience of being able to eigendecompose a summary matrix and yield orthogonal vectors belongs to the class of normal matrices, which by definition are diagonalizable by a unitary matrix. The class of normal matrices includes symmetric and skewsymmetric matrices, among others. This fact suggests a broader class of PCA variants that are a subject of future work.”</p> </blockquote> <p>In order to understand the inner workings of matrix decomposition, I’d like to review first the following definitions.</p> <blockquote> <p><em>Def</em>: The Hermitian tranpose (or conjugate transpose) of a matrix \(A\) is denoted as \(A^{\dagger} = \bar{A}^{T}\), which the tranpose of the complex conjugate (applied to each entry). Equivalent notations are \(A^H, \; A^{*}\). Notice that for a real matrix \(A \in \mathbb{R}^{n \times n}, \; A^{\dagger} = A^{T}\).</p> <p><em>Def</em>: A matrix is called <strong>Hermitian</strong> (or <strong>self-adjoint</strong>) if \(A = A^{\dagger}\). Notice that A real <strong>Hermitian</strong> matrix is equivalently <strong>symmetric</strong>.</p> <p><em>Def</em>: A matrix \(U\) is called <strong>unitary</strong> if \(U^{\dagger}U = I\) Notice that a real <strong>unitary</strong> matrix is equivalently <strong>orthogonal</strong>.</p> </blockquote> <p>Notice that we also have the following theorems for the above special matrices (more discussions seen in this <a href="https://www.math.purdue.edu/~eremenko/dvi/lect3.26.pdf">lecture note</a>):</p> <blockquote> <p><em>Spectral theorem</em> for Hermitian matrices: For a <strong>Hermitian</strong> matrix,</p> <p>(i) all eigenvalues are real,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>and likewise</p> <blockquote> <p><em>Spectral theorem</em> for unitary matrices: For a <strong>unitary</strong> matrix,</p> <p>(i) all eigenvalues have magnitude 1,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>Consequently, Hermitian and unitary matrices are always <strong>diagonalizable</strong> (indeed some eigenvalues might be the same). Notice that eigenvectors of any matrix corresponding to distinct eigenvalues are linearly independent. Here with Hermitian and unitary matrices they are not only linearly indepedent, but also <strong>orthogonal</strong>.</p> <p>Theorems (i) for both Hermitian and unitary matrices could be proven separately. Since (ii) and (iii) are the same, we might be thinking about looking for a greater class of matrices which include these two types, but carrying with more general properties, entailing (ii) and (iii). Let me introduce the following:</p> <blockquote> <p><em>Def</em>: A normal matrix is a matrix that commutes with its adjoint: \([A, A^{\dagger}] = 0\) where [B, C] = BC - CB</p> </blockquote> <p>Note that Hermitian and unitary matrices are special cases of a normal matrix. Also, the previous definitions will be helpful for the next extension into Lie group/algebra. For normal matrices we have the following:</p> <blockquote> <p><em>Spectral theorem</em> for normal matrices: A matrix is <strong>normal</strong> <em>if and only if</em> there is an orthogonal basis consisting of eigenvectors.</p> </blockquote> <p>From the above theorems we could easily see that a symmetric matrix \(A\) has all real eigenvalues (becuase it’s a Hermitian matrix). Consequently, it has the decomposition:</p> \[A = B \Lambda B^{-1}\] <p>where \(\Lambda\) is a real iagonal matrix, B orthogonal.</p> <p>There’s also a famous fact between Hermitian and unitary matrix:</p> <blockquote> <p>There exists a 1-1 correponsdence between the set of unitary matrices \(U\) and the exponental of the set of Hermitian matrices \(H\), i.e.:</p> \[U = exp(iH)\] </blockquote> <p>The proof is not hard and not displayed here. The message it encodes is important: the unitary matrices are exactly in the format of the exponential map of Hermitian matrices.</p> <p>Similarly, we could show the following for orthogonal matrix: Given \(A\) as a skew-symmetric (real) matrix (\(A^{\dagger} = A^{T} = -A\)), notice first that</p> \[-iA = iA^T = iA^{\dagger} = (-iA)^{\dagger}\] <p>Consequently, \(-iA\) is a Hermitian matrix and thus since</p> \[e^{A} = e^{i(-iA)} = e^{i(-iA)^{\dagger}}\] <p>Then \(e^{A}\) is a unitary matrix. Since \(A\) is real, \(e^{A}\) has to be real. Consequently, \(e^{A}\) is an orthogonal matrix.</p> <p>However, the exponential map of skew symmetric matrices does not result in all orthogonal matrix:</p> \[det(e^A) = e^{trA} = e^{0} = 1\] <p>which means that this way only characterizes rotation matrices (determinant equal to 1, unlike reflection which changes the orientation).</p> <h3 id="relations-with-lie-group-and-lie-algebra">Relations with Lie group and Lie algebra</h3> <p>Since we are talking about dynamics with symmetric and skew-symmetric matrices, how do they relate to Lie group/algebra?</p> <p>Also, since this constrained optimization problem has a unique global optimum (how should we know this)?</p>]]></content><author><name></name></author><category term="Computational Neuroscience"/><category term="Latent Dynamics"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Introduce neural latent dynamics from jPCA]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://jasmineruixiang.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://jasmineruixiang.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>