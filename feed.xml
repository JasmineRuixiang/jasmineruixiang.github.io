<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jasmineruixiang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasmineruixiang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-18T00:42:02+00:00</updated><id>https://jasmineruixiang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Manifold and Riemannian Geometry (4): Connections (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (4): Connections (in progress)"/><published>2026-01-15T15:48:02+00:00</published><updated>2026-01-15T15:48:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(4)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(4)/"><![CDATA[<p>This is episode 4 on the smooth manifold series. Today we will be exploring more on tangent vectors, and another key concept related to tangent spaces for different tangent planes: connections.</p> <h2 id="intuition-directional-derivatives">Intuition: Directional Derivatives</h2> <p>üåê The Connection: Bridging Derivatives from $\mathbb{R}^3$ to Curved Manifolds The concept of a connection is the necessary tool that allows us to perform differential calculus on curved spaces (manifolds), such as the surface of a sphere. It generalizes the familiar idea of the directional derivative from flat Euclidean space ($\mathbb{R}^3$).</p> <ol> <li>Directional Derivatives in Euclidean Space ($\mathbb{R}^3$) In $\mathbb{R}^3$ with Cartesian coordinates $(x, y, z)$, the directional derivative provides a simple way to measure change. The basis vectors $\left{ \mathbf{i}, \mathbf{j}, \mathbf{k} \right}$ (or the equivalent operators $\left{ \frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z} \right}$) are constant, allowing us to define derivatives simply as component-wise partial derivatives. Let $p=(1, 2, 0)$ be a point, $X=(y, -x, 3x)$ be the direction vector field, and $V=(xz, y^2, -2x)$ be a vector field. A. Derivative of a Scalar Function ($D_X f$) This is the directional derivative of a smooth scalar function $f$ in the direction $X$.</li> </ol> <p>Perspective Formula Example Result for f(x,y,z)=xy2+z at p Vector-Based (Calculus) $D_X f = \nabla f \cdot X$ $D_X f(p) = \langle 4, 4, 1 \rangle \cdot \langle 2, -1, 3 \rangle = \mathbf{7}$ Point Derivation (Geometry) $X[f] = X^i \frac{\partial f}{\partial x^i}$ $Xf = (y^3 - 2x^2y + 3x)\big</p> <p>This confirms that in differential geometry, a tangent vector $X$ is rigorously defined as a point derivation‚Äîan operator that mimics the directional derivative by satisfying the Leibniz rule. B. Derivative of a Vector Field ($D_X V$) This derivative measures how the vector field $V$ changes as we move in the direction $X$. In $\mathbb{R}^3$, this is calculated by taking the directional derivative of each component of $V$. Using the vector fields $X$ and $V$: The $k$-th component of the resulting vector $D_X V$ is $(D_X V)^k = \sum_{i} X^i \frac{\partial V^k}{\partial x^i}$. Component 1 (i.e., $k=1$): $(D_X V)^1 = yz + 3x^2$ Component 2 (i.e., $k=2$): $(D_X V)^2 = -2xy$ Component 3 (i.e., $k=3$): $(D_X V)^3 = -2y$ Evaluating at $p=(1, 2, 0)$ gives:</p> <p>\(D_X V(p) = \langle 3, -4, -4 \rangle\)</p> <ol> <li>The General Connection: The Covariant Derivative ($\nabla_X V$) The formula for $D_X V$ fails on a curved manifold $M$ because the tangent spaces $T_p M$ and $T_q M$ at nearby points $p$ and $q$ are distinct. We cannot simply subtract the vector $V(p)$ from $V(q)$. A Connection ($\nabla$) is the rule that provides the necessary ‚Äúcorrection‚Äù to define the derivative intrinsically on $M$. The resulting derivative is called the covariant derivative $\nabla_X V$. A. Definition and Axioms The connection is an operator $\nabla: C^{\infty}(M) \times C^{\infty}(M) \to C^{\infty}(M)$ that maps two vector fields, $X$ and $V$, to a new vector field $\nabla_X V$, satisfying: Linearity over Functions in $X$: $\nabla_{fX} V = f \nabla_X V$ Linearity in $V$: $\nabla_X (aV + bW) = a \nabla_X V + b \nabla_X W$ Leibniz Rule: $\nabla_X (fV) = (Xf) V + f \nabla_X V$ (where $Xf$ is the directional derivative of $f$) B. The Coordinate Form and Christoffel Symbols In local coordinates, the covariant derivative $\nabla_X V$ is defined using the Christoffel symbols ($\Gamma^k_{ij}$), which represent the rate of change of the coordinate basis vectors $\left{ \frac{\partial}{\partial x^i} \right}$:</li> </ol> <p>\((\nabla_X V)^k = \underbrace{X^i \frac{\partial V^k}{\partial x^i}}_{\text{I. Flat-Space Derivative Term}} + \underbrace{X^i \Gamma^k_{ij} V^j}_{\text{II. Curvature Correction Term}}\) The Christoffel symbols $\Gamma^k_{ij}$ are defined by the action of the connection on the basis vectors:</p> <p>\(\nabla_{\frac{\partial}{\partial x^i}} \frac{\partial}{\partial x^j} = \sum_{k} \Gamma^k_{ij} \frac{\partial}{\partial x^k}\) Difference from Euclidean Case: In $\mathbb{R}^3$ with Cartesian coordinates, $\Gamma^k_{ij} = 0$, and the second term vanishes, resulting in $\nabla_X V = D_X V$. On a curved manifold, $\Gamma^k_{ij} \neq 0$, and the correction term is essential.</p> <ol> <li>The Levi-Civita Connection In Riemannian Geometry, a Riemannian metric $g$ is introduced to measure lengths and angles. The Levi-Civita Connection is the unique connection that respects this metric structure. It is defined by two crucial properties: Metric Compatibility: The connection must preserve the metric $g$ under parallel transport.</li> </ol> <p>\(X(g(V, W)) = g(\nabla_X V, W) + g(V, \nabla_X W)\) Zero Torsion: The connection must satisfy:</p> \[\nabla_X Y - \nabla_Y X = [X, Y]\] <p>where $[X, Y]$ is the Lie bracket. The Christoffel symbols of the Levi-Civita Connection are thus entirely determined by the components of the metric $g_{ij}$ and their first derivatives:</p> \[\Gamma^k_{ij} = \frac{1}{2} g^{k\ell} \left( \frac{\partial g_{j\ell}}{\partial x^i} + \frac{\partial g_{i\ell}}{\partial x^j} - \frac{\partial g_{ij}}{\partial x^\ell} \right)\] <h2 id="connections">Connections</h2> <h2 id="christoffel-symbols">Christoffel Symbols</h2> <h2 id="levi-civita-connections">Levi-Civita Connections</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Exploration of Connection]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (3): Immersion, Submersion and Embedding (in progress)"/><published>2026-01-14T20:22:25+00:00</published><updated>2026-01-14T20:22:25+00:00</updated><id>https://jasmineruixiang.github.io/blog/2026/Manifold(3)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2026/Manifold(3)/"><![CDATA[<p>This is episode 3 on the smooth manifold series. Today we will be diving into the properties of maps between manifolds. I will first summarize how to understand and compute the differential of a smooth map between manifolds, both abstractly and concretely, culminating in the matrix-valued example<br/> [ F(A) = A^\top A. ]</p> <hr/> <h2 id="1-differential-canonical-definition">1] Differential: canonical definition</h2> <h3 id="11-differential-of-a-smooth-map-intrinsic-definition">1.1 Differential of a Smooth Map (Intrinsic Definition)</h3> <p>Let [ F : M \to N ] be a smooth map between smooth manifolds.</p> <p>For any point ( p \in M ), the <strong>differential</strong> is a linear map which is a <strong>pushforward of derivations</strong>. [ dF_p : T_p M \longrightarrow T_{F(p)} N. ]</p> <h3 id="derivation-based-definition">Derivation-based definition</h3> <p>If ( v \in T_p M ) is a tangent vector viewed as a derivation, then [ (dF_p v)(g) := v(g \circ F), \qquad g \in C^\infty(N). ]</p> <p>This definition is <strong>coordinate-free</strong>.</p> <p>It might appear at first both unnecessarily abstract and underestimated as to its computation. We might claim that it is essentially just a Jacobian matrix in local coordinates. However, the essence of this concept resides on its definition to be conceptually a coordinate-independent linear map between tangent spaces.</p> <hr/> <h3 id="12-coordinate-representation-and-the-jacobian">1.2 Coordinate Representation and the Jacobian</h3> <p>To compute ( dF_p ) in practice:</p> <ol> <li>Choose a chart ( (U,\varphi) ) on ( M ) with ( p \in U )</li> <li>Choose a chart ( (V,\psi) ) on ( N ) with ( F(p) \in V )</li> </ol> <p>Define the coordinate expression: [ \tilde F = \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n. ]</p> <p>Then: [ \boxed{ dF_p \;\text{is represented by}\; D\tilde F(\varphi(p)) } ]</p> <p>That is, <strong>the Jacobian matrix is the coordinate representation of the differential</strong>.</p> <blockquote> <p>The Jacobian depends on coordinates; the linear map ( dF_p ) does not.</p> </blockquote> <hr/> <h3 id="13-why-this-is-not-just-the-jacobian">1.3 Why this is not ‚Äújust‚Äù the Jacobian</h3> <p>The Jacobian depends on coordinates;<br/> ( dF_p ) does not.</p> <p>More precisely:</p> <ul> <li>( dF_p ) is a <strong>geometric linear map</strong></li> <li>The Jacobian is a <strong>matrix representation</strong> of that map in chosen bases: [ \frac{\partial \bigl(\psi^1 \circ F,\;\dots,\;\psi^n \circ F\bigr)} {\partial \bigl(x^1,\;\dots,\;x^m\bigr)} ]</li> </ul> <p>If you change charts, the matrix changes by:</p> <p>[ \boxed{ J_{\text{new}} = D\psi\,\cdot\, J_{\text{old}} \,\cdot\, (D\varphi^{-1}) } ]</p> <p>but the underlying linear map ( dF_p ) stays the same.</p> <hr/> <h2 id="2-differential-alternative-interpretation">2] Differential: alternative interpretation</h2> <h3 id="21-curve-based-definition">2.1 Curve-based definition</h3> <p>This is also coordinate-free:</p> <p>If<br/> [ \gamma : (-\varepsilon,\varepsilon) \to M ] is a smooth curve with [ \gamma(0) = p \quad \text{and} \quad \gamma‚Äô(0) = v \in T_p M, ] then [ \boxed{ dF_p(v) = (F \circ \gamma)‚Äô(0) \in T_{F(p)} N. } ]</p> <p>No coordinates anywhere. This viewpoint is often the most intuitive and is fully equivalent to the derivation definition.</p> <hr/> <h3 id="22-curves-in-local-coordinates">2.2 Curves in local coordinates</h3> <p>Choose charts:</p> <ul> <li>( (U,\varphi) ) on ( M ) with ( p \in U )</li> <li>( (V,\psi) ) on ( N ) with ( F(p) \in V )</li> </ul> <p>Define the coordinate representation: [ \tilde F := \psi \circ F \circ \varphi^{-1} : \mathbb{R}^m \to \mathbb{R}^n. ]</p> <p>Now define the coordinate curve: [ \tilde\gamma := \varphi \circ \gamma : (-\varepsilon,\varepsilon) \to \mathbb{R}^m. ]</p> <p>Consequently, in coordinates the statement is the following:</p> <p>[ \boxed{ D\tilde F(\varphi(p)) \cdot \tilde\gamma‚Äô(0) = D(\psi \circ F \circ \varphi^{-1})(\varphi(p)) \cdot (\varphi \circ \gamma)‚Äô(0). } ]</p> <p>In the coordinate formula, [ \gamma‚Äô(0) \quad \text{really means} \quad \tilde\gamma‚Äô(0) = (\varphi \circ \gamma)‚Äô(0). ]</p> <p>Here‚Äôs a diagram that makes everything explicit</p> <p>[ \begin{array}{ccc} T_p M &amp; \xrightarrow{dF_p} &amp; T_{F(p)} N <br/> \downarrow d\varphi_p &amp; &amp; \uparrow d\psi^{-1}_{\psi(F(p))} <br/> \mathbb{R}^m &amp; \xrightarrow{D\tilde F(\varphi(p))} &amp; \mathbb{R}^n \end{array} ]</p> <p>Thus:</p> <ul> <li>( \gamma‚Äô(0) ) lives in ( T_p M )</li> <li>( (\varphi \circ \gamma)‚Äô(0) = d\varphi_p(\gamma‚Äô(0)) \in \mathbb{R}^m )</li> <li>( D\tilde F(\varphi(p)) ) acts on that coordinate vector</li> </ul> <hr/> <h3 id="23-sidenote-graph-differential">2.3 Sidenote: Graph Differential</h3> <p>The <strong>graph</strong> of ( F ) is [ \Gamma_F = { (p, F(p)) \mid p \in M } \subset M \times N. ]</p> <p>Define the graph map: [ \Phi : M \to M \times N, \quad \Phi(p) = (p, F(p)). ]</p> <p>Its differential is: [ \boxed{ d\Phi_p(v) = (v, dF_p(v)). } ]</p> <p>This is what is often called the <strong>graph differential</strong>.</p> <hr/> <h2 id="3-special-case-maps-between-vector-spaces">3] Special Case: Maps Between Vector Spaces</h2> <h3 id="31-a-great-simplification">3.1 A great simplification</h3> <p>If ( M = \mathbb{R}^m ), ( N = \mathbb{R}^n ), then: [ T_p M \cong \mathbb{R}^m, \quad T_{F(p)} N \cong \mathbb{R}^n. ]</p> <p>In this case:</p> <ul> <li>( dF_p ) is a linear map ( \mathbb{R}^m \to \mathbb{R}^n )</li> <li>Its matrix is exactly the <strong>Jacobian matrix</strong></li> <li>( dF_p(H) ) coincides with the <strong>Fr√©chet / directional derivative</strong></li> </ul> <hr/> <h3 id="32-worked-example--fa--atop-a-">3.2 Worked Example: ( F(A) = A^\top A )</h3> <p>Let [ F : \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}, \quad F(A) = A^\top A. ]</p> <p>Since ( \mathbb{R}^{n \times n} ) is a vector space, [ T_A(\mathbb{R}^{n \times n}) \cong \mathbb{R}^{n \times n}. ]</p> <h4 id="321-direct-computation">3.2.1 Direct computation</h4> <p>There are several ways to compute the differential. The most straight-forward method is to do the following (as you might imagine):</p> <p>For ( H \in T_A(\mathbb{R}^{n \times n}) ), [ dF_A(H) = \left.\frac{d}{dt}\right|_{0} (A+tH)^\top(A+tH). ]</p> <p>Expanding: [ (A+tH)^\top(A+tH) = A^\top A + t(H^\top A + A^\top H) + t^2 H^\top H. ]</p> <p>Thus: [ \boxed{ dF_A(H) = H^\top A + A^\top H. } ]</p> <h4 id="322-curve-based-computation">3.2.2 Curve-based computation</h4> <p>Let ( A(t) ) be a smooth curve with: [ A(0)=A, \quad A‚Äô(0)=H. ]</p> <p>Then: [ dF_A(H) = \left.\frac{d}{dt}\right|_{0} A(t)^\top A(t) = H^\top A + A^\top H. ]</p> <p>This makes it clear that the definition of $dF_A(H)$ is <strong>coordinate-free</strong>.</p> <p>Sidenote: if we restrict $A$ to symmetric or SPD matrices, what will we see? Or what if we connect this to Riemannian geometry on $GL(n)$ or $SPD(n)$? We‚Äôll come back to this later when we discuss Lie group and Lie algebra.</p> <hr/> <h2 id="4-association-with-inverse-function-theorem">4] Association with inverse function theorem</h2> <h2 id="5-immersion-submersion">5] Immersion, submersion</h2> <h2 id="6-embedding">6] Embedding</h2> <h2 id="7-submanifolds">7] Submanifolds</h2>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Explore relations of maps between manifolds]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (2): Tangent/Cotangent Space (in progress)"/><published>2025-09-17T23:30:02+00:00</published><updated>2025-09-17T23:30:02+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(2)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(2)/"><![CDATA[<p>This is episode 2 on the smooth manifold series. Today we will be diving into concepts that appear initially very intuitive at first glance, but then the extended version of which is indeed quite abstract.</p> <h2 id="familiar-examples">Familiar examples</h2> <p>Let me start with two simple concrete examples to illustrate what tangent vectors and tangent space are. Indeed they match up to our intuition!</p> <p>Let‚Äôs first say that we have a unit circle in \(\mathbb{R}^2\), or basically let‚Äôs denote it as \(S^1\) (this is a standard notation as</p> <p>\(S^{n} = \{ x \in \mathbb{R}^{n+1} \mid |x| = 1 \}\),</p> <p>representing the surface of an \((n+1)\)-dimensional ball). Pick \(p = (1, 0)\). If I ask you what the tangent vector is starting at \(p\) and tangnet to \(S^1\)? Your answer is probably a vector pointing upward or downward with its tail at \(p\). Indeed,</p> <h2 id="three-equivalent-definitions-of-tangent-space">Three equivalent definitions of Tangent Space</h2> <p>We‚Äôll cover three equivalent definitions tangent space.</p> <h3 id="1-tangent-vectors-are-equivalence-classes-of-curves">[1] Tangent vectors are equivalence classes of curves</h3> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>tangent vectors are equivalence classes of smooth curves through \(p\):</p> \[T_pM = \{\frac{d}{dt}\Big|_{t = 0} \gamma(t) \Big| \gamma: (\epsilon, \epsilon) \rightarrow M, \; \gamma(0) = p \}\] <p>where \(\gamma_1 \sim \gamma_2\) if in some (equivalently, any) coordinate chart \(\phi: U \subset M \rightarrow \mathbb{R}^n\):</p> \[\frac{d}{dt}\Big|_{t = 0} \phi(\gamma_1(t)) = \frac{d}{dt}\Big|_{t = 0} \phi(\gamma_2(t))\] <p>Consequently, each tangent vector is represented by the velocity of a curve through \(p\).</p> <h3 id="2-tangent-vectors-are-derivations-at-p">[2] Tangent vectors are derivations at \(p\)</h3> <p>Let \(M\) be a manifold, the tangent space at \(p \in M\), denoted as \(T_pM\), is the the vector space of all derivations at \(p\). Notice that a derivation at \(p\) is a <strong>linear</strong> operator (at \(p\)):</p> \[D: C^{\infty}_{p}(M) \rightarrow \mathbb{R}\] <p>satisfying the Leibniz rule:</p> \[D(fg) = f(p)D(g) + g(p)D(f), \; \forall f, g \in C^{\infty}(M)\] <p>where \(C^{\infty}_{p}\) is the equivalence class of \((f, U)\), where \(f \in C^{\infty}\) and \(U\) is a neighbourhood of \(p\). Two functions \((f, U)\) and \((g, V)\) are equivalent iff \(\exist \; W \subset U \cap V\) a neighbourhood of \(p\) such that \(f(x) = g(x), \forall x \in W\), where \(g \in C^{\infty}\) and \(V\) is a neighbourhood of \(p\).</p> <p>Intuitively, \(D\) is like a directional derivative operator acting on smooth functions near \(p\). Consequently,</p> \[T_p{M} = \{ D \mid D \; \mathrm{is\ a\ derivation\ at} p \}\] <p>and a tangent vector \(v \in T_p{M}\) is a derivation \(D\). In fact, Tu (Theorem 2.2, An introduction to manifolds) showed that there‚Äôs a bijection between derivations at \(p\) and directional derivaties.</p> <h3 id="3-tangent-vectors-as-equivalences-on-function-germs">[3] Tangent vectors as equivalences on function germs</h3> <h3 id="local-coordinate-description">Local coordinate description</h3> <p>Specifically if we adopt interpretation [2], then with \((U, \phi)\) a chart with coordinates \((x^1, \dots, x^n)\) near \(p\)</p> \[\{ \frac{\partial}{\partial x^1}\Big|_p, \dots, \frac{\partial}{\partial x^n}\Big|_p \}\] <p>form a basis of \(T_pM\).</p> <p>Thus any tangent vector \(v\) can be written uniquely as</p> \[v = \sum_{i = 1}^{n} v^i \frac{\partial}{\partial x^i}\Big|_p\] <p>where \((v^1, \dots, v^n)\) are the components of the vector in this coordinate system.</p> <h2 id="equivalence-between-derivations-and-curves">Equivalence between derivations and curves</h2> <p>Perhaps not so surprisingly, the above two definitions are compatible and even equivalent to one another.</p> <p>From curves to derivations:</p> <h2 id="discussions">Discussions</h2> <p>Firstly, note that tangent/cotangent space is an intrinsic? property. A usual image of interpreting tangent space is \(S^2\) embedded in \(\mathbb{R}^3\). Our definition above does not require so, and in fact the shift of perspective from extrinsic into intrinsic properties of geometric objects is a grand evolution starting from Gauss and Rieman.</p> <p>[TODO: An illustrative figure of a sphere embedded in Euclidean space]</p> <p>From derivations to curves:</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Intuition and definition of tangent/cotangent space]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble(2): Ricci Flow and Ricci Curvature (in progress)"/><published>2025-09-17T17:49:33+00:00</published><updated>2025-09-17T17:49:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Ricci_Flows_Curvature/"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Today I will cover a beautiful subject in differenetial geometry: Ricci flows and Ricci curvature.</p> <p>Generally, in Riemannian geometry, curvature measures how space bends. For instance, on a sphere, geodesics (shortest paths) come closer together compared to flat space; on a hyperbolic surface, they diverge.</p> <p>Ricci curvature is a particular way of summarizing curvature: Instead of describing how all directions bend (that‚Äôs what the full <strong>Riemann curvature tensor</strong> does), Ricci curvature focuses on <strong>volume distortion</strong>. More concretely, it tells us how the volume of a small geodesic ball deviates from the volume we‚Äôd expect in flat Euclidean space.</p> <p>Intuitively, we could summarize into the following:</p> <blockquote> <p>Positive Ricci curvature (like on a sphere) means geodesics tend to converge, and small balls have less volume than in flat space.</p> <p>Zero Ricci curvature (like in Euclidean space) means geodesics neither converge nor diverge, so volumes match Euclidean.</p> <p>Negative Ricci curvature (like on a hyperbolic space) means geodesics diverge, so small balls have more volume than Euclidean.</p> </blockquote> <p>Mathematically, Ricci curvature is obtained by ‚Äútracing‚Äù the <strong>Riemann curvature tensor</strong>. It compresses information about how different directions curve into a symmetric 2-tensor <code class="language-plaintext highlighter-rouge">Ric</code>.</p> <h3 id="riemannian-geometry-and-tensor">Riemannian Geometry and Tensor</h3> <p>The Riemann tensor is written in the following way:</p> \[R(X, Y)Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X, Y]} Z\] <p>This tensor captures all information about curvature. Succinctly, this is a 4-tensor: \(R_{ijkl}\).</p> <p>The above is an unfair treatment of Riemannian geometry. I‚Äôll have a separate blog on that subject soon.</p> <p>How to understand: Riemannian metric tensor informs the manifold where to expand, shrink, and curve. How does Riemannian metric tensor relate with curvature?</p> <h3 id="ricci-curvature">Ricci Curvature</h3> <p>Based on the Riemann tensor, what is the curvature?</p> <p>To get Ricci curvature, we take a <strong>trace</strong> of the Riemann tensor:</p> \[Ric_{ij} = R^{k}_{ikj} = g^{kl}R_{kilj}\] <p>This reduces the information down to a 2-tensor (like the metric itself). Geometrically, this represents the volume distortion of geodesic balls.</p> <h3 id="ricci-flow">Ricci Flow</h3> <p>Introduced by <code class="language-plaintext highlighter-rouge">Richard Hamilton (1982)</code>, Ricci flow is a process that evolves a Riemannian metric \(g(t)\) over time:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>The factor -2 is just convention (to simplify later computations). We could think of this as a heat equation for geometry: Just as heat diffuses to smooth out temperature differences, Ricci flow smooths out <strong>irregularities</strong> in curvature. As illustrated in the above section of Ricci curvature, we could naturally arrive at the result that positive curvature regions tend to shrink and negative curvature regions expand. Over time, the underlying geometry becomes more ‚Äúregular‚Äù, like ironing out wrinkles.</p> <p>The effect of Ricci flow on curvature could be expressed in the following way. The derivative of scalar curvature \(R\) under this flow is:</p> \[\frac{\partial R}{\partial t} = \Delta R + 2 |Ric|^2\] <p>This resembles a heat equation $(\Delta R)$ plus a positive correction. Consequently, curvature tends to diffuse out but also grows in positive-curvature regions.</p> <h4 id="examples-of-ricci-flow">Examples of Ricci Flow</h4> <p>Perhaps the simplest example is to imagine how a sphere evolves under Ricci flow. We all know that a round sphere has positive Ricci curvature. The Ricci flow equation says:</p> \[\frac{\partial g_{ij}}{\partial t} = -2 Ric_{ij}\] <p>Since Ricci is positive, the metric shrinks, and thus makes the sphere to contract uniformly, eventually collapsing to a point. This closely mirrors the idea that positive curvature makes geodesics converge. Under the Ricci flow, it tightens further.</p> <p>Another simple example is the flat torus. Since the Ricci curvature is 0 everywhere,</p> \[\frac{\partial g_{ij}}{\partial t} = 0\] <p>the torus will stay unchanged after the flow forever. This is analogous to heat diffusion on a perfectly uniform temperature field where nothing would effectively changes.</p> <p>Likewise, a hyperbolic surface has negative Ricci curvature. Consequently, under the Ricci flow, the metric would expand and the hyperbolic surface would grow larger and more uniform in curvature.</p> <p>In a nutshell, irregular geometries with bumps or folds (different curvature in different regions) get ‚Äúsmoothed‚Äù over time. All the high-curvature ‚Äúwrinkles‚Äù would get flatten out, like how heat equalizes temperature.</p> <h3 id="application-of-ricci-flow">Application of Ricci Flow</h3> <p>Poincare conjecture, Ricci flow, surgery theory, what Terrence Tao called ‚Äúone of the most impressive recent achievements of modern mathematics‚Äù</p> <p>Poincare conjecture:</p> <blockquote> <p>Any closed 3-manifold that is simply-connected, compact, and boundless is homeomorphic to a 3-sphere.</p> </blockquote> <p>Specifically, Poincare conjecture in higher dimensions has been solved around 1961, and dimension 4 case has been proved by Michael Freedman who by which won Fields medal in 1986. The \(n = 3\) case seemed really difficult to crack and it was only at 2002 that Grisha Perelman proved it using Ricci flow.</p> <p>Very briefly, since a sphere has positive curvature, by applying Ricci flow through time such sphere will contract and eventually vanish. Perelman proved the opposite also holds: if metric goes to 0, it must have been a sphere. To prove Poincare‚Äôs conjecture using Ricci flow,</p> <p>One of the most triumphant use of Ricci flow happens when Grigori Perelman (2002‚Äì2003) to prove <strong>the Poincar√© conjecture</strong> and the more general <strong>Thurston geometrization conjecture</strong>. He showed how Ricci flow with ‚Äúsurgery‚Äù (cutting and patching when singularities form) classifies 3-manifolds.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Introduction of Ricci flows/curvatures]]></summary></entry><entry><title type="html">The Dance of Space (2): Differential Forms (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/" rel="alternate" type="text/html" title="The Dance of Space (2): Differential Forms (in progress)"/><published>2025-09-16T16:34:22+00:00</published><updated>2025-09-16T16:34:22+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Differential_Forms</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Differential_Forms/"><![CDATA[<p>The second episode to appreciate the inner workings of space is through the differential forms. Differential forms is not an ancient subject, ‚Ä¶</p> <h3 id="vector-outer-product">Vector Outer Product</h3> <h3 id="wedge-productexterior-derivative">Wedge Product/Exterior Derivative</h3> <h3 id="three-in-one">Three in One</h3> <p>Green‚Äôs theorem, Gauss‚Äô theorem, Stoke‚Äôs theorem.</p> <p>Green‚Äôs theorem:</p> \[\int_{L} Pdx + Qdy = \iint_{D}(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y})dxdy\] <p>Generalized Stoke‚Äôs theorem.</p> <p>A k-form is supposed to be integrated over an oriented k-dimensional manifold</p> <h3 id="fundamental-theorem-of-calculus-ftoc">Fundamental Theorem of Calculus (FTOC)</h3> <p>High dimensional Stoke‚Äôs theorem is exactly the fundamental theorem of calculus (FTOC) in high-dimensional space.</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Intuite and define differential forms]]></summary></entry><entry><title type="html">Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Equivalence/" rel="alternate" type="text/html" title="Equivalence: What does ‚Äúbeing equal‚Äù represent? (in progress)"/><published>2025-09-07T00:23:16+00:00</published><updated>2025-09-07T00:23:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Equivalence</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Equivalence/"><![CDATA[<p>Explain and compare multiple equivalences from differential geometry and topology, including homeomorphism, diffeomorphism, homotopy equivalence, homomorphism, isomorphism, etc.</p> <p>Invariant and equivariant functions (CNN is equivariant).</p> <p>Also discussions on cardinality among sets (including finite and infinite (countably infinite \(N0\)? and uncountably infinite), Hillbert Hotel problem, equipotent sets)</p>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Topology"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Compare various equivalences in geometry/topology/group theory]]></summary></entry><entry><title type="html">Manifold and Riemannian Geometry (1): Rigorous Definition (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Manifold(1)/" rel="alternate" type="text/html" title="Manifold and Riemannian Geometry (1): Rigorous Definition (in progress)"/><published>2025-08-29T02:32:56+00:00</published><updated>2025-08-29T02:32:56+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Manifold(1)</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Manifold(1)/"><![CDATA[<h3 id="preface">Preface</h3> <p>Starting with this blog, I‚Äôll gradually introduce the core elements and the corresponding theories of (differentiable/smooth) manifold, which is, without any undue exaggeration or affected self-indulgence, the most fundamental arean upon which modern geometry unfolds (other than thoeries of curves and surfaces in \(\mathbb{R}^3\)).</p> <p>It‚Äôs only after I seriously learned manifold that I started to appreciate its elegant geometric intuition (unintuited indeed or not easy to grasp at first glance sometimes) and its abstraction and extensions of our known, visually perceived \(\mathbb{R}^3\) into higher dimensional space. It grants us infinite power to have a glimpse of what we are not quite possible at all for our entire lives to interpret and still acquire the confidence to draw conclusions on properties, hierarchies, and variations in the spave above us.</p> <p>As the first blog of the entire manifold Omnibus series, I aim to cover the rigorous definition of manifold, and a general introduction to its properties and different categories, to set the basis for further rigorous discussions.</p> <p>I would refrain from diverting to much from the many interesting discussions (but will review together the definition) of topological space, topology, open sets, Hausdorff space, and homeomorphism.</p> <h3 id="history-and-motivation">History and motivation</h3> <p>Motivation of manifolds:</p> <p>Geometrically, ‚Ä¶</p> <p>Algebraically, solution sets for polynomial equations and differnetial equations.</p> <p>Immanuel Kant‚Äôs idea on the word manifold.</p> <h3 id="a-snapshot">A snapshot</h3> <p>The following might touch upon numerous unfamiliar terms and concepts, no worries I will cover them all as time unfolds. For now, let‚Äôs get a little intuition and some impressionistic perception of this subject.</p> <p>What exactly is a manifold? The most basic kind of a manifold, a topological (n-dimensional) manifold \(M\) is a topological space locally homeomorphic to \(\mathbb{R}^n\). Specifically, for each point \(p \in M\) there exists a chart around \(p\), which is essentially a pair \((U, \Phi)\) such that \(U \subset M\) is an open set(in the topology sense) containing \(p\) and \(\Phi\) a homeomorphism from \(U\) to an open subset of \(\mathbb{R}^n\). For some technical details, the definition of manifolds would also add two other conditions: 1] the Hausdorff property and 2] second-countability. Any manifold has to be a topological manifold at least.</p> <p>The main focus of this series will be on smooth manifolds, which add some nice properties other than being topological manifolds. A smooth manifold acquires a collection of charts compatible with each other, such that the composition of coordinate functions is a diffeomorphism (they form what‚Äôs called a smooth structure on the topological manifold). This nicely ensures differentiability and enables many basic calculus on the manifolds to be valid operations.</p> <p>Examples of manifold include both objects we know and some more abstractly constructed entities: \(\mathbb{R}^n\), any onpen subsets of a manifold, the product space \(M \times N\) (if \(M\) and \(N\) are manifolds).</p> <p>Another famous family of manifolds comes from compact connected 2-manifolds, specifically closed surface with genus \(g &gt; 0\): for example, the familiar \(S^n\) (\(g = 0\)) and the torus \(S^1 \times S^1\) (\(g = 1\)).</p> <p>If we specify our manifolds to be <code class="language-plaintext highlighter-rouge">non-orientable</code>, we would obtain some even more famous/esoteric objects like the (open) <strong>Mobius strip</strong> (open Mobius strip is non-compact, since the boundary is not included) and <strong>Klein bottle</strong> \(K = I \times I / \sim\). From the pure topology standpoint, they are actually quite basic topological spaces. Needless to mention the famous theorem about <strong>classification of surfaces</strong> which states that every compact connected 2-manifold appears as \(S^2\) glued with tori or Mobius stirps (up to homeomorphism). There‚Äôs a little subtlety here in that there do exist ‚Äúmanifolds with boundaries‚Äù which are locally homeomorphic to \([ 0 \times \infty) \times \mathbb{R}^{n-1}\) instead of \(\mathbb{R}^n\), like Mobius strips in a usual sense.</p> <p>Other more abstract example exist such as the projective planes (frequently showing up in Topology) \(\mathbb{RP}^n\cong S^n/\sim\) (with the equivalence relation \(v \sim -v\)). Notice that there‚Äôre multiple ways to define quotient spaces for \(\mathbb{RP}^n\). This example is usually utilized to illustrate quotient space. The Grassmanian Manifold is also frequently studied: \(Gr_k (\mathbb{R}^m) = \{ \mathrm{k\ dimensional\ vector\ subspaces\ of} \; \mathbb{R}^m \}\), where \(0 \leq k \leq m\). This is a compact manifold with dimensions \(k(m-k)\) (Interested readers might come up with a guess or proof). Interestingly, \(Gr_1 (\mathbb{R}^m) \cong \mathbb{RP}^{m-1}\).</p> <p>Since manifold is literally the backbone of modern differential geometry which has evolved into countless sub-branches and domains, it‚Äôs impossible to even sample a fair amount in these blogs. Consequently, I‚Äôll be mainly focusing the following:</p> <ul> <li>Definitions of smooth manifolds</li> <li>Submanifolds and embeddings <ul> <li>An embedding is an injective smooth map from \(M\) to \(N\) which is also an isormorphism between \(M\) and a submanifold of \(N\)</li> </ul> </li> <li>Tangent and cotangent spaces <ul> <li> \[p \in M, dim(M) = n, T_p N \cong \mathbb{R}^n\] </li> <li>The differential of smooth maps between manifolds \(M, N\) is a <strong>linear</strong> map from \(T_p M\) to \(T_{f(p)}N\)</li> <li>The cotangent space is the dual vector space of \(T_p M\), containing elements as \(d_p f \in T_p^{*}M\), where \(f: M \rightarrow \mathbb{R}\) is smooth</li> </ul> </li> <li>Tangent and cotangent bundles <ul> <li>\(\pi: TM (\mathrm{2n\ dimensional}) \rightarrow M\), where \(\pi^{-1}(\{ p\}) = T_p M\)</li> <li>Example of vector bundle</li> <li>Similarly a cotangent bundle \(Y: T^{*}M \rightarrow M\)</li> </ul> </li> <li>Tangent and cotangent fields <ul> <li>A <strong>section</strong> of the tangent bundle \(X: M \rightarrow TM\) is a tangent field (or tangent vector field, or vector field on \(M\)): \(X(p) \in T_pM\)</li> <li>\(X\) as a vector field, \(X(p) \in T_pM, \forall p \in M\), tracing out a flow on \(M\)</li> <li>This is framed in the language of ODE and flows</li> <li>A <strong>section</strong> of the cotangent bundle is a cotangent field (or 1-forms, or differentials)</li> <li>Cotangent fields are differential 1-forms, if \(f: M \rightarrow \mathbb{R}\) is smooth, \(p \in M\), then \(d_pf \in T_p^{*}M\)</li> <li>Frobenius integrability theorem</li> </ul> </li> <li>Differnetial \(k\)-forms on manifold \(M\) <ul> <li>Tensor bundles and tensor fields</li> <li>A section of the \(k\)th exterior power of the cotangent bundle \(T^{*}M\) is a differential \(k\)-form</li> <li>0-forms: functions on \(M\)</li> <li>1-forms: cotangent fields</li> <li>k-forms: sections of the \(k\)th exterior power of \(T^{*}N\)</li> <li>de Rham derivative and de Rham cohomology groups: \(d: \Omega^{k}M \rightarrow \Omega^{k+1}M\)</li> <li>For \(M\) compact and orientable, \(\omega \in \Omega^n M\), \(\int_{M} \omega \in \mathbb{R}\)</li> </ul> </li> <li>General Stokes‚Äôs Theorem <ul> <li>If M is compact, oriented, with boundary, n-dimensional and \(\omega \in \Omega^{n-1}M\), then \(\int_{M}d\omega = \int_{\partial M} \omega\)</li> </ul> </li> <li>Lie group and Lie algebra <ul> <li>Manifold \(+\) group at the same time</li> </ul> </li> </ul> <h3 id="something-dark-or-darker">Something dark, or darker</h3> <p>One last note to keep before we starting diving in, especially for readers from computational/systems neuroscience who probably have heard of the concept <code class="language-plaintext highlighter-rouge">neural manifold</code>. For the past few decades, as the technologies of simultaneous recordings of multiple neurons quickly evolve, data recorded as an entire neural population has gradually become a solid reality for many subdomains of neuroscience, and the level of data analysis promptly and adaptively shifts from the single-neuron perspective to neural population analysis. Many theories have been developed to cope with such high dimensional data (could be as many as several hundreds), like the dynamical system perspective as <a href="/blog/2025/jPCA/, /blog/2025/Constraint-Learning/">my previous two blogs covered</a>, or neural manifold. This is really not a novel idea, as in many domains there‚Äôs abundance of observation that among the high dimensional data there exists some low dimensional structure that captures the <code class="language-plaintext highlighter-rouge">patterns</code> of data.</p> <p>However, in most cases there‚Äôs virtually nothing lost if we simply swap the word ‚Äúneural manifold‚Äù into, say, ‚Äú<strong>low dimensional structure</strong>‚Äù. What worries me is that at least up to now there is no theoretical proof that a collected neural population is indeed a (at least topological) manifold. Also, from an ontological perspective, because other than employing this word itself, there‚Äôs very few rigorous adaptation or usage of the many theorems and properties associated to a manifold, the true essence behind the entire domain of manifold theory is left untouched at all. Indeed, we might argue that at a figurative level such neural manifold is a nice symbol for building up intuition, but there‚Äôs still a conceptual subtlety that the very first motivation of a manifold is to extend the familiar Euclidean space around us to higher dimensional spaces. In other words, the charm of manifolds shines in the high dimensional space, which, without further notice, might comletely mislead people into linking ‚Äúmanifold‚Äù with ‚Äúlow dimesional space‚Äù. To be fair, in computational/systems neurscience, even when ‚Äúneural manifold‚Äù is used to indicate ‚Äúlow dimensional space‚Äù, it may still be 10 or 30, which for mathematicians are usually ‚Äúhigh‚Äù (high vs low is a relative concept. I don‚Äôt think there‚Äôs a disagreement with regard to the aboslute ‚Äúhigh‚Äù). For neuroscientists, ‚Äúlow dimension‚Äù is low because of its direct comparison to the original space in \(\mathbb{R}^{384}\), for example; for a geometer, both \(\mathbb{R}^{10}\) and \(\mathbb{R}^{384}\) are ‚Äúhigh dimensional space‚Äù because they extend beyond \(\mathbb{R}^{3}\). Finally, manifold not only extends \(\mathbb{R}^n\) to \(\mathbb{R}^N\), where \(n &gt;&gt; N\), but more importantly entirely discards the many constraints of Euclidean space and works with more general spaces (indeed, as in the shift of focus of intrinsic vs extrinsic properties).</p> <p>I never doubt that some low dimensional structure is imbedded in neural population, as countless research has corroborated it. And I deeply appreciate the remarkable wisdom and significant efforts others have paid along this line of research under the name of neural manifold. In the end, I‚Äôm just hoping concepts to stay clear and to minimize the downplay of any important concepts that are both aesthetically appearling and fundamentally stunning. I‚Äôm reluctant to observe those ideas get muddled, mumbled, and straggled among hypes and later turned into buzzwords that, at the down-to-earth applied level, mistakes its potential and hides the key questions regarding how to adapt the abstract manifold concepts into codable algorithms, or at the abstract level noisify our perception of the world (as many other instances already in an awkard situation, like ‚Äúcomputation‚Äù or ‚Äúrepresentation‚Äù).</p> <p>Perhaps I‚Äôm being a little over-reactive?</p> <p>Or‚Ä¶perhaps not so surprising, in recent years among Deep Learning there‚Äôs more and more serious consideration and introduction of embedding geometry/topology into the modeling pipelines (geometric deel learning, interested readers might want to check out Michael Bronstein). I highly enjoy the geometric approaches whether in DL or neuroscience, and I‚Äôm very glad that I live in this era when I could witness the entire trajectory of geometry involvement. Meanwhile, I‚Äôll do my best to clarify things and introduce more into the pure beauty of geometry. Enough sentiment, now let‚Äôs jump in.</p> <h3 id="a-bit-of-topology">A bit of topology</h3> <p>Topological sapce:</p> <p>Homeomorphism:</p> <h3 id="manifold-entry-point-topological-manifold">Manifold entry point: Topological manifold</h3> <p>Definition of a topological manifold:</p> <blockquote> <p>A topological space \(M\) is an \(n\)-dimensional topological manifold if</p> <p>1] \(\forall p \in M\), there exists an open neighborhoood \(U\) (\(p \in U \subset M\)) such that there exists a homeomorphism between \(U\) and an open subset of \(\mathbb{R}^n\)</p> <p>2] \(M\) is Hausdorff</p> <p>3] \(M\) is second-countable</p> </blockquote> <p>The first requirement is the frequently cited notion that a manifold is <em>locally like</em> a Euclidean space, but rigorously defined in terms of topological equivalence. The third condition about second-countability indicates that there exists a countable set of topolgy bases. This is required for guaranteeing the existence of the partition of unity on \(M\) subordinate to any open cover.</p> <p>Conditions 1] and 3] could be combined into one statement:</p> <blockquote> <p>\(M\) is the union of finite or countably infinite open subsets \(U \subset M\) which is homeomorphic to some open subsets of \(\mathbb{R}^n\)</p> </blockquote> <p>Whenever a manifold is mentioned without any prefix, it‚Äôs a topological manifold. More conditions could be specified to add more ‚Äúflavors‚Äù to the manifold: like if we prescribe a metric to the manifold to obtain a Riemannian manifold. But without adding more complicated structure, we will only ask for something simpler to include: the differentiability, which leads to the main subject of this series of blogs: differentiable/smooth manifolds. But before going there, let‚Äôs take a look at few examples that are not manifolds (to also deepen our interpretation of the definition):</p> <h3 id="what-is-not-a-manifold">What is not a manifold?</h3> <p>1] An ‚Äú8‚Äù-shaped structure is not a manifold, since near the center it‚Äôs not homeomorphic to any Euclidean space.</p> <p>2] A long line constructed by the first uncountable ordinal number (this violates the countability criterion)</p> <p>3] A canonical example that fails the Hausdorff condition is a line with two origins: \(X = \mathbb{R}_1 \sqcup \mathbb{R}_2 / \sim\) where \(x \sim y\) if and only if \(x = y \neq 0\) (so the two 0s \(0_1 \in \mathbb{R}_1, 0_2 \in \mathbb{R}_2\) are still distinct). These two 0s cannot be distinguished because they share the same neighbors all the time. Around the origin we cannot tell from the neighbourhood if it‚Äôs \(0_1\) or \(0_2\). The topology forces neighbourhoods to overlap too much such that the Hausdorff property fails. This example could also be extended into 2D versions.</p> <p>Another slightly different counterexample for Hausdorffness is not on separating some point, but glueing spaces together: Consider two copies of Euclidean plane: \(\mathbb{R}^2_1, \mathbb{R}^2_2\), and glue them on the right half plane</p> \[H = \{(x, y) \in \mathbb{R}^2 | x \geq 0 \}\] <p>Equivalently, \(X = \mathbb{R}^2_1 \sqcup \mathbb{R}^2_2 / \sim\), where \((x_1, y_1) \sim (x_2, y_2)\) iff \(x \geq 0\). Obviously, \(X\) is not Hausdorff since there‚Äôs not disjoint neighbourhood around \((0, 0)_1\) and \((0, 0)_2\).</p> <h3 id="collection-of-descriptors-charts-and-atlas">Collection of descriptors: Charts and atlas</h3> <p>The above definition emphasizes the essence of manifold that we could only approach it locally. To derive a global view of a maifold is enabled by taking many local ‚Äúcameras‚Äù and projections together. Consequently, we naturally arrive at the following definition:</p> <blockquote> <p>Def: An n-dimensional chart for \(M\) is a pair \((U, \Phi)\) such that \(U \subset M\) is an open sutset, \(\Phi: U \rightarrow \Phi(U)\) a homoemorphism from \(U\) to an open subset of \(\mathbb{R}^n\).</p> <p>Def: A chart for \(M\) is a collection \(\{U_{\alpha}, \Phi_{\alpha} \}_{\alpha \in A}\) such that \(\bigcup_{\alpha \in A} U_{\alpha} = M\)</p> </blockquote> <p>With the concept of chart and atlas, we now are fully capable of pulling the manifold into different Euclidean frames to analyze them, since a chart \((U, \Phi)\) determines real-valued functions \(x_1, x_2, \dots x_n\) such that \(\Phi(p) = (x_1(p), x_2(p), \dots, x_n(p))\) where \(p \in U \subset M\) and \(x_i: U \rightarrow \mathbb{R}^n\). These \(x_i\) are called the coordinate functions of the selected chart.</p> <p>Some readers might have a concern now. If there exist some charts \((U, \Phi)\) and \((V, \Psi)\) where \(U \cap V \neq \varnothing\), then \(\Phi(U \cap V)\) and \(\Psi(U \cap V)\) will have different coordinates. Would this cause a mess?</p> <h3 id="the-real-arena-smooth-manifold">The real arena: Smooth manifold</h3> <p>Given \(f: u \subset \mathbb{R}^n \rightarrow \mathbb{R}\), where \(u\) is an open set, \(f\) is \(C^k\) if the \(k\)th derivative \(\frac{\partial^k f}{\partial x_j^k}\) exists and is continuous \(\forall 1 \leq j \leq n\). Likewise, for a multivariable function \(f: u \subset \mathbb{R}^n \rightarrow \mathbb{R}^m\), \(f = (f_1, f_2, \dots f_m)\) is \(C^k\) if \(f_i\) is \(C^k \; \forall i\). If we take \(k\) to be \(\infty\), then \(f\) is called a smooth. In my blogs, whenever any function is called differentiable without specifying the \(k\)th order, we will assume \(k\) as \(\infty\) and thus a differentiable function is the same as a smooth function. A differentiable manifold is the same as the smooth manifold, both I‚Äôll use interchangeably.</p> <p>There‚Äôs a another way to define smooth of functions in a coordinate-free manner (with linear approximation other than calculating derivatives (See Extra Notes # 2.))</p> <p>Another important concept is called diffeomorphism. Diffeomorphism in \(\mathbb{R}^n\) goes like this: suppose \(\Phi: u \in \mathbb{R}^n \rightarrow v \in \mathbb{R}^n\)</p> <p>\(u, v\) both being open subsets. \(\Phi\) is a diffeomorphism if \(\Phi\) is smooth, \(\Phi\) exists, and \(\Phi^{-1}\) is also smooth. In that case, if \(y = \Phi(x)\), then the matrix \((\frac{\partial y_i}{\partial x_j})\) is invertible. If a function is a diffeomophism, then does it have to be a homeomorphism?</p> <p>The definiton of homeomorphism and charts allow us to pull functional analysis from \(C^{\infty}(M)\) or \(C^{\infty}(M, N)\) on \(M\) into \(\mathbb{R^n}\) itself and thus we could proceed with techqniues built within the Euclidean space. Later when defining tangent/cotangent space from the geometric standpoint, we will see another side of the same story.</p> <p>The point of adding smooth structure to a topological manifold: this enables us to characterize a real-valued function \(f\) on \(M\) to be smooth or not ‚Äî- for a chart \((U, \Phi)\), \(f\) is smooth when it composed with the inverse map \(\Phi^{-1}: \Phi(U) \rightarrow U\): \(f \circ \Phi^{-1}: \Phi(U) \subset \mathbb{R}^n \rightarrow \mathbb{R}\) is smooth (the composed function \(f \circ \Phi^{-1}\) maps from Euclidean space to Euclidean space so we could solicit the familiar definitions of smoothness/differentiability). Careful readers might have spotted a subtle issue: the above definition of smoothness (the composed function) hinges upon the specific selection of chart. So we are led to use only some charts, not all charts.</p> <h3 id="a-few-others">A few others?</h3>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><category term="Topology Concepts/Tools"/><summary type="html"><![CDATA[Rigorous definition of manifold and related topology concepts]]></summary></entry><entry><title type="html">Geom/Topo/Dynam Mumble (1): Exponential map (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Exponential-map/" rel="alternate" type="text/html" title="Geom/Topo/Dynam Mumble (1): Exponential map (in progress)"/><published>2025-08-29T02:25:33+00:00</published><updated>2025-08-29T02:25:33+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Exponential-map</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Exponential-map/"><![CDATA[<p>Exponential map as dynamical flow in differential geometry and dynamical systems.</p> <h1 id="differential-geoetry-lie-group">Differential geoetry (Lie group)</h1> <h1 id="dynamical-system-linear">Dynamical system (Linear)</h1>]]></content><author><name></name></author><category term="Differential Geometry"/><category term="Geometry Concepts/Tools"/><summary type="html"><![CDATA[Exponential maps applied in Lie group & dynamical systems]]></summary></entry><entry><title type="html">Learning and Constraints (in progress)</title><link href="https://jasmineruixiang.github.io/blog/2025/Constraint-Learning/" rel="alternate" type="text/html" title="Learning and Constraints (in progress)"/><published>2025-08-25T22:09:34+00:00</published><updated>2025-08-25T22:09:34+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/Constraint-Learning</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/Constraint-Learning/"><![CDATA[<p>This blog covers two papers which focuses on exploring constraints during learning in monkey‚Äôs behavior from different perspectives. The first one <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a> approaches the constraint as a spatial restriction of neural activities residing upon some low dimensional neural manifold, while the second <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a> deliberately perturbs the motion sequence, the temporal order movement to explore whether the monkeys are capabable of adapting neural dynamics. In terms of methods, these two papers share much in common in terms of latent space calculation, and in order not to make this blog unbearably long, I will elaborate more on the first paper‚Äôs algorithms, while glossing over many details in the second. In total, through juxtaposing these studies, hopefully we could glean some integrated thoughts on the constraints of neural dynamics form both the spatial and temporal perspectives.</p> <p>Both spatial (neural manifold view) and temporal (dynamical system view) on constraints of neural activities.</p> <h1 id="spatial-constraints-sadtler-etal-2014">Spatial constraints (Sadtler et.al. 2014)</h1> <h2 id="experiment-setup">Experiment setup</h2> <p>For <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>, two male Rhesus macaques were trained to perform closed-loop BCI cursor task (Radial 8). Around 85-91 neural units (threshold-crossings) were recorded. The experiment pipeline is demonstrated below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1a.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 1a in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. Note that on the right hand side the authors already presented where to place the two kinds of perturbation. The color scheme (green, yellow, and black) is consistent throughout figures in this paper. </div> <h2 id="decoding-paradigm">Decoding paradigm</h2> <h3 id="dimensionality-reduction-technique">Dimensionality reduction technique</h3> <p>The control space is just 2D because the decoder output is cursor velocities in \(\mathbb{R}^2\), illustrated as black line in Fig.2 black line (Note: it‚Äôs actually a 2D plane, but here for simplicity shown as a black line (\(\mathbb{R}^1\))).</p> <p>They used Factor Analysis ({cite (Factor-analysis methods for higher-performance neural prostheses) }{cite (Gaussian Process Factor analysis)}; I‚Äôll write a blog on GPFA later) to extract what they called the ‚Äúintrinsic manifold‚Äù, which captures the co-modulation patterns among the recorded neural population. This is shown as the underlying yellow plane in Fig.2 (might be confusing, but it‚Äôs not the 2D control space). Note that at the time of publication, neural manifold was not yet in a popular trend, so the authors briefly characterized the term ‚Äúintrinsic manifold‚Äù with the following illustration:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1b.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 1b in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. . </div> <p>The authors further elaborated on the intrinsic manifold and its associated dimensionality at the end of the paper. For consistency of comparisons, Sadtler et. al. used a <strong>linear</strong> 10-D intrinsic manifold across all days. They then performed some offline analysis to explore if 10 is a legitimate choice. Specifically, they estimated the intrinsic dimensionality (EID) as the peak of maximal cross-validated log-likelihood (LL). In panel a the vertical bars represent the standard error of LL from across 4 cross-validation folds. Panel b. shows EID for all days and both 2 monkeys. The authors also showed the LL difference between 10D manifold vs manifold with EID (panel c., with units being the the number of standard errors of LL for the EID model). From panel c. the authors observed that 89% of days the 10-D manifold only differs within one standard error of LL with the EID manifold. Panel d. indicates the cumulative explained variance by the 10-D manifold. Notice that 10 dimensions already explained almost all neural variance.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_4.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.4 in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <p>The factor analysis method works in the following way (I‚Äôll keep the same notation as the paper). Let‚Äôs assume the high dimensional neural signal (here the z-scored spike counts) acquired every 45ms time bin is denoted as \(u \in \mathbb{R}^{q \times 1}\) (naturally, \(q\) neural units), and \(z \in \mathbb{R}^{10}\) the latent variable. Factor analysis assumes the observed neural activity is related to the unobservable latent variables under a Gaussian distribution:</p> \[\begin{equation} u \mid z \sim N(\Lambda z + \mu, \psi) \end{equation}\] <p>where the latent vector is assumed to come from</p> \[\begin{equation} z \sim N(0, I) \end{equation}\] <p>Here the covariance matrix \(\psi\) is diagonal. Consequently, the intrinsic manifold is defined on the span of the columns of \(\Lambda\), and each column of \(\Lambda\) represents a latent dimension where \(z\) encodes the corresponding projections/coordinates. All three parameters \(\Lambda, \mu, \psi\) are estimated from <strong>Expectation-Maximization (EM)</strong> method (I‚Äôll also write a blog about this later, especially how it as a classical inferenc engine is closely related to Evidence Lower Bound (<strong>ELBO</strong>), a populat loss/objective function for modern-day generative models based on DNN like VAE and Diffusion).</p> <h3 id="intuitive-mapping">Intuitive mapping</h3> <p>The intuitive mapping is selected by fitting a modified Kalman Filter ({cite Wu W. Gao Y., Bayesian population decoding of motor cortical activity using a Kalman filter}). Specifically, for each <strong>z-scored</strong> spike count \(z_t\), after obtaining the posterior mean \(\hat{z}_{t} = E[z_t \mid u_t]\) and <strong>z-scoring</strong> each dimension (these z-scorings are important, which will be stressed a multiple times later), the authors started with the common linear dynamical system (LDS) assumption of Kalman Filter:</p> \[\begin{align} x_t \mid x_{t-1} &amp;\sim N(Ax_{t-1} + b, Q) \\ \hat{z}_t \mid x_t &amp;\sim N(Cx_t + d, R) \end{align}\] <p>The parameters \(A,b,Q,C,d,R\) are obtained by maximum likelihood estimation, where \(x_t\) is the estimate of monkey‚Äôs intended velocity (label for the data). Since the spike counts and the latent factors were both <strong>z-scored</strong> and the calibration kinematics were centered, \(\mu = d = b = 0\).</p> <p>Consequently, by filtering the goal is to estimate \(\hat{x}_{t} = E[x_t \mid \hat{z}_{1}, \;, ... \;, \hat{z}_{t}]\). The authors directly gave out the formula below to express \(\hat{x}_t\) in terms of the final decoded velocity at the previous step \(\hat{x}_{t-1}\) and the current z-scored spike count \(u_{t}\):</p> \[\begin{equation} \hat{x}_t = M_1 \hat{x}_{t-1} + M_2 u_t \end{equation}\] \[\begin{equation} M_1 = A - KCA \end{equation}\] \[\begin{equation} M_2 = K\Sigma_{z}\beta \end{equation}\] \[\begin{equation} \beta = \Lambda^T(\Lambda \Lambda^T + \Psi)^{-1} \end{equation}\] <p>where \(K\) is the steady-state Kalman gain matrix. As part of the process of z-scoring the latent factors, \(\Sigma_z\) is a <strong>diagonal</strong> matrix whose diagonal element (\(p, p\)) refers to the inverse of standard deviation of the \(pth\) factor. Since both spike counts and latent factors are <strong>z-scored</strong>, the perturbed mappings (see in the next section) ‚Äúwould not require a neural unit to fire outside of its observed spike count range‚Äù.</p> <p>The above formula might sound confusing, so I present below a detailed derivation. It‚Äôs not so complicated but readers who are not interested in derivation feel free to skip it.</p> <h4 id="derivation-of-the-iterative-filtering-equation">Derivation of the iterative filtering equation</h4> <p>I‚Äôll derive the above formula (5 - 8) in the following 3 steps. In the end, this is nothing brand new and elusive.</p> <blockquote> <p>Step 1: Obtain the posterior of \(z \mid u\)</p> <p>Step 2: z-score the latents</p> <p>Step 3: Apply Kalman filter</p> </blockquote> <h5 id="step-1-linear-gaussian-system">Step 1: Linear Gaussian system</h5> <p>I‚Äôll start with a well-known fact about linear Gaussian system (this derivation is also the core of Gaussian Process and Kalman Filter; Stop for a second and marvel again at the all-encompassing power of Gaussian distribution). Assume two random vectors \(z \in \mathbb{R}^m\) and \(x \in \mathbb{R}^n\) which follow the Gaussian distribution:</p> \[\begin{equation} p(z) = N(z \mid \mu_z, \Sigma_z) \end{equation}\] \[\begin{equation} p(x \mid z) = N(x \mid Az + b, \Omega) \end{equation}\] <p>The above illustrates a <strong>linear Gaussian system</strong>. Note that \(A \in \mathbb{R}^{n \times m}\). Consequently, the correponsding joint distribution \(p(z, x) = p(z)p(x \mid z)\) is also a Gaussian with an \((m + n)\) dimensional random vector:</p> \[\begin{equation} p(z, x) = N( \begin{bmatrix} z \\ x \end{bmatrix} \mid \tilde{\mu}, \tilde{\Sigma}) \end{equation}\] <p>where</p> \[\begin{align} \tilde{\mu} &amp;= \begin{bmatrix} \mu_z \\ A\mu_z + b \end{bmatrix} \\ \tilde{\Sigma} &amp;= \begin{bmatrix} \Sigma_z &amp; \Sigma_z A^T \\ A\Sigma_z &amp; A\Sigma_z A^T + \Omega \end{bmatrix} \end{align}\] <p>The above could be easily derived from matching the corresponding moments, so I will not show in full details. From this joint Gaussian, we could thus easily continue to write out the posterior distribution:</p> \[\begin{equation} p(z \mid x) = N(z \mid \mu', \Sigma') \end{equation}\] <p>where</p> \[\begin{equation} \mu' = \mu_z + \Sigma_z A^T(A\Sigma_z A^T + \Omega)^{-1}(x - (A \mu_z + b)) \end{equation}\] \[\begin{equation} \Sigma' = \Sigma_z - \Sigma_z A^T(A\Sigma_z A^T + \Omega)^{-1}A\Sigma_z \end{equation}\] <p>The above posterior is known as <strong>Bayes‚Äô rule for Gaussians</strong>. It states that if both the prior \(p(z)\) and the likelihood \(p(x \mid z)\) are Gaussian, so is the posterior \(p(z \mid x)\) (equivalently, Gaussian prior is a <strong>conjugate prior</strong> of Gaussian likelihood or Gaussians are <strong>closed under updating</strong>, <a class="citation" href="#pml2Book">(Murphy, 2023)</a> P29). One interesting fact is that although the posterior mean is a linear function of \(x\), the posterior covariance is entirely independent of \(x\). This is a peculiar property of Gaussian distribution (Interested readers please see more explanations in <a class="citation" href="#pml2Book">(Murphy, 2023)</a> sections 2.3.1.3, 2.3.2.1-2, and 8.2). Finally, keen readers might already perceive the equation (15,16) prelude the form of the Kalman Filter posterior update equations.</p> <p>From the above posterior Gaussian form, by plugging in the notations specified in (1-2) with \(z = z_t, \; x = u_t, \; \mu_z = 0, \; \Sigma_z = I \;, A = \Lambda, \; b = \mu \;, \Omega = \Psi\) we obtain the following:</p> \[\begin{equation} p(z_t \mid u_t) = N(z_t \mid \mu_{post}, \Sigma_{post}) \end{equation}\] <p>where</p> \[\begin{align} \mu_{post} &amp;= 0 + I \Lambda^T(\Psi + \Lambda I \Lambda^T)^{-1}(u_t - (\Lambda 0 + \mu)) \\ &amp;= \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}u_t \end{align}\] <p>and</p> \[\begin{align} \Sigma_{post} &amp;= I - I\Lambda^T(\Psi + \Lambda I \Lambda^T)^{-1}\Lambda I \\ &amp;= I - \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}\Lambda \end{align}\] <p>I deliberately used a different set of notations for the posterior linear Gaussian system, so if you are interested please do this little derivation on your own. Since \(\hat{z}_t = E[z_t \mid u_t]\), from above we know that</p> \[\begin{equation} \hat{z}_t = \Lambda(\Psi + \Lambda \Lambda^T)^{-1}u_t \end{equation}\] <h5 id="step-2-perform-z-scoring">Step 2: Perform z-scoring</h5> <p>The second step is to z-score the posterior mean \(\hat{z}_t\), which, here, is dividing each position of this vector by the corresponding standard deviation:</p> \[\begin{equation} \hat{z}_{t, z-scored} = \begin{bmatrix} \hat{z}_{t}^{1} / \sigma_{1} \\ \hat{z}_{t}^{2} / \sigma_{2} \\ \vdots \\ \hat{z}_{t}^{p} / \sigma_{p} \end{bmatrix} = \begin{bmatrix} \frac{1}{\sigma_1} &amp; 0 &amp; 0 &amp;\cdots &amp; 0 \\ 0 &amp; \frac{1}{\sigma_2} &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; &amp; \ddots &amp; &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \frac{1}{\sigma_{p}} \end{bmatrix} \hat{z}_t = \Sigma_z \hat{z}_t \end{equation}\] <p>For simplicity, for the below I‚Äôll replace \(\hat{z}_{t, z-scored}\) with \(\hat{z}_t\).</p> <h5 id="step-3-apply-kalman-filter">Step 3: Apply Kalman Filter</h5> <p>As in Step 1, let me right down the filtering equation for a pure Kalman Filter based on the Gaussian linear assumption made in (9-10) (For the following I‚Äôll drop off \(b, d\) since they are 0). The notations below might be a little messy, but I want to keep it rigorous. The hat on \(x, P\) refer to estimates not ground truth, and \(t \mid t-1\) indicates prior estimation while \(t \mid t\) indicates posterior estimation at time point \(t\).</p> \[\begin{align} \hat{x}_{t|t-1} &amp;= A\hat{x}_{t-1|t-1} \\ \hat{P}_{t|t-1} &amp;= A\hat{P}_{t-1|t-1}A^T + Q \\ K_t &amp;= \hat{P}_{t|t-1}C^T(C\hat{P}_{t|t-1}C^T + R)^{-1}\\ \hat{x}_{t|t} &amp;= \hat{x}_{t|t-1} + K_t(\hat{z}_{t} - C\hat{x}_{t|t-1}) \\ \hat{P}_{t|t} &amp;= (I - K_tC)\hat{P}_{t|t-1} \end{align}\] <p>Again, we play the trick of substitution, starting with (27):</p> \[\begin{align} \hat{x}_{t\mid t} &amp;= \hat{x}_{t \mid t-1} + K_t(\hat{z}_{t} - C\hat{x}_{t \mid t-1}) \\ &amp;= A\hat{x}_{t-1 \mid t-1} + K_t(\hat{z}_t - CA\hat{x}_{t-1 \mid t-1}) \\ &amp;= (A - K_tCA)\hat{x}_{t-1 \mid t-1} + K_t \hat{z}_t \\ &amp;= (A - K_tCA)\hat{x}_{t-1 \mid t-1} + K_t\Sigma_z( \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}u_t) \\ &amp;= (A - K_tCA)\hat{x}_{t-1 \mid t-1} + K_t\Sigma_z \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}u_t \end{align}\] <p>Finally, if we define \(M_1 = A - KCA\), \(M_2 = K\Sigma_z \beta\), \(\beta = \Lambda^T(\Psi + \Lambda \Lambda^T)^{-1}\) (formula (6-8)), we‚Äôd claim what we saw in (5):</p> \[\hat{x}_t = M_1 \hat{x}_{t-1} + M_2 u_t\] <p>What I wrote as \(\hat{x}_{t \mid t}\) is the posterior prediction which the authors denoted \(\hat{x}_{t}\) for simplicity (same logic for \(t-1\)). The only subtlety that remains here is that in the above derivation I used the dynamic Kalman Gain \(K_t\), calculated for every time point $t$. In the paper the authos utilized steady Kalman Gain (that‚Äôs why there‚Äôs no subscript \(t\)) but that‚Äôs basically iterate the above filtering equations many times with the given set of model parameters (\(A,Q,C,R\)) until \(K_t\) converges to some matrix \(K = \lim_{t \rightarrow \infty} K_t\), and then use that matrix for all time steps. Basically, you could just replace \(K\) with \(K_t\) without changing the backbone of the inference structure.</p> <p>Before we jump into the perturbation method, the formula (5) does inform us the central philosophy of Kalman Filter: it integrates model prediction by its specified linear dynamics and the observation together, to arrive at an optimal (I‚Äôll not dive deep into what optimality represents here) posterior inference. Extracting out the linear relationship between prediction at timestep \(t\) and the previous step \(t-1\) together with the observation input would help understanding the perturbation method below.</p> <h2 id="perturbation-method">Perturbation method</h2> <p>Then the core methodology of this study is to change the BCI mapping so that the altered control space would be lying either within or outside of the insintric manifold. The paper does present some confusion as to how intuitive mapping and control space would be distinguished. My interpretation is that the control space refers to the ideal potential neural subspace for which to control the cursor optimally. Since within a short time neural connectivity is kept unaltered, the true intrinsic manifold is approximately invariant and thus the required potential neural subspace might not be reachable. By default the control space/intuitive mapping lies within the intrinsic manifold (that‚Äôs why it‚Äôs called ‚Äúintuitive‚Äù, because that‚Äôs is what the neural network system has learned to achieve).</p> <p>The neuronal connectivity statistics is referred to as the natural co-modulation patterns.</p> <p>In short, a within-manifold perturbation only reoriented the control space such that it still resides in the intrinsic manifold (shown in Fig.3 red line). This does not require monkeys to readapt neuronal connectivity patterns to achieve such new control space. It only altered the function from the intrinsic manifold to cursor kinematics. On the other hand, an outside-manifold perturbation alters the control space allowing it to live off the intrinsic manifold (Fig.3 blue line). Notice that if such outside-manifold perturbation pushes the control space along the orthogonal subspace that passes through the original control space, then the mapping from the neural comodulation patterns to cursor kinematics is preserved (basically just project the altered control space to the intrinsic manifold, then would recover the original control space). However, the underlying comodulation/covariation patterns among the neural population are altered.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_1c.png" class="img-fluid rounded z-depth-1" width="45%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.1c in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <p>After the perturbation, the authors observed if the monkeys could eventually learn to readapt to the new mapping, to achieve great cursor control performance. For within-manifold perturbation, the monkeys only need to learn to associate cursor kinemaitcs to a new set of neural comodulation patterns (still within reach because lying in the same intrinsic manifold). However, for outside-manifold perturbation, they had to generate new co-modulation patterns in order to reach outside of the existing intrinsic manifold. Consequently, the authors predicted that within-manifold perturbation is easier to learn compared to outside-manifold perturbation. The authors did find results to back up this claim and since they are not the main focus of this blog, I‚Äôll refer interested readers to the original paper to take a look (FIgure 2).</p> <h3 id="perturbation-as-permutation">Perturbation as permutation</h3> <p>Other than that, I do want to dive deep into how such within/outside-manifold perturbations were implemented. Specifically, from the derivation section readers should be already familiar with equations (5-8), the intuitive mapping. The perturbed mappings are the corresponding modified versions.</p> <p>The within-manifold perturbation still manages to maintain the relationship between neural units to latent factors, but perturb that between latents and cursor kinematics: \(\hat{z}_{t}\) is permuted before going into the Kalman inference (multiplying with the Kalman Gain)(Figure 1 red). Since permutation is simply re-orientation, the within-manifold perturbation is equivalent to re-orientating the coordinate system within the manifold (the manifold is preserved because the row space of \(\Lambda^T\) is preserved (equation (7))). Mathematically,</p> \[\begin{align} \hat{x}_t = M_1 \hat{x}_{t-1} + M_{2, WM}u_t \\ M_{2, WM} = K\eta_{WM}\Sigma_z\beta \end{align}\] <p>and \(\eta_{WM}\) is a \(10 \times 10\) permutation matrix (10 because of the latent dimensionality)</p> <p>For outside-manifold perturbation, it changes the relationship between the neural units and latent factors by permuting \(u_t\) before passing it into factor analysis (Figure 1 blue). Specifically,</p> \[\begin{align} \hat{x}_t = M_1 \hat{x}_{t-1} + M_{2, OM}u_t \\ M_{2, OM} = K\Sigma_z\beta \eta_{OM} \end{align}\] <p>and \(\eta_{OM}\) is a \(q \times q\) permutation matrix (\(q\) is the number of neural units). The underlying logic is that in the <strong>neural space</strong> for \(u_t\), the monkeys might not be able to adapt to conteract the perturbation brought by permutation, since \(\eta_{OM}\) directly acts upon \(u_t\).</p> <h3 id="select-perturbed-mappings">Select perturbed mappings</h3> <p>The authors also devised a clever plan to dictate the specific permutation matrices to choose (for a permutation matrix with size \(k \times k\), there‚Äôre \(k!\) numbers of them) in three steps, with the central goal that the perturbed mapping would not be too difficult nor easy to learn:</p> <h4 id="step-1-find-the-candidate-set">Step 1: Find the candidate set</h4> <p>For within-manifold perturbations, all \(10!\) possible permutation matrices are treated as the candidate set. For outside-manifold perturbations, th strategy differs for two monkeys. For one monkey only permutations of nueral units with largetst modulation depths are selected. For the other monkey, the solution is to randomly put all units with the highest modulation depths into 10 groups of \(m\) each (the rest with low modulation forms an 11th group). The outside-manifold perturbation is formed by permutating these 10 groups instead of each unit (thus \(10!\) in total matching that of within-manifold perturbation).</p> <h4 id="step-2-open-loop-velocities-prediction-per--perturbation">Step 2: Open-loop velocities prediction per perturbation</h4> <p>The second step hinges on estimating the open-loop velocities for each candidate permutation. Specifically, it approximates the decoded cursor kinematics if the monkeys did not learn to adapt:</p> \[\begin{equation} x_{OL}^i = M_{2, P}u_{B}^i \end{equation}\] <p>where \(u_{B}^i\) is the mean z-scored spike counts across all trials on the \(i^{th}\) target(\(8\) in total), and \(P\) represents either \(OM\) or \(WM\). The method here echoes the dissection made explicity in equation(5), where the current step prediction \(\hat{x}_t\) is a linear combination of prediction from last step \(\hat{x}_{t-1}\) and neural activities at the present \(M_2 u_t\).</p> <h4 id="step-3-determine-potential-perturbations">Step 3: Determine potential perturbations</h4> <p>Finally, to determine a perturbation the authors compared the open-loop velocities under the perturbed mapping with those under the intuitive mapping for each target. These velocities should only differ in an acceptable range so the monkeys would not find it too simple nor difficult to learn. The authors quantified this metric by defining a range over differences in velocity angles and magnitude, and chose perturbations that fall in this specified range for all targets.</p> <h2 id="quantifiable-metric">Quantifiable metric</h2> <h3 id="amount-of-the-learning">Amount of the learning</h3> <p>To quantify the potential amount of learning under two perturbation kinds, the authors resorted primarily to two performance metric: (the change of) relative acquisition time and relative success rate across perturbation blocks. Specifically, as shown below, the black dot represents the intuitive mapping, while the red and blue dots indicate the imediate performance just after corresponding perturbations. Red and blue asterisks represent the best performance during the within the perturbation sessions. The dashed line indicates the maximum learning vector \(L_{max}\) (note that it starts on the red dot), and thus the aount of learning (\(A_i \in \mathbb{R}\)) is quantified as the length of the projection of the raw learning vector onto the maximum learning vector, normalized by the length of the maximum learning vector:</p> \[A_i = \frac{L_{raw, i} \cdot L_{max}}{\|L_{max}\|^2}\] <p>where \(i \in \{red, blue\}\). Pictorially, it‚Äôs illustrated as below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Fig_2cd.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.2c and Fig.2d in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <p>Note that the asterisks in the above represent the time point/bin corresponding to <strong>maximal</strong> amount of learning. In real case, for each time bin the authors would pinpoint the end points for the learning vectors (for calculations in details, please refer to the METHODS section of the paper), and compute the amount of learning correspondingly (the red and blue learning vectors might end in different positions with a diferent set of relative acquisition time and success rate up to that time bin).</p> <p>The amount of learning for all sessions was presented above in the right pannel. Notice that a value of 1 indicates ‚Äúcomplete‚Äù learning of the new relationship between the required neural co-modulation and cursor kinematics, reverting to the performance level of the intuitive control, while 0 indicates no learning. The authors did observe that there‚Äôs significant amount of learning for within-manifold perturbations than outside-manifold perturbation. To see changes in success rates and acquisition time during perturbation blocks, instead of a single metric \(L\) as shown above, the authors also plotted them separately as below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Ext_Fig_2.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.2 in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <h3 id="after-effects">After-effects</h3> <p>A second metric Sadtler et. al. employed is observe how the monkeys performed as they reintroduce the intuitive mapping, or the so-called after-effects after washout of the perturbed mapping. Specifically, the after-effect is measured as the amount of performance imparement (tentative: acquisition time, success rate) at the beginning at the wash-out block (like how impairement was measured at the beginning of a perturbation block). A large wash-out effect indicates that the monkeys have learned and adapted to the perturbed mapping. For within-manifold perturbation, the authors did observe brief impaired performance but not so for outside-manifold perturbation, indicating that learning did occur during the former.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Ext_Fig_3.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.3 in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>. </div> <h3 id="principalcanonical-angles">Principal/canonical angles</h3> <p>To quantify the comparisons between the intuitive and the perturbed mappings, Sadtler et.al. also calculated the principal angles between two linear subspaces. Notice that by formula () above, both subspaces are spanned by the rows of \(M_2\) (\(M_{2, WM}\) for within-manifold perturbation, and \(M_{2, OM}\) for outside-manifold perturbation). Consuquently, the two principal angles specify the maximum and minimum angles of separation between the intuitive and the perturbed control spaces. Notice that since the spike counts are z-scored, the control spaces also center at the origin.</p> <p>Sidenote: How do principal angles relate with principal eigenvalues and principal curvatures?</p> <p>To give a short summary of principal angles calculation:</p> <p>Note the role of SVD decompsition.</p> <h2 id="discussions">Discussions</h2> <p>Since Sadtler et. al. employed closed-looop BCI control, they were able to causally alter the model/map from neural activities to the decoded cursor velocities.</p> <p>This paper highlights a potential methodology of BCI research: since the mapping from neural activities to control correlates is <strong>fully specified</strong>, thus could be causally perturbed to explore the corresponding changes of controlled behavior. This allowed the authors to <strong>design/know apriori</strong> the optimal/required neural activities (specified by the altered mapping) to achieve task success, and thus to observe if animals could generate such neural patterns.</p> <p>An outside-manifold perturbation does not necessarily specify that it lives in orthogonal subspace of the intrinsic manifold. Also, because here the intrinsic manifold is illustrated as a plane, in real world scenario, it is unlikely that it exists as a linear subspace. Consequently, specifying a space that is ‚Äúorthogonal‚Äù to the potential manifold (nonlinear, other than a linear subspace) might be problematic.</p> <p>The amount of learning is entirely dependent upon performance itself, which is difficult to causally link to neural changes.</p> <p>For the amount of learning metric, why would 0 indicate no learning? Orthogonal learning?</p> <p>Notice that the after-effects analysis echoed a lot of research methodology in force-field or curl-field perturbation for cursor control in monkey motor control studies.</p> <p>The authors also showed that learning did not improve through sessions (readers might refer to Extended Figure 4 in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a> for further information).</p> <p>The perspective and consideration that Sadtler et.al took to ensure alternative explanations for the observed distinction of learnability do not hold are informative. I enjoyed its rigorosity, especially when they considered perturbed maps which might be initialy difficult to learn and carefully implement the controls (demonstrate that they have controled). To not diverge from the main focus of this blog, I‚Äôll not cover those dicussions. The way that the authors listed clearly alternative explanations and how they tackled each is a very inviting, powerful, and efficient way of writing.</p> <p>From the methods the authors used, they only estimated a linear manifold. According to {cite}, linear manfolds should require more dimensions than a nonlinear manifold which could explain similar amount of variance.</p> <p>The dissection of control into an estimation of intrinsic manifold (Factor Analysis) and then build an intuitive mapping (decoder, Kalman Filter) to relate the latent factors to cursor kinematics is different from directly mapping neural activities to movement. Such dissection becomes a common theme for the past two decades, with the emphasis on latent manifold structure beecomes increasingly popular. More than a theoretical refinement, practically speaking, such dissection also allows the authors to perform two different types of perturbation.</p> <p>Talk more here and also relate this to the nonlinear manifold paper 2025.</p> <p>For defining the candidate set of potential outside-manifold perturbation for the second monkey, while the group looping strategy is a clever way to equalize possible permutation matrices with within-manifold perturbation (\(10!\)), I‚Äôm not completely persuaded by the logic behind it. When explaning the logic for the second monkey outside-manifold perturbation, the authors stated that the within-maifold perturbation only permutes the neural space with \(10\) dimensions, while the outside-manifold perturbation on average deal with 39 dimensions (number of permuted units). Thus the monkey would have to search for more space for outside-manifold perturbation. I think the subtlety here lies in the fact that within-manifold perturbation is not performed on the level of the ambient neural space, but on the latent 10-dimensional space which is extracted out of the original neural space and each basis vector of the latent space is a <strong>linear combination</strong> of the neural units. Its is not explicitly clear to me whether/how these two spaces should be compared solely based on the dimensonality it carries.</p> <p>Additionally, though I do appreciate the flavor of group assignment (there‚Äôs even a flavor of group action here; anyway, permutation matrices form a permutation group), I am not sure if each of the \(10!\) permutations on groups of neural units is ‚Äúequivalent‚Äù to a permutation among 10 latent axes.</p> <p>Another question is outside-manifold perturbation is not necessarily guaranteed to be outside the manifold?</p> <h3 id="required-changes-in-preferred-directions">Required changes in preferred directions</h3> <p>There‚Äôs one interesting method that I don‚Äôt have enough time to delve into, which is the calculation of changes in preferred directions for each neural unit. This is calculated to make sure that learning two perturbation types would require similar effort for the monkeys to adapt (interested readers might refer to Figure 3b in the paper). The authors began by discussing comparing the columns of \(M_2, \; M_{WM, or OM}\), which reflects how each neural unit impacts the decoded cursor kinematics. This strategy is not adopted because for the second monkey \(M_2\) and \(M_{OM}\) share many columns for the un-permuted columns, making it an unfair comparison. However, it‚Äôs informative if we associate this view of \(M_2 u_t\) by assiging each column of \(M_2\) with a coordinate in \(u_t\) with that mentioned in the principal angles section where rows of \(M_2\) represnet an axis upon which \(u_t\) is projected. These two perspectives which interpret linear matrix multiplication as either a <strong>transformation</strong> (of basis vectors in space) or a <strong>projection</strong> which will be further illustrated in an upcoming post.</p> <p>Then, the authors came up with another technique. They assumed that</p> <blockquote> <p>1] under perturbation the monkeys would still manage to keep the same cursor velocity in the intutive mapping,</p> <p>2] The perturbed firing rates should be as close as possible to those in the intuitive mapping</p> </blockquote> <p>and these two assumptions transform into a constrained optimization problem: Find \(u_p^{i}\) such that its Euclidean distance with \(u_B^{i}\) is minimized when \(M_2u_B^i = M_{2, P}u_p^i\):</p> <blockquote> \[u_p^{i, *} = \arg\min_{u_p^i} ||u_p^i - u_B^i||_2\] <p>s.t. \(M_2u_B^i = M_{2, P}u_p^i\)</p> </blockquote> <p>This can be solved in closed-form with Lagrange multipliers (for rigorosity, the inverse below should be replaced with the Moore-Penrose Pseudoinverse, unless \(M_{2, P}\) is full-rank, which I‚Äôm not sure I could theoretically make that claim). Due to limited space, I‚Äôll not leave the proof to another blog on linear transformation, which I also illustrate its relationship with CCA and another paper (Juan Gallego, trajectory alignment):</p> \[\begin{equation} u_p^i = u_B^i + M_{2, P}^T(M_{2, P}M_{2, P}^T)^{-1}(M_2 - M_{2, P})u_B^i \end{equation}\] <p>where \(u_B^i\) is the mean normalized spike count vector across all trials for each target \(i\) in the basline blocks.</p> <p>Then the authors fit a standard cosine tuning model for each unit \(k\) with all targets:</p> \[\begin{equation} u_B^i(k) = m_k \cdot cos(\theta_i - \theta_B(k)) + b_k \end{equation}\] <p>where for each neural unit \(k\), its preferred direction is encoded as \(\theta_B(k)\), \(m_k\) the depth of modulation, \(b_k\) the model offset, \(\theta_i\) the direction of the \(ith\) target. Apply the same calculation for \(u_P^i\) to obtain the preferred direction \(\theta_P(k)\) for each unit \(k\) under the perturbaed mapping. Finally, the preferred direction changes (for each neural unit) is calculated as:</p> \[\begin{equation} \mid \theta_P(k) - \theta_B(k) \mid \end{equation}\] <h3 id="selection-of-intrinsic-dimensionality">Selection of intrinsic dimensionality</h3> <p>Usually we do not have a coherent and systematic way to detemrine the optimal intrinsic dimensionality. Here for factor analysis, based on its explicit probabilistic inference structure, the authors could easily compute the likelihood for cross-validated data.</p> <h3 id="measurement-of-cumulative-shared-variance">Measurement of cumulative shared variance</h3> <p>Based on equations (13), the original covariance of \(u\) is decomposed (with minor substitutions) into a shared component \(\Lambda \Lambda^T\) and an independent component \(\Psi\). In order to calculate the amount of shared variance along orthogonal directions within the manifold (notice this is a linear manifold), consequently, the authors calculated the eigenvalues of \(\Lambda \Lambda^T\) which present the shared variances, each corresponds to an orthonormalized latent dimension. This is similar to Churchland 2012 the last blog‚Ä¶</p> <h2 id="conclusions">Conclusions</h2> <p>The neural manifold reflects the inherent connectivity which constrains (in a short term) the potentially learnable patterns. Consequently, the neural connectivity network structure dictates possible neural patterns and corresponding behavior repertoire the animals are capable of performing.</p> <p>This paper strengthens my belief in the legit usability of the low dimensional structure among neural population, and more crucially the value of perturbation methods to causally verify the neural manifold. Specifically, the extraction of latent factors, other than directly mapping neural activties to cursor kinematics, not only adds more interpretability to the framework, but also provies a readily distinguishable strategy of within/outside-manifold perturbations. This reminds me of many other models with latent factors in between: (xxx, xxx, xxx, xxx).</p> <h1 id="dynamical-constraints-oby-etal-2025">Dynamical constraints (Oby et.al. 2025)</h1> <p>This paper <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a> presented some surprising facts about neural dynamics. One key question the authors are interested is whether the sequential representation or computation that neural population could perform is temporally locked. The critical assumption is that if neural dynamics does reflect the underlying connectivity, then it should be robust and difficult to alter. More specifically, they tested if such neural population patterns could be produced but in a <strong>reversed</strong> time ordering and the results show otherwise: even when the animals were presented with different visual feedback or strong incentive to change the time order (for example, reversing the time course) of the neural activities, the temporal evolution of the neural dynamics is still robust and difficult to violate (thought I think the claim might be too strong. See more in the discussions).</p> <h2 id="different-views-of-the-high-dimensional-neural-space">Different views of the high dimensional neural space</h2> <div class="row mt-3"> <div class="col-sm-7"> <p> Note that unlike many previous BCI works, neural activities are mapped to curosr __positions__ directly instead of velocities. The modeling framework is similar to <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>, in that high dimensional neural activities <strong>90D</strong> are mapped into <strong>10D</strong> latents by <strong>GPFA</strong> (instead of simply <strong>FA</strong> in <a class="citation" href="#Sadtler2014">(‚ÄúNeural Constraints on Learning,‚Äù 2014)</a>), and then build corresponding linear maps from <strong>10D</strong> latent factors to <strong>2D</strong> cursor positions using different maps. Since each map is a linear projection of latent factors to <strong>2D</strong> space, it is geoemtrically equivalant to observing the high dimensional signals from a specified angle. The key ingredient of this paper is that the authors found if with some linear <strong>2D</strong> mapping/projection, the neural trajectories are readily flexible and reversible, whereas some other views robustly exhibited no significant change even if the monkeys were inspired to alter the neural trajectories. </p> <p> There're two mappings the authors emphasizd, one is called "movement-intention" (MoveInt) mapping, under which the monkeys could intuitively control the cursor to move between two diametrically placed targets <strong>A</strong> and <strong>B</strong>. Indeed, under MoveInt, the neural trajectories (equivalent to cursor motion here) could go back and forth between <strong>A</strong> and <strong>B</strong> with significant overlapping. This might lead readers to believe that the neural dyanmcis are also reversible. </p> </div> <div class="col-sm-5 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Adapted from Fig. 2 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. a: decoding pipeline. b: Overlapping neural/cursor trajectories under multipl orientations. From this it seems that time courses of neural dynamics are flexible for different orientations (thus including reversing the temporal order). </div> </div> </div> <p>However, under another 2D mapping selected as maximizing the separations (\(SepMax\)) between \(A\) to \(B\) trajectories v.s. \(B\) to \(A\) trajectories, the neural trajectories overlapping by \(MoveInt\) projection are clearly distinguished. This is shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_3.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 3 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. See a. and b. where A-B and B-A trajectories are distinct. Panels c. and d. show results from quantitative metric (discriminability <strong>d'</strong> between midpoints of trajectories signify separation of trajectories). </div> <h3 id="moveint-vs-sepmax-projections">MoveInt vs SepMax projections</h3> <p>Both \(MoveInt\) and \(SepMax\) projections are calculated by fitting a simple linear transformation to the extracted \(10D\) latents \(\hat{z}_t\) to acquire \(2D\) cursor positions \(\hat{x}_t\), which is used to displace the cursor in closed-loop control:</p> \[\begin{equation} \hat{x}_t = W\hat{z}_t + c \end{equation}\] <p>except they have different weight matrices and bias \(W\) and \(c\).</p> <p>For \(MoveInt\) projection, \(\hat{x}_t\) is selected as the label velocity vectors, and thus \(W_{MI} \in \mathbb{R}^{2 \times 10}\) and \(c_{MI} \in \mathbb{R}^{2 \times 1}\) are obtained by the correpsonding linear regression:</p> \[\begin{align} W_{MI} = XZ^{T}(ZZ^T)^{-1} \\ c_{MI} = -W_{MI}(\frac{1}{n}\sum_{t=1}^{n}\hat{z}_t) \end{align}\] <p>where \(Z = [\hat{z}_1, \hat{z}_2, ..., \hat{z}_n] \in \mathbb{R}^{10 \times n}\) represents the collection of \(GPFA\) latent states, and \(X = [x_1, x_2, ..., x_n] \in \mathbb{R}^{2 \times n}\) indicates the intended cursor position. \(n\) is the total number of timesteps during calibration.</p> <p>For \(SepMax\) projection, the goal is to identify subspace upon which cursor trajectories showcased maximal separation during the two-target task from target pair \(A\) to \(B\) and \(B\) to \(A\). The projections is acquired by satisfying the following 3 objectives: 1] maximizing separation between midpoints of both ways (\(\bar{z}_{AB}\) and \(\bar{z}_{BA}\)); 2] minimizing trial-to-trial variance at midpoints (\(\Sigma_{AB}\) and \(\Sigma_{BA}\)); and 3] maximizing projections of \(\bar{z}_{A}\) and \(\bar{z}_{B}\).</p> <p>The algorithm for \(SepMax\) projection could be summarized in the following:</p> <blockquote> <p>1] Compute trial-averaged starting points \(\bar{z}_{A}, \; \bar{z}_{B}\) from \(A\) to \(B\) and \(B\) to \(A\) trials, respectively,</p> <p>2] Calculate midpoint \(m = \frac{\bar{z}_{A} + \bar{z}_{B}}{2}\)</p> <p>3] For a given trial, project the trajectory \(\hat{z}_t\) onto the axis connecting \(\bar{z}_{A}\) and \(\bar{z}_{B}\). Define \(\hat{z}_{t_{c}}\) as the trial midpoint, where \(t_{c}\) is the timepoint where the projection is closest to \(m\),</p> <p>4] Define \(\bar{z}_{AB}\) as the trial-averaged midpoint over \(\hat{z}_{t_{c}}\) from \(A\) to \(B\) trajectories, similarly \(\bar{z}_{BA}\) from \(B\) to \(A\),</p> <p>5] Simiar to 4], calculate covariance \(\Sigma_{AB}\) over \(\hat{z}_{t_{c}}\) for \(A\) to \(B\) trials, and simiarly \(\Sigma_{BA}\) for \(B\) to \(A\) trials,</p> <p>6] Finally, to obtain the ideal \(2D\) projection and find two orthonormal vectors collected in \(P_{SM} = [p_1, p_2] \in \mathbb{R}^{10 \times 2}\), solve the optimization problem below:</p> \[J = -w_{mid}p_{1}^T(\bar{z}_{AB} - \bar{z}_{BA}) + w_{var}p_{1}^T(\Sigma_{AB} + \Sigma_{BA})p_1 - w_{start}p_{2}^T(\bar{z}_B - \bar{z}_A)\] <p>7] To align the space from \(P_{SM}\) with animals‚Äô workspace: \(W_{SM} = AP_{SM}^T, \;, c_{SM} = -AP_{SM}^Tm\)</p> <p>where</p> \[A = R_{\theta}OS\] </blockquote> <p>Note that \(\bar{z}_{A}, \bar{z}_{B}, m, \bar{z}_{AB}, \bar{z}_{BA} \in \mathbb{R}^{10 \times 1}\), \(W_{SM} \in \mathbb{R}^{2 \times 10}, A \in \mathbb{R}^{2 \times 2}, c_{SM} \in \mathbb{R}^{2 \times 1}\) and \(w_{mid}, w_{var}, w_{start}\) are all weighting hyperparameters. The above objective function is composed of 3 separate aims which match exactly the those mentioned in above. Consequently, along \(p_1\) trajectories of different directions are maximally separated, while targets are distinguished on an orthogonal \(p_2\) axis.</p> <p>The goal for step 7] is that after projection the starting points of \(A\) to \(B\) and \(B\) to \(A\) align with the two targets \(A\) and \(B\) in animals‚Äô workspace. \(A\) is comprised with 3 transformations:</p> <blockquote> <p>1) Scaling the axes of \(P_{SM}\) with a diagonal matrix \(S \in \mathbb{R}^{2 \times 2}\) such that distance between \(\bar{z}_A\) and \(\bar{z}_B\) is identical to \(A\) and \(B\) in \(MoveInt\) projection,</p> <p>2) Optionally flip the projection along the \(A - B\) axis with \(O \in \mathbb{R}^{2 \times 2}\) (see more in discussion), such that the controlled cursor movements align with the expected orientation,</p> <p>3) Rotation with \(R_{\theta}\) so the projection aligns with workspace targets \(A\) and \(B\).</p> </blockquote> <p>The above process is illustrated below in panels d. and e. Notice that the above postprocessing is not an isometry, in that a scaling is also applied (\(P_{SM}\)).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Ext_Fig_10.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.10 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. d., e.: demonstration of the above quantities. f.: projection of the midpoints of both way trajectories, <span class="math">\(\bar{z}_{AB}\)</span> and <span class="math">\(\bar{z}_{BA}\)</span>, onto an axis <strong>a</strong> with maximal midpoint separation. The discriminability <strong>d'</strong> is calculated from the projected mean and variance: <span class="math">\(d' = \frac{|\bar{z}_{AB}^Ta - \bar{z}_{BA}^Ta|}{\sqrt{\tfrac{1}{2}a^T(\Sigma_{AB} + \Sigma_{BA})a}}\)</span> </div> <p>After identifying the existence of irreversible neural trajectories, the authors continued to explore how robust time course evolution is, with 3 experiments that built upon the previous ones which increasingly motivated the monkeys to adapt neural dynamics.</p> <h2 id="task-1-visual-feedback-task">Task 1: Visual feedback task</h2> <p>Firstly, the authors gave monkeys the visual feedback of neural trajectory mappings by \(SepMax\) projection (not \(MoveInt\)). Based on the underlying inherent tendency to streighten cursor trajectories, the authors intended to observe if the monkeys would indeed made such adjustments, which would then demonstrate that temporal evolution of neural patterns is flexible. However, the authors found that the cursor trajectories were robust and were independent of which visual feedback the monkeys received. This further corroborates the claim that the monkeys did not have volitional control over certain time order of neural trajectories. The experiment paradigm and results are summarized below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_4.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 4 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. Given visual feedback from the <strong>SepMax</strong> projection (b.), the authors still do not observe straightening of cursor trajectories for <strong>SepMax</strong> projection (c.). In fact, projections in either linear subspace are robust whether which visual feedback is presented (d.). </div> <p>One key remaining question is whether the robustness of constraint exists only in the \(SepMax\) projection subspace, or if it‚Äôs a common phenomena also observable in other dimensions. Consequently, the authors applied random \(2D\) projections of the \(10D\) latents and conducted flow field analysis (I did not have space to go into details, but flow field analysis was employed to quantify the discrimination of trajectories).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Ext_Fig_4.png" class="img-fluid rounded z-depth-1" width="90%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.4 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. Latent factors are projected into <strong>2D</strong> space by random projection matrices and the flow field is calculated. This is repeated for 400 times and the corresponding comparisons of flow fields in the corresponding projections under different feedbacks (<strong>MoveInt</strong> or <strong>SepMax</strong>) are shown in cyan distribution as "other feedback" in g. and h. Note that d. is an example comparison of flow fields for <strong>SepMax</strong> projection. In order to interpret the other-feedback distributions the authors also devised two control groups where no change or maximal change in flow fields is expected. For no-change distribution, subsets of trials from the same feedback is used to calculate flow fields, called "fixed feedback". For maximal change, two distributions are devised: one is reversing the direction of the flow field so they overlap and are maximally different (called "time-reversed", e.), and the other with altered starting targets (called "alternate-target", f.). The difference in flow field is min-max normalized by fixed feedback and time reversed feedback, respectively, the smaller the less difference among flow field comparisons. g. indicates that other feedback distribution is not significantly different from fixed feedback. h. shows that state space highly overlaps between fixed feedback, other feedback, andn time reversal. Taking stock, the other feedback distributions show low flow difference under and hight flow overlapping under different feedbacks, suggesting that the neural trajectories are indeed robust in the <strong>10D</strong> latent space. </div> <p>Note that the above figure only demonstrates that neural trajectories are robust and highly consistent/<strong>constrained</strong> under different feedbacks in many different dimensions, viewed from distinct perspectives/projections. This does not necessarily translate to the unversality of separability of trajectories in different subspaces other than \(SepMax\) projection. It is the former, the inability to alter neural trajectories under whatever visual feedback in some projected space, that the authors refer to as the <strong>constraint</strong>. It is this constraint that the authors found could be extended to many other dimensions (other than just \(SepMax\) projection) by the distribution from random projections (other feedback) shown above.</p> <h2 id="task-2-it-task">Task 2: IT task</h2> <div class="row mt-3"> <div class="col-sm-7 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Adapted from Fig.6 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. Looking at the flow field generated by the previous two-target trials (a.), the authors explored if the monkeys could move back against the flow field (b.), to achieve an "intermediate target" (IT) on the midway (c.). Black lines: single-trial trajectories, thick lines being the trial average. For quantification, initial angles of trajectories were calculated under multiple comparison groups. d.: trajectory to IT (black) v.s. direct path against the flow field (black dashed), compared to trajectory to the blue target (red) v.s.direct path (black dashed), showed as % difference in e. 0% indicates the cursor movement is along the flow field, whereas 100% implies that monkeys could move against the flow field. The authors also performed similar analyses with no-change (f. and g.: initial angles between direct path to IT and early trials (red), with direct path to IT and late trials(dashed red)) and full-change control (h. and i.: initial angles between directh path to IT with different targets (here blue and aqua)) </div> </div> <div class="col-sm-5"> <p> In the second task, the authors explicitly motivated the monkeys to present the neural activities (by the cursor movement) in a time-reversed ordering by motivating them to move against an empirically derived flow field (from past cursor trajectories) towards an "intermediate target" (<strong>IT</strong>) along the path from <strong>A</strong> to <strong>B</strong>. The authors observed that in order to go from target <strong>B</strong> to <strong>IT</strong>, the monkeys did not adopt the time-reversed pathway of <strong>A</strong> to <strong>B</strong>, but followed, at least initially, the path specified by the flow field from <strong>B</strong> to <strong>A</strong> and then reverted back towards <strong>IT</strong>. Consequently, the monkeys failed to reproduce the existing neural activities patterns in the reversed time-ordering, and they did not (initially) violated the flow field. This is quantified by initial angles of trajectories across multiple control/comparison groups ('no-change': early vs. late trials, where there is no expectation of difference; 'full-change': difference of initial angles among trials of different end targets (same starting target) in the <strong>MoveInt</strong> projection), and the authors discovered that the initial angles during the IT task is not significantly different from the 'no-change' condition, but not with 'full-change' condition. This demonstrates that the time course of neural population is robust and not easily modifiable. </p> </div> </div> <h2 id="task-3-instructed-path-task">Task 3: Instructed path task</h2> <p>In the third task, in order to further motivate the monkeys to reverse neural trajectories, the authors appied an ‚Äúinstructed path task‚Äù where a visualb boundary was applied to the time-reversed trajectory. However, the monkeys only minimally modified the cursor trajectories as the bounday was reduced in size, and the trajectories followed the flow field instead of violating it while showcasing the reverting/hooking phenomena in the \(IT\) task. This again demonstrates that the temporal evolution of neural activity patterns is robust and thus it is robustly constrained.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Fig_7.png" class="img-fluid rounded z-depth-1" width="85%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig.7 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. The monkeys were required to move the cursor to the IT (as in Task 2) within the visual boundary (a.), with the boundary decreasing in size to motivate monkeys to change trajectories (b., failed trials shown in red). c.: five boundary sizes with different distances from the boundary to the target(dashed line), and trial-averaged trajectories for successful trials only for each boundary. d. and e.: Comparison of the initial angle between direct IT path (black dashed) and the trial-averaged trajectory of all trials (black), and that between direct IT path (black dashed) and trial-averaged trajectories in the two-target taks (task 1, dark red). The distribution of percent difference is similar to the no-change distribution (dark red triangle and line) seen in Fig.6. indicating that even with visual boundaries the cursor trajectories are still robust. </div> <h2 id="discussions-1">Discussions</h2> <p>I like how the experiments are built upon each other to add more constraints and motivation for the monkeys to reverse the time course of neural activities. The quantification methods used (flow field analysis, initial angle comparison among trajectories) are great tools to add to future research. Many adjustments and tweaks of experiment sessions to enable further exploration are interesting to read, like for Task 1 the authors also flipped the projection on the screen along the axis connecting two targets, in order to observe if cursor trajectories would also be reflected or not (details not discussed in this blog, illustration shown below; should be self-explanatory).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/learning_constraint/Oby_Ext_Fig_5.png" class="img-fluid rounded z-depth-1" width="85%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Extended Fig.5 in <a class="citation" href="#Oby2025">(‚ÄúDynamical Constraints on Neural Population Activity,‚Äù 2025)</a>. When reflection of the projection is added (a., rotation matrix <strong>O</strong>), two possilibies based on whether the neural trajectories were dependent on visual preference (curvature is preserved, Possibility 1) or the underlying network connectivity (curvature is also flipped, Possibility 2). c. shows that the orientation of the cursor trajectories under the reflected <strong>SepMax</strong> feedback in the reflected <strong>SepMax</strong> projection is also flipped compared to those under the <strong>SepMax</strong> feedback in the <strong>SepMax</strong> projection (A typo above the third panel of c., should be "Reflected Separation-Maximizing Projection" instead of "Reflected Separation-Maximizing Feedback"). d.: quantified flow field difference between <strong>SepMax</strong> and reflected <strong>SepMax</strong> projections (under corresopnding feedbacks) is significantly larger than the difference between <strong>SepMax</strong> under <strong>MoveInt</strong> and <strong>SepMax</strong> feedbacks. Consequently, the neural trajectories are determined not by visual preference but underlying connectivity constraints. </div> <p>A dynamical system does not simply allow flowing back.</p> <p>Instead of an abrupt change of the experiment, graduallly apply the changes: not to go to reversa in total in an instant: reminds me of hysterisis. The conclusion seems so strong.</p> <p>Would this be a byproduct of the training process itself? ‚Äî- if the monkeys were trained with visual feedback of the maximum separation view, would that be different?</p> <h3 id="linear-mapping">Linear mapping</h3> <p>Linear manifold, linear mapping, orthogonal/null space</p> <h3 id="dynamical-systems-explanation">Dynamical systems explanation</h3> <p>The authors claimed that the observed time course activities reflect the underlying connectivity among the neural population. They made associations with network models where at any present, the activities of each node is determined by the previous time behavior of all nodes, the network connectivity, and the external input. This is explicitly reflected by the dynamical systems perspective (let‚Äôs not go to SDE for now):</p> \[\begin{equation} \dot{x}(t) = f(x(t)) + u(t) \end{equation}\] <p>Discretizing the above would reveal previous time step dependence, and the dynamics is specified by \(f\) determined by network connectivity. The paper <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a> discussed in <a href="/blog/2025/jPCA/">my earlier post</a> made this the backbone of modeling. From this perspective, it seems not surprising that neural trajectories do not necessarily need to be reversible. One extreme hypothetical case would: Imagine the following simple \(2D\) linear dynamical system:</p> \[\begin{equation} \dot{x}(t) = x(t) \end{equation}\] <p>The phase space could be visualized as below:</p> <p>It‚Äôs not difficult to observe that the vector field on each state \(\dot{x}_t\) is perpendicular to the state itself \(x_t\). Consequently, the dynamics exhibit as pure rotation. If the starting point is \(p_1\), then to move towards \(p_2\), it cannot go counter-clockwise, although it seems closer, but it has to travel through longer distance clockwise and reach \(p_2\).</p> <p>Consequently, to alter the time course would require substantial adjustment of the connectivity itself to change \(f\), which in a short time span is not quite likely readily achievable.</p> <h3 id="constraints-on-the-input">Constraints on the input</h3> <p>Since the monkeys were assumed to freely change their neural activities input to \(M1\), the fact that they were no capable of altering the trajectory indicates that changing inputs could only affect the dynamics in a limited way. The authors hypothesized that perhaps the inputs could only affect certain dmensions of neural dynamics (for example, shown by \(MoveInt\)), or it could not overcome the intrinsic dynamics the neural network carries. s</p> <h3 id="hysteresis-explanation">Hysteresis explanation</h3> <h3 id="eventual-but-not-initial-violation-of-flow-field">Eventual, but not initial, violation of flow field</h3> <h3 id="determinism-and-the-exception">Determinism and the exception</h3> <p>To arrive at a given state \(z(t)\), \(z(t-1)\) is pre-determined. The only exception is attractor and limit cycle?</p> <h3 id="relation-with-rotational-dynamics">Relation with rotational dynamics</h3> <p>The study might remind us of rotational dynamics (cite jPCA paper). Previous study showed that simply by reversing the hand kinematics would not necessarily lead to reversal of the direction of the rotational dynamics (% cite russo2018 %). What‚Äôs even more striking here is that there‚Äôs no arm movement component included here, thus further emphasizing that kinematic movement or somatosensory feedback (as shown coupling many monkey studies) is not a necessary condition for engendering the neural dynamics, which nonetheless carries on an inherent property of the underlying neuronal connectivity.</p> <h3 id="relation-with-low-tangling">Relation with low-tangling</h3> <p>On another hand, the monkeys were motivated to produce neural latents that evolve with high-tangeling, but in fact neural trajectories showed only low tangling.</p> <h3 id="relation-with-output-nullpotent-space">Relation with output-null/potent space</h3> <p>Finally, this does echoe the flavor of output-potent/null subspace (another blog coming soon). Indeed, the distingushed properties of neural dynamics observed in \(MoveInt\) v.s. \(SepMax\) projections echoe the observation that different perspectives of viewing neural dynamics might lead to functionally distinguished inerpretations: preparatory activities encoded in the null-space are not read out as they are projected onto the ‚Äúsame‚Äù coordinates on the readout output-potent space, but they specify the initial conditions to drive the temporal evolution of neural activities along the output-potent space.</p> <h2 id="conclusions-1">Conclusions</h2> <p>The animals‚Äô dynamical structure is robust and highly constrained.</p> <h1 id="discussions-2">Discussions</h1> <p>These two studies offer powerful information that dimensionality reduction could be not just a visualization tool, but a causal summary of the underlying neural connectivity and anatomical constraints, which correlates to the neural computations that neural population could implement.</p> <p>Associating Sadtler 2014 with Churchland 2012, there‚Äôs a common convergence on using matrices to explore transformations. This again reinforces my idea that perhaps group theory needs to be rigorously introduced into neursocience for ‚Ä¶ (also associate with Barack and Kraukauer ‚ÄúTwo views on the cognitive brain‚Äù, which relates <strong>computation</strong> to <strong>transformation of representations</strong> to explain cognitive phenomena)</p> <h1 id="conclusions-2">Conclusions</h1>]]></content><author><name></name></author><category term="Brain-Computer Interface"/><category term="Learning"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Neural onstraints, both spatial and temporal]]></summary></entry><entry><title type="html">Rotational Dynamics in Neural Population</title><link href="https://jasmineruixiang.github.io/blog/2025/jPCA/" rel="alternate" type="text/html" title="Rotational Dynamics in Neural Population"/><published>2025-08-16T18:30:16+00:00</published><updated>2025-08-16T18:30:16+00:00</updated><id>https://jasmineruixiang.github.io/blog/2025/jPCA</id><content type="html" xml:base="https://jasmineruixiang.github.io/blog/2025/jPCA/"><![CDATA[<h2 id="preface">Preface</h2> <blockquote> <p>All changed, changed utterly:</p> <p>A terrible beauty is born.</p> <p>‚Äì‚Äì‚Äì‚Äì William Butler Yeats, <em>Easter, 1916</em></p> </blockquote> <hr/> <p>Many years later, when I reflect back on the first quarter of the 21st century studies on computational neuroscience and brain-computer interface (BCI), among papers on fancy Neural Network based decoders and clinical breakthroughs expanding from cursor and motor BCI to speech and vision, I might still recollect a distant afternoon when I first heard the name Mark Churchland, and more specifically, neural population dynamics and jPCA (Avid readers of <em>Gabriel Garc√≠a M√°rquez</em> might be smiling or provoked at the blatantly imitated time traversal imbued in this sentence structure). ‚ÄúTerrible beauty‚Äù, that‚Äôs the phrase which came to my mind at that time. Since then, such impression has taken its roots only deeper as we observe an ongoing and accelerating burst of more research with similar flavor supporting the dynamical system view of neural population.</p> <p>I perhaps first read this paper <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a> at least a decade after it got published, but the astonishing finding and the elegance of the algorithm, together with its implicit influences which reshaped my views on interpreting neural population and, also, the current buzzword ‚Äúemergence‚Äù, made it a classic enduring the test of time. This post is both a flashback and an exploration, where I extract a few key components from the article, specifically its dynamical systems interpretation and the jPCA algorithm itself, and briefly discuss around a few questions and extend from the paper to some open queries in the end.</p> <h2 id="dynamical-systems-perspective">Dynamical systems perspective</h2> <p>A traditional perspective characeterizes neural activities from the primary motor cortex (M1) as representing the corresponding movement parameters. Equivalently, we could write out a parametric equation:</p> \[r_n(t) = f_n(param_{1}(t), param_{2}(t), param_{3}(t), ...)\] <p>where \(r_n(t)\) is the firing rate for the \(n\)th neuron, tuned by the corresponding function \(f_n\). Alternatively, instead of a representational model, another perspective based on neural population encoding which reflects behavior parameters not on the single neuron level, but on the population level with a <strong>dynamical system</strong>, could be written as follows:</p> \[\dot{r}(t) = f(r(t)) + u(t)\] <p>Here \(f\) might represent a linear dynamical system (linear dynamical system, or can be nonlinear) and \(u(t)\) is an unknown external input. In this view, the dynamics, i.e., the evolution of population response, encodes the movement parameters. Or put differently, within a dynamical system model, each single-unit response should reflect the ‚Äúdynamical factors‚Äù exhibited from each latent state in the latent space, which we aim to identify from the observed high-dimensional neural pouplation recordings.</p> <h2 id="quasi-rhythmic-responses">Quasi-rhythmic responses</h2> <p>As made clear in the article, the critical finding of this study is that reaching, a non-oscillatory movement (unlike the swimming leech or a walking monkey), leads to a quasi-oscillatory neural trajectory. More surprisingly, the rotations are distinct not by reaching curvatures but determined from the initial conditions, which are encoded by the <strong>preparatory activities</strong>. We will see that <strong>preparatory activities</strong> feature as an essential ingredient in Churchland and his colleagues‚Äô research, leading to surpising results from analysis on nullspace/output-potent space <a class="citation" href="#nullspace">(‚ÄúCortical Activity in the Null Space: Permitting Preparation without Movement,‚Äù 2014)</a> and <a class="citation" href="#prep_review">(Mark M Churchland, 2024)</a> a few years later. Specifically, as the authors summarized, the trajectories have the following primary properties, which support the dynamical systems perspective:</p> <blockquote> <p>1] Rotation is a ubiquitous phenomenon during behavior;</p> <p>2] Trajectories have the same directions for all rotations;</p> <p>3] Preparatory activities determine the initial conditions which govern trajectories;</p> <p>4] Rotations do not directly correlate with the curvature</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_3.png" class="img-fluid rounded z-depth-1" width="65%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 3 in <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a>. jPCA projections into the first 2 jPC dimensions from different monkeys on tasks across different conditions and targets. Each trajectory plots the first <strong>200ms</strong> activities following the preparatory state (initial conditions). Colors correpond to projection onto jPC1. </div> <p>An interesting reflection is that with the dynamical systems perspective hypothesis, rotations of neural states should be similar no matter what reaching conditions: only the initial conditions would determine the trajectories, with the underlying \(f\) being identical. I found this line of thought extremely insightful, which demonstrates how important it is to deeply understand theory to deduce results, though they might look unorthodox at first glance for example, the neural states were expected be similar even when the reaches are in opposite directions if they share similar preparatory activities, because they are highly correlated with initial conditions. The only question that remains is how to determine initial conditions. Again, this ties by to experimental observations and penetrative reflections/insights into the perhaps daily orthodox phenomena.</p> <p>The authors also carried some control (shuffling) analyses, and corroborated that such rotational pattern does explain a significant amount of data variance. Interestingly, the authors also discovered that, although rotations are consistent for all conditions in the same jPCA plane (similar orietations and speeds), such rotations actually exist in multiple jPCA planes. As shown below, all top 3 jPCA planes contain rotations, but with higher-numbered jPCA planes carrying less ordered rotations with slower speed. Perhaps not so surprisingly, both PMd and M1 exhibit such rotational structures, with initial states/prepatory activities better distinguished in PMd (PMd is known for movement planning and with stronger preparatory activities. Eight years later, Russo et al. <a class="citation" href="#russo">(‚ÄúNeural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation,‚Äù 2020)</a> would showcase an obvious distinction between how M1 and PMd encode movement sequence/planning by exploring their corresponding latent trajectories, providing further insights into representational differences between these two brain regions)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_4.png" class="img-fluid rounded z-depth-1" width="55%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 4 in <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a>. jPCA projections into different planes exhibit similar rotation structures. </div> <h2 id="jpca">jPCA</h2> <p>Based on the dynamical systems perspective, since we focus on time-dependent variations, a naive PCA is not sufficent to extract such temporal structures from neural activities (PCA is not specifically designed for incapsulating dynamical structures). Here Churchland et. al. developed an algorithm called jPCA to reoslve this issue. Specifically, it finds orthonormal axes (thus basis which define linear subspaces) which capture the strongest rotational components from the subspace identified by PCA (to ensure that the rotational dynamics come from subspaces that efficiently ‚Äúrepresent‚Äù the high dimensional neural space). Conseuqently, this is equivalent to rotating the PCA projections to help viewers better ‚Äúsee‚Äù the rotation most clearly (as shown in Supplementary Movie 2 below). In the paper the authors chose the PCA dimension as 6, and the data projected from the 6-D PCA space to the first 2 jPCA components, thus a plane which captures the strongest rotations, is displayed (Adapted Supplementary Movie 2, shown below) to reveal the underlying oscillatory structure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 2 in <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a>, showing jPCA projections are essentially a different view of PCA projections. </div> <p>The core of jPCA lies in the following steps:</p> <blockquote> <p>1] Fit a linear dynamical system (linear dynamical system) for \(\dot{X}_{red} = M_{skew}X_{red}\), where \(X_{red}\) is of size \(6 \times ct\), \(c\): conditions, \(t\): time. Notice that this is really just a matrix formulation of \(\dot{x}_{red, i} = M_{skew}x_{red, i}\), where \(\dot{x}_{red, i}, \; x_{red, i}\) are column vectors and \(\dot{X}_{red} = [\dot{x}_{red, 1}, \; \dot{x}_{red, 2}, \; ..., \; \dot{x}_{red, (c(t-1))}], X_{red} = [x_{red, 1}, \; x_{red, 2}, \; ..., \; x_{red, c(t-1)}]\). Since \(\dot{X}_{red}\) is of size \(6 \times c(t-1)\) (use discrete time difference as approximations for the differentice), \(M_{skew} \in \mathbb{R}^{6 \times 6}\). Notice that to be precise, \(X_{red}\) is also truncated to be \(6 \times c(t-1)\). 6 is the dimensionlality of a PCA projection before starting jPCA, as mentioned above;</p> <p>2] Since \(M_{skew}\) is a skew-symmetric matrix, it has pure imaginary eigenvalues and thus captures rotational dynamics;</p> <p>3] Identify the complex vectors \(V_1\) and \(V_2\) corresponding to the largest two imaginary eigenvalues. From these locate the real planes: \(jPC_1 = V_1 + V_2\), and \(jPC_2 = j(V_1 - V_2)\);</p> <p>4] The first 2 jPC projection is thus \(X_{jPC} = [jPC_1; jPC_2] \times X_{red}\). Similar projections for other jPCA planes.</p> </blockquote> <p>Note that the above \([jPC_1; jPC_2] \in \mathbb{R}^{2 \times 6}\). Also, for a given jPCA plane, the choice of orthogonal basis is arbitruary, so the authors pick \(jPC_1\) and \(jPC_2\) such that the rotation is anti-clockwise and the preparatory activities spread along most clearly on \(jPC_1\).</p> <p>For the sections below, I‚Äôll dive into details of the jPCA algorithm. Readers might want to skip the following if the above information is sufficient. For a more detailed description of jPCA, let‚Äôs start by reordering \(X\) as \(X \in \mathbb{R}^{ct \times n}\), and for instance \(X\) might be choosen as \(X_{red} \in \mathbb{R}^{ct \times k}\), for \(k = 6\) after PCA projection. For simpler and more general notation, for the following I will use \(X\) for illustration.</p> <h3 id="dynamical-summary-of-data">Dynamical summary of data</h3> <p>For a canonical PCA projection, we would start by finding the covaraince matrix \(\Sigma = X^TX \in \mathbb{R}^{n \times n}\) (notice that here \(n\) represents not the number of samples, but the number of features). We assume that \(X\) is already mean-centered. To capture dynamical structure, we will need a different \(n \times n\) matrix to summarize the data: consider a time-invariant linear dynamical system: \(\dot{x}(t) = x(t)M, M \in \mathbb{R}^{n \times n}\) (in accordance with the size of \(X\), \(x(t)\) is a row vector: \(x(t) \in \mathbb{R}^{1 \times n}\)). Consequently, this reduces to solving</p> \[M^{*} = \argmin_{M \in \mathbb{R}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>Or simply put, \(M^{*} = X \backslash \dot{X}\). Notice that this optimization has a simple and closed-form solution:</p> \[M^{*} = (X^TX)^{-1}X^T\dot{X}\] <p>As the authors argued, the data covariance matrix \(\Sigma\) captures <strong>an ellipsoid which best fits the data</strong>. It geometrically represents an ellipsoid because quadratic forms from symmetric positive semidefinite matrices naturally yield ellipsoids. Notice that a quadratic form</p> \[q(x) = x^TAx\] <p>where \(A\) is symmetric, could be interpreted as the square of distorted length of x. Consequently, the level set \(x^TAx = c\) defines the set all points with distorted length (by \(A\)) equal to \(\sqrt{c}\). If \(A\) is symmetric and positive definite, then the level set becomes an ellipsoid Specifically, \(\Sigma\) (with data already demeaned) encodes the spread and correlation of data along different directions. The corresponding eigenvectors point at the principal directions of variation, while the eigenvalues indicate how much the data varies along those directions. Notice that here ‚Äúbest fit‚Äù is not in the sense of \(L_2\), but that the covariance ellipsoid axes align with the data‚Äôs main directions of variability, and its radii scale with the spread of the data. In other words, If we project the data onto the directions of eigenvectors/ellipsoid axes, the ellipsoid gives a natural compact summary of how spread out the data are. Consequently, if the data is uncorrelated in all feature dimensions and the variance along each feature is identical (isotropic), then the covariance is \(\Sigma = \sigma I\) and thus the ellipsoid is a sphere. In summary, \(\Sigma\) is a geometric label of the second order structure of the data. However, given such benefits, it does not take temporal information into consideration, whereas \(M^*\) characterizes a linear dynamical system which best fits (linearly) the data \(X\) (to be fair, here the data \(X\) already carries with it some dynamical structure).</p> <h3 id="skew-symmetric-specification-for-rotation">Skew-symmetric specification for rotation</h3> <p>For a general linear dynamical system, the dynamics include both expansions/contractions and rotations. Specifically, if \(M\) has real eigenvalues \(\lambda_i\), the system exhibits exponential growth (unstable direction, all \(\lambda_i &gt; 0\)), or exponential decay (stable direction, all \(\lambda &lt;0\)) along the corresponding eigenvectors, or saddle behavior (some \(\lambda_i &lt; 0\) while some \(\lambda_j &gt; 0\)). Geometrically, this looks like stretching or shrinking along certain axes. If \(M\) has complex eigenvalues \(\alpha_i \pm i\beta_i\) the system combines exponential scaling (from \(\alpha_i\)) with rotation (from \(\beta_i\)). If \(\alpha_i=0\), it‚Äôs pure rotation. If \(\alpha_i \neq 0\), then the dynamics exhibit a spiral (either expansion or contraction depending on the sign of \(\alpha_i\)). Note that the above are local behaviors around fixed pionts, and will be easier to visualize (with fewer combinatorial conditions) in second-order linear dynamical system since there‚Äôll only be 2 eigenvalues.</p> <p>In this study, the authors are particularly interested in extracting potentially pure rotational dynamics. Consequently, Churchland and colleagues started with the decomposition that every linear transformation \(M\) (thus the linear dynamics of \(\dot{X}\), within the same space) can be dissected into two components:</p> \[M = M_{symm} + M_{skew}\] <p>where \(M_{symm} = (M + M^T)/2, M_{skew} = (M - M^T)/2\), so that \(M_{symm} = M_{symm}^T, \; M_{skew} = -M_{skew}^T\). This raises an interesting point whether all linear transformations can be decomposed into symmetric and skew-symmetric components: well, not quite, since here the formula only exist for \(M\) being a square matrix. However, even if \(M \in \mathbb{R}^{k \times l}, \; k \neq l\), it is still a linear transformation. How should we reconcile and think deeper here? I will leave this to the last open queries for further discussions.</p> <p>With such decomposition, \(M_{symm}\) has purely real eigenvalues and \(M_{skew}\) has purely imaginary eigenvalues (in conjugate pairs). They describe expansions/contractions and rotations, correspondingly (This relation is further illustrated in the last section together with matrix exponential and Lie group/algebra).</p> <p>Consequently, if we specify the set of skew-symmetric matrices as \(\not \mathbb{S}^{n\times n}\), and</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <h3 id="solution-of-the-constrained-optimization">Solution of the constrained optimization</h3> <h4 id="matrix-problem-rewritten-into-vector-form">Matrix problem rewritten into vector form</h4> <p>For solving the above optimization constrained to only skew-symmetric matrices, Churchland and colleagues did the following modifications: first notice that when solving \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\), each column of M is independently determined by the corresponding column of \(\dot{X}\). Consequently, the matrix optimization problem could be written in vector format: unroll \(M \in \mathbb{R}^{n \times n}\) into \(m = M(:)\), where \(m \in \mathbb{R}^{n^2}\), and thus the unconstrained least squares problem could be rewritten as:</p> \[m^{*} = \argmin_{m \in \mathbb{R}^{n^2}} ||\dot{x} - \tilde{X}m||_{F}\] <p>where \(\dot{x} = \dot{X}(:)\), and \(\tilde{X}\) is a block diagonal matrix with \(X\) repeated on the \(n\) diagonal blocks. PS: I totally agree that the matrix/vector formats are equivalent, but it does not take \(M^{*} = X \backslash \dot{X} = (X^TX)^{-1}X^T \dot{X}\) to realize such format conversion. By inspecting upon \(\dot{X} = XM\), we should already know that each column of \(\dot{X}\) is independently informed by the corresponding column of \(M\). The deeper reason for this rewrittiing is revealed as below:</p> <h4 id="skew-symmetric-matrix-constraint">Skew-symmetric matrix constraint</h4> <p>To add skew-symmetricity constraint to the optimization problem, notice that for \(\not \mathbb{S}^{n \times n}\), the degrees of freedom is \(n(n-1)/2\) (the diagonals are automatically 0, thus with \(n\) less than \(n(n+1)/2\) as the degrees of freedon/dimensionality in symmetric matrix). The next crucial step is to realize that a skew-symmetric matrices are equivalently represented as vectors of the form \(k \in \mathbb{R}^{n(n-1)/2}\). I also want to mention that this is fundamentally related to the underlying Lie group structure [TODO] and I‚Äôll leave them to the last section for more discussoins.</p> <p>A side note: the paradigm of setting constraints/descriptions even before fitting the model also reinforces my view that acquiring <strong>geometric priors</strong>, or basically whatever kinds of priors, embedded in the decoding models is alleviating model‚Äôs ‚Äú<strong>cognitive burden</strong>‚Äù (exactly like a spider offloading its attention onto its web to detect when its prey is coming instead of staying focused all the time (See Chapter 1 of %cite modelsmind %)). It vaguely evokes the flavor of the current trend of speech BCI, which entails large language models for improving RNN decding. Geometric deep learning is also a field with similar guiding principle.</p> <p>Back to the central topic, with this vector \(k \in \mathbb{R}^{n(n-1)/2}\), we could likewise simply form a linear map \(H\) which transforms \(\mathbb{R}^{n(n-1)/2}\) into \(\mathbb{R}^{n^2}\). Specifically,</p> \[m = Hk\] <p>where \(H \in \mathbb{R}^{n^2 \times n(n-1)/2}\), and \(m \in \mathbb{R}^{n^2}\). There‚Äôs no unique way to design \(H\); one simple method might be the following:</p> \[H = \begin{bmatrix} I_{n(n-1)/2} \\ 0_{n \times n(n-1)/2} \\ -I_{n(n-1)/2} \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; \cdots &amp; 1 \\ 0 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; \cdots &amp; 0 \\ -1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; -1 &amp; \cdots &amp; 0 \\ \vdots &amp; 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; \cdots &amp; -1 \\ \end{bmatrix}\] <p>In a nutshell, \(k\) encodes the lower/upper triangle of \(M_{skew}\) and \(H\) can be hand-coded (not shown here, but sparse and has only \(n(n-1)/2\) positions of 1 and \(n(n-1)/2\) positions of -1).</p> <h4 id="final-closed-form-optimization">Final closed-form optimization</h4> <p>Notice that now we finally see the benefit of the vector format: \(Hk\) returns a vector of shape \(\mathbb{R}^{n^2}\) which equivalently represents \(M_{skew} \in \not \mathbb{S}^{n \times n}\). Consequently, the original constrained optimzation problem</p> \[M^{*} = \argmin_{M \in \not \mathbb{S}^{n \times n}} ||\dot{X} - XM||_{F}\] <p>is eventually rewritten as</p> \[k^{*} = \argmin_{k \in \mathbb{R}^{n(n-1)/2}} ||\dot{x} - \tilde{X}Hk||_{2}\] <p>Notice that this is similar in form to the OLS problem, and can thus be written in closed form:</p> \[k^{*} = (\tilde{X}H)\backslash \dot{x}\] <p>and eventually</p> \[M^*_{skew} = R(Hk^*)\] <p>where \(R\) is a reshaping map from \(\mathbb{R}^{n^2}\) to \(\mathbb{R}^{n \times n}\).</p> <p>The authors also briefly discussed a few other implementation techniques, including solving the constrained optimization problem using a gradient descent based method. However, since this optimization problem has a unique global optimum, they should return the same results.</p> <h2 id="discussions">Discussions</h2> <h3 id="what-does-it-represent-to-decompose-the-dynamics">What does it represent to decompose the dynamics?</h3> <p>With \(M^{*}_{skew}\), the authors could find out the corresponding eigenvectors and eigenvalues and do projections accordingly. Since skew-symmetric matrices only have pure imaginary eigenvalues and the eigenvectors come in <strong>orthogonal, complex conjugate</strong> pairs (thus could think about the decomposition of \(M_{skew}^{*}\) as extracting orthogonal planes), the real projection planes \({u_{i, 1}, u_{i, 2}}\) are calculated from the complex conjugate vector pairs \({v_{i, 1}, v_{i, 2}}\) (with the corresponding eigenvalues \(\pm \beta_i\)) by \(u_{i, 1} = v_{i, 1} + v_{i, 2}, \; u_{i, 1} = j(v_{i, 1} - v_{i, 2})\) with normalization (magnitude 1). Why do we eigendecompose the summary matrix (either \(\Sigma\) or \(M\))? Or more specifically, what do these eigenvectors/eigenvalues inform us? Notice that the eigenvector/eigenvalue decomposition logics are different here: For PCA, eigenvectors of the covariance matrix \(\Sigma\) serve as principal components of the underlying data (geometric view: axes of ellipsoid as mentioned above); For jPCA, the eigenvectors stipulate the axes/subspace along which the solutions of the underlying dynamical system show rotational patterns. Projections onto jPC‚Äôs with the largest magnitude of the corresponding eigenvalues present the most significant rotation: higher frequency and more consistency (also shown in Supplemantary Video 4).</p> <p>Theoretically, this results from the fact that the magnitude of the pure imaginary eigenvalues encodes the frequency of oscillation (rotation speed). In our case here, since each conjugate pair \(\pm \beta_i\) corresponds to a 2D <strong>invariant subspace</strong> in which the flow is a rotation at frequency \(\beta_i\), the 6D dynamics are essentially a <strong>direct sum</strong> of 2D oscillatory patterns, each rotating at its own frequency. The real basis (\(u_{i, 1}, u_{i, 2}\)) is formed using the above calculation (why would the above calculation make sense? A question for readers), and they form an real 2D invariant subspace where the projection exhibits a circle/ellipse trajectory at the frequency \(\beta_i\). Moreover, there‚Äôs no expansion nor contraction and the magnitude of \(\beta_i\) indicates the <strong>oscillation frequency/rotation speed</strong>. Why does the magnitude of eigenvalues encode rotational frequency? This can be seen from the solution to the linear dynamical system.</p> <p>Let me give a very simple exmaple (perhaps the simplest in this case): Assume for our LDS: \(\dot{x}(t) = Ax(t), \; x(t) \in \mathbb{R}^2\), where</p> \[A = \begin{bmatrix} 0 &amp; \omega \\ -\omega &amp; 0 \end{bmatrix}\] <p>with \(\omega &gt; 0\). The eigenvalues are easily calculated to be \(\lambda = \pm i\omega\), and thus the solution is</p> \[x(t) = e^{At}x(0)\] <p>where</p> \[e^{At} = \begin{bmatrix} \cos(\omega t) &amp; -\sin(\omega t) \\ \sin(\omega t) &amp; \cos(\omega t) \end{bmatrix}\] <p>Consequently, it‚Äôs obvious that \(e^{At}\) is a rotation matrix, applied on \(x(0)\) with pure rotation with angular speed \(\omega\). Notice that in the above I also utilize the fact that the general solution for LDS entails matrix exponential. I‚Äôm currently also writing another blog on exponential maps not only on its application in linear differential equations, but also in differential geometry/Lie group. It turns out to be much more fundamental than how contrived it might appear at first glane. A fantastic object.</p> <p>Now let me also show the solution for a general LDS with pure imaginary eigenvalues [TODO].</p> <h3 id="quasi-similarity-micro-vs-macro--explainers">Quasi-similarity: micro-vs macro- explainers</h3> <p>One caveat from this paper is that the extracted neural trajectory is only ‚Äúquasi-oscillatory‚Äù, not always a complete rotation: the oscillation usually exists for barely 1 cycle. It‚Äôs a little unclear how the activities would evolve if more than \(200ms\) of neural population activities, recorded outside of this structured delay paradigm, are projected using jPCA. At the same time, for many neurons, the preparatory and movement-related single neuron responses do exhibit some quasi-oscillations which last for about 1 cycle (See in below).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_fig_2.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Fig. 2 in <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a>. Trace colors are according to the strength of preparatory activities (red with greatest preparotory response). It allows clear view of the evolution of preparatory patterns during the movement. </div> <p>I mentioned the word <strong>emergence</strong> at the beginning a little bit. I don‚Äôt think anyone ever doubts that the population level behavior is composed of, or resulting from, micro-level patterns from units in the population. The key question is what is the <strong>correct</strong> first level of explanation for the corresponding object of the selected problem. Will the microstates encode the ‚Äúfundamental truth‚Äù, if any, or the emergent structures provide the most fundamental explanation. You probably can tell that I‚Äôm being very vague here. Actually, this debate, or the gradual shift from reductionism to emergence, exists in many fields nowadays across computational/systems/cognitive neuroscience, LLM from deep learning, and of course physics, etc. In this paper the authors did not give a clear explanation of how such single unit level multiphasic response produces the observed rotation structure at the population level. However, it was indeed observed that the jPCA projection onto each axis does exhibits similar patterns as single-neuron response(shown below)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_1.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 1 in <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a>. The projections onto jPC's are similar to single neuron response. </div> <p>The quasi-similarities (I think this might nicely capture the fuzzy feeling here) of signal patterns through time (shown by the 2 figures above) in the population level and the single-neuron level could be explained by the fact that jPC‚Äôs capture the most conscipuous patterns in the data among the recorded neural population, or that each jPCA projection is a weighted sum/average of individual-neuron resopnses. Likewise, each single neuron could also be interpreted as a combination of all the jPC projected patterns, except that the intertwined relationship is not obvious if just viewed from a single neuron or a pattern itself.</p> <h3 id="multiphasic-response-and-phase-difference">Multiphasic response and phase difference</h3> <p>A multiphasic response means that a single neuron‚Äôs activity might rise and fall (or exhibit multiple peaks and troughs) over time during a reaching movement. Indeed, from the above figures we could observe that motor cortex neurons often exhibit these complex, multiphasic temporal patterns. From there we might argue that perhaps a sufficient condition for the rotation structures is that neurons exhibit multiphasic responses. However, this turns out to be not necessarily true. Such complexity at the level of individual neurons, while intriguing, doesn‚Äôt automatically translate into coherent rotational dynamics at the level of the population. The authors also tested on a ‚Äúcomplex-kinematic model‚Äù and recorded EMG. Though both carry multi-phasic reponses, they do not show clear rotation patterns.</p> <p>In the end, as the authors argued, multiphasic responses are not enough: we also need the multi-phasic patterns to have consistent phases for around 90 degrees apart. This is an intriguing math argument. Indeed, for the population to trace out a rotational trajectory, at least 2 temporal patterns need to be phase-shifted by <strong>90 degrees</strong> (if there‚Äôs amplitude change then the trajectory should be an ellipse) with the same frequency (if different frequencies or time-dependent frequencies, might be more complex). Observed rotation implies the existence of a complex eigenvector pair and a 2D subspace whose coordinates are quadrature components (\(90\) degrees apart) with common frequency after appropriate linear transform/projection, since such 2D subspace might not show itself in high D space. On the contrary, wwo components in \(90\) degrees phase lag with the common frequency produce an elliptical rotation in their plane (circle if with the same amplitude). If frequency is different or with drifting phase/amplitude, a clean rotation need not appear. In reality, among high dimensional noise and condition-dependent variance, even a clean rotation might be burried in non-oscillatory trajectories.</p> <h3 id="from-oscillation-to-non-oscillation">From oscillation to non-oscillation</h3> <p>Another core question is how such rotational pattern generates non-oscillatory movement patterns, e.g. reaching. The authors conducted some further simulation analyses and demonstrated that it‚Äôs possible to reconstruct EMG (deltoid) activities from weighted sum of rotations with different magnitude and phases (they call this the generator-model). It‚Äôs interesting to see this because EMG does not acquire the latent rotational patterns, yet it could be reconstructed from rotational dynamics. Further quantification analyses also showed that only the neural data and generator-model exhibit rotational patterns, whereas two other models (velocity-tuned, complex-kinematic models) do not. Since EMG showed only weak rotation, as the authors put, this further illustrates that the latent rotation dynamics do not necessarily result from a muli-phasic reponse, but how such response is constructed.</p> <h3 id="speed-vs-amplitude">Speed vs. amplitude</h3> <p>One interesting finding is that the speed of movements is not encoded as the speed of the latent trajectories (they have similar angular velocity). The amplitude of rotation dictates different movement speeds (for example faster or slower reaches). As Churchland et al. argued, this could be nicely explained since rotations with larger amplitude represent more strongly multi-phasic responses, with which EMG frequently entails larger accelerations.</p> <h3 id="the-effect-of-cross-condition-mean">The effect of cross-condition mean</h3> <p>The rotational structure is dependent upon the initial states (dynamical systems view) specified by the prepratory activities, which captures the condition-dependent activities (here condition refers to experiment conditions). Indeed, to clearly observe such differences, the cross-condition mean is substracted first for each single neuron before PCA/jPCA. The authors also illustrated the effects of keeping the cross-condition mean, as shown below:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/jPCA/jPCA_supfig_11.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Fig. 11 in <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a>. </div> <p>The goal of mean subtraction is to preserve only data distinguished in different conditions. In panel a. and b., each trace represents a trial-averaged firing rate for one (experiment) condition, with the same coloring scheme indicating the strength of preparatory activities. The yellow trace indicates the cross-condition mean for that neuron. Panel b. substracts the mean (yellow line being 0 all along) while panel a. keeps it. If without substraction of cross-condition mean, the first jPCA plane would preserve condition-independent curved trajectories. The authors argued that this is not surprising, since many neurons exhibit similar behaviors across different conditions, which are thus naturally captured by PCA projections. Since jPCA is extracting patterns from PCA space, it‚Äôs expected that such patterns would be kept. Interestingly, even in panel c., these trajectories still carry some curvature, though it‚Äôs difficult to pinpoint how it‚Äôs theoretically generated. Panel d. displays the jPCA projection onto the second jPC plane (\(jPC3_, \; jPC_4\)) and we do see similar rotational structures dependent upon initial states specified by the preparatory activities. Consequently, to always explore the condition-dependent dynamics, the authors argued to substract cross-condition mean first before PCA/jPCA.</p> <h3 id="skew-symmetric-constraint-prior-or-posterior">Skew-symmetric constraint: prior or posterior</h3> <p>The authors stressed that because they are interested in only in rotational linear dynamical systems, the original least square problem is solved under constrained optimization. However, I‚Äôm a little confused why not fitting a general linear dynamical system and then extract out the <strong>unique</strong> skew-symmetric component, onto which the neural data could be projected to observe the rotational effects. In this manner, the fitted \(M^{*}\) instead of \(M_{skew}^{*}\) might capture more subtle dynamics and would not distort the inherent dynamical components.</p> <h2 id="conclusions">Conclusions</h2> <p>In summary,</p> <blockquote> <p>1] M1 reaching behavior supports the dynamical systems perspective</p> <p>2] Rotation is a ubiquitous phenomenon, and does not relate to movement curvature and orientation</p> <p>2] Preparatory activities encode the initial conditions for the underlying dynamical system</p> </blockquote> <p>The shift of views from single neuron analysis in pursuit of movement/behavior correlates/representations, to a dynamical system analysis at the neural population level, is becoming an increasingly populat domain. As we will see later, the other side of the same story, geometry instead of dynamics, will quickly come into the arena under the name of neural manifold. This notion, with both the aboundance of aesthetic elegance and disatisfying lack of mathematical rigor from either topology or differential geometry, will be an eternal theme for the following research, projects, and blogs of mine.</p> <h2 id="open-querries">Open Querries</h2> <p>1] The annotomical circuitry that leads to the dynamical system/latent trajectory is still unclear.</p> <p>2] What happens after the 200ms? As shown in Supp Movie 3 below, how would the observed rotations evolve later in time (notice that all the rotations exist for merely 1~1.5 cylces)? Do they end up in the same positions (or just same in this subspace)?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/jPCA/jPCA_Supp_Video_3.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Adapted from Supplementary Movie 3 in <a class="citation" href="#jpca">(‚ÄúNeural Population Dynamics during Reaching,‚Äù 2012)</a>, showing the evolution of the projected states. </div> <p>3] Although in the Discussions section, I briefly talked about the reason for existence of the rotational structures, The problem still remains as to how individual oscillations of different phases and amplitudes would result in a consistent rotation.</p> <p>4] Inspired from the jPCA algorithm, apart from \(\Sigma\) (covariance of demeaned data) or \(M^*\), is there other way of capturing the data statistics from different perspectives?</p> <h2 id="extensions">Extensions</h2> <h3 id="hermitian-unitary-and-normal-matrices">Hermitian, unitary, and normal matrices</h3> <p>Churchland and colleagues put the following at the end of the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fnature11129/MediaObjects/41586_2012_BFnature11129_MOESM225_ESM.pdf">Supplementary Information</a>:</p> <blockquote> <p>‚ÄúThe convenience of being able to eigendecompose a summary matrix and yield orthogonal vectors belongs to the class of normal matrices, which by definition are diagonalizable by a unitary matrix. The class of normal matrices includes symmetric and skew-symmetric matrices, among others. This fact suggests a broader class of PCA variants that are a subject of future work.‚Äù</p> </blockquote> <p>In order to understand the inner workings of matrix decomposition, I‚Äôd like to review first the following definitions.</p> <blockquote> <p><em>Def</em>: The Hermitian tranpose (or conjugate transpose) of a matrix \(A\) is denoted as \(A^{\dagger} = \bar{A}^{T}\), which is tranpose of the complex conjugate (applied to each entry). Equivalent notations are \(A^H, \; A^{*}\). Notice that for a real matrix \(A \in \mathbb{R}^{n \times n}, \; A^{\dagger} = A^{T}\).</p> <p><em>Def</em>: A matrix is called <strong>Hermitian</strong> (or <strong>self-adjoint</strong>) if \(A = A^{\dagger}\). Notice that if A is a real <strong>Hermitian</strong> matrix, then it is equivalently <strong>symmetric</strong>.</p> <p>. <em>Def</em>: A matrix \(U\) is called <strong>unitary</strong> if \(U^{\dagger}U = I\) Notice that a real <strong>unitary</strong> matrix is equivalently <strong>orthogonal</strong>.</p> </blockquote> <p>We also have the following theorems for the above special matrices (more discussions seen in this <a href="https://www.math.purdue.edu/~eremenko/dvi/lect3.26.pdf">note</a>):</p> <blockquote> <p><em>Spectral theorem</em> for Hermitian matrices: For a <strong>Hermitian</strong> matrix,</p> <p>(i) all eigenvalues are real,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>and likewise</p> <blockquote> <p><em>Spectral theorem</em> for unitary matrices: For a <strong>unitary</strong> matrix,</p> <p>(i) all eigenvalues have magnitude 1,</p> <p>(ii) eigenvectors corresponding to distinct eigenvalues are orthogonal,</p> <p>(iii) there is an orthonormal basis consisting of eigenvectors.</p> </blockquote> <p>Consequently, Hermitian and unitary matrices are always <strong>diagonalizable</strong> (indeed some eigenvalues might be the same). Notice that eigenvectors of any matrix corresponding to distinct eigenvalues are linearly independent. Here with Hermitian and unitary matrices they are not only linearly indepedent, but also <strong>orthogonal</strong>.</p> <p>Theorems (i) for both Hermitian and unitary matrices could be proven separately. Since (ii) and (iii) are the same, we might be thinking about looking for a greater class of matrices which include these two types, while carrying more general properties, entailing (ii) and (iii). Let me introduce the following:</p> <blockquote> <p><em>Def</em>: A normal matrix is a matrix that commutes with its adjoint: \([A, A^{\dagger}] = 0\) where [B, C] = BC - CB</p> </blockquote> <p>Note that Hermitian and unitary matrices are special cases of a normal matrix. <strong>Skew-Hermitian</strong> (\(A^{\dagger} = -A\); in real case, skew-symmetric) matrices are also normal matrices. Also, the previous definitions will be helpful for the next extension into Lie group/algebra. For normal matrices we have the following:</p> <blockquote> <p><em>Spectral theorem</em> for normal matrices: A matrix is <strong>normal</strong> <em>if and only if</em> there is an orthogonal basis consisting of eigenvectors.</p> </blockquote> <p>The above is equivalent to saying that a normal matrix \(A\) is <strong>unitarily diagonalizable</strong>:</p> \[A = U\Lambda U^{\dagger}\] <p>where \(U\) is unitary (orthogonal if \(A\) is real), and \(\Lambda\) is diagonal. From the above theorem we could easily see that a symmetric matrix \(A\) has all real eigenvalues (becuase it‚Äôs a Hermitian matrix). Consequently, it has the decomposition:</p> \[A = B \Lambda B^{-1}\] <p>where \(\Lambda\) is a real diagonal matrix, B orthogonal.</p> <p>The speicality of the spectral theorem on normal matrices is that for non-normal matrices, you might still have eigenvectors, but they don‚Äôt form an orthogonal basis (sometimes they don‚Äôt even span the whole space). For example, \(A = \begin{bmatrix}1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}\), which has identical eigenvector so even not a full basis. Normal matrices are exactly those that can be diagonalized by an orthogonal (or unitary) change of basis, with nice orthogonal eigenvectors spanning the whole space.</p> <p>There‚Äôs also a famous fact between Hermitian and unitary matrix:</p> <blockquote> <p>There exists a 1-1 correponsdence between the set of unitary matrices \(U\) and the exponental of the set of Hermitian matrices \(H\), i.e.:</p> \[U = exp(iH)\] </blockquote> <p>The proof is not hard and not displayed here. The message it encodes is important: the unitary matrices are exactly in the format of the exponential map (same name as in LDS, different meaning, but could be interpreted similarly; see the future blog) of Hermitian matrices.</p> <p>Similarly, we could show the following for orthogonal matrices: Given \(A\) as a skew-symmetric (real) matrix (\(A^{\dagger} = A^{T} = -A\)), notice first that</p> \[-iA = iA^T = iA^{\dagger} = (-iA)^{\dagger}\] <p>Consequently, \(-iA\) is a Hermitian matrix and thus since</p> \[e^{A} = e^{i(-iA)} = e^{i(-iA)^{\dagger}}\] <p>Then \(e^{A}\) is a unitary matrix. Since \(A\) is real, \(e^{A}\) has to be real. Consequently, \(e^{A}\) is an orthogonal matrix. In other words, the exponential map of <strong>skew-symmetric</strong> matrices leads to an <strong>orthogonal</strong> matrix. However, this mapping does not result in all orthogonal matrices:</p> \[det(e^A) = e^{trA} = e^{0} = 1\] <p>which means that this mapping only characterizes rotation matrices (determinant equal to 1, unlike reflection which changes the orientation). Again, we arrive at the logic of fitting a skew-symmetric matrix for rotational dynamics, from pure matrix calculation (a little bit of Lie group/algebra flavor, but without resorting to the fact that the solution of LDS is matrix exponential mapping; Or maybe there‚Äôs a deeper connection?).</p> <h3 id="linear-transformation-and-decomposition">Linear transformation and decomposition</h3> <p>Notice that within jPCA, the authors made the following decomposition: \(M = M_{symm} + M_{skew}\), which naturally dissects the linear dynamics into expansions/contractions and rotations. I‚Äôm wondering whether similar decomposition can be made for general linear transformation (not necessarily in the linear dynamical systems framework) \(A \in \mathbb{R}^{k \times k}\) for \(y = Ax, \; x,y \in \mathbb{R}^{k}\).</p> <p>Again, any real square matrix \(A\) can be uniquely decomposed into:</p> \[A = S + K\] <p>where \(S = (A + A^T)/2\) is symmetric, and \(K = (A - A^T)/2\) is skew-symmetric. Consequently, \(y = Ax = (S + K)x = Sx + Kx\). Let‚Äôs interpret each component separately.</p> <p>For the symmetric part \(Sx\), based on the above <strong>spctral theorem for Hermitian matrices</strong>, because \(S\) is diagonalizable with real eigenvalues and orthogonal eigenvectors, its effect is to scale space (stretch or compress) along mutually orthogonal directions. An example will be to turning a circle into an ellipse (pure stretching).</p> <p>For the skew-symmetric part \(Kx\), it represents the ‚Äúrotation-like‚Äù part of the transformation. In fact, \(\forall x, \; x^T(Kx) = 0\) (simple proof). Consequently, \(Kx\) is always orthogonal to \(x\). While \(K\) teleports \(x\) to orthogonal positions, it might also scale it (according to its matrix elements). A simple example below: the 2D skew-symmetric matrix is of the form:</p> \[\begin{bmatrix} 0 &amp; -\alpha \\ \alpha &amp; 0 \end{bmatrix}\] <p>if \(\alpha \neq 0\), then alongside the orthogonal teleportation there‚Äôs indeed some scaling. A deeper interpretation of the decomposed skew-symmetric matrix here is still to relate it with the exponential map and rotation matrix: unsurprisingly, the exact reason it‚Äôs associated with ‚Äúrotation‚Äù is that the matrix exponential of a skew-symmetric matrix \(e^{tK}\) is always a rotation. Although I proved this statement above, we could also draw some intuition if we begin with some infinitesimal analysis:</p> <p>If we look at rotation by a small angle \(\theta\) (still in 2D):</p> \[R(\theta) \approx I + K\theta\] <p>where</p> \[K = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}\] <p>Here \(R(\theta)\) is finite rotation, and \(K\) is the infinitesimal generator: the first-order linear part of the rotation near identity (Lie algebra?). This connection is exact through the matrix exponential:</p> \[R(\theta) = e^{\theta K}\] <p>For any skew-symmetric \(K\), \(e^{\theta K}\) is a true rotation matrix, and that‚Äôs why we call \(K\) a generator: by exponentiating it we recover a roataion matrix. If we take the first order Tyler expansion on \(e^{\theta K}\) (really, just truncate the first two terms \(I, K\theta\) based on the matrix exponential definition), we get the exact approximation of \(R(\theta)\) as above. In the end, \(K\) by itself is not a rotation (since it‚Äôs not orthogonal), but it encodes the infinitesimal direction of change: skew-symmetric matrices are the infinitestimal generators of rotations.</p> <p>Geometrically, we could also think about rotate a vector in a small time step \(\Delta t\):</p> \[v(t + \Delta t) \approx v(t) + \Delta Kv(t)\] <p>where \(Kv(t)\) is perpendicular to \(v(t)\), thus encoding a vector tangent to the movement trajectory (thus a circle \(\rightarrow\) rotation).</p> <p>By the way, the above is a 2D example. In 3D, skew-symmetric matrices correspond to rotation axes, via the famous <strong>Rodrigues formula</strong>.</p> <p>Then a next question is: how about \(M \in \mathbb{R}^{k \times l}, k \neq l\)? This definitely cannot be a dissection into symmetric and skew-symmetric matrices because this general \(M\) is not a square matrix at all. Moreover, if we want to extend to non-square matrix, we are absolutely out of the realm of dynamical systems. How do we characterize a general linear transformation?</p> <p>In that case we can conduct the following 2 decompositions: one is called polar decomposition. For any real matrix \(A\), it admits a polar decomposition:</p> \[A = UP\] <p>where \(U\) is a ‚Äúpartial‚Äù isometry (orthogonal on the column space of \(A\), behaving like a rotation/reflection), and \(P = A^TA/2\) is symmetric positive semidefinite (behaving like scaling/stretching). If \(A\) is square and invertible, \(U\) is orthogonal and \(P\) is symmetric positive definite.</p> <p>Or an even more general form of decomposition: the well-known <strong>SVD</strong> decomposition:</p> \[A = U\Sigma V\] <p>where \(U, V\) are orthogonal and \(\Sigma\) is diagonal. In this decomposition, \(V\) rotates the input space, \(\Sigma\) stretches along the axes, and \(S\) adds another result to the output space.</p> <h3 id="relations-with-matrix-lie-group-and-lie-algebra">Relations with Matrix Lie group and Lie algebra</h3> <p>In fact, the above discussion already taps into the domain of Lie algebra but without formally defining the underlying group/algebra structure. Specifically, the set of all \(n \times n\) real skew-symmetric matrices \(K\) forms a <strong>vector space</strong>, denoted \(\mathfrak{so}(n)\). Importantly, this space is closed under the Lie bracket:</p> \[[K_1, K_2] = K_1K_2 - K_2K_1\] <p>(we‚Äôve already seen this above for definition of normal matrices). Consequentl, \(\mathfrak{so}(n)\) is formally a <strong>Lie algebra</strong>. Not surprisingly, the correspondingly <strong>Lie group</strong> is the <strong>special orthogonal group</strong>:</p> \[SO(n) = \{Q \in GL_n(\mathbb{R}) | QQ^T = I, det(Q)=1\}\] <p>This is exactly the group for rotation. The relation between the Lie group and Lie algebra is naturally established by the <strong>exponential map</strong>. Lie theory tells us that the tangent space of a Lie group at the identity element forms a Lie algebra. Specifically,</p> \[Q = e^K \in SO(n), \; \forall K \in \mathfrak{so}(n)\] <p>Interestingly, we arrived at the same exact relationship between skew-symmetric matrices and matrices representing rotation (special orthogonal groups with determinant 1) from 2 perspectives: dynamical systems and Lie group/algebra. Consequently, you might feel that there‚Äôs a more fundamental connection. Yes there is and I‚Äôll write out a separate blog on it!</p> ]]></content><author><name></name></author><category term="Computational Neuroscience"/><category term="Latent Dynamics"/><category term="Neural Manifold"/><category term="Dimensionality Reduction"/><summary type="html"><![CDATA[Introduce neural latent dynamics from jPCA]]></summary></entry></feed>